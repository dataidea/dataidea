{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e314b07-1d2f-43cf-b259-0ad8659f84af",
   "metadata": {},
   "source": [
    "---\n",
    "title: Normalization vs Standardization\n",
    "keywords: [normalization, standardization, rescaling]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc37e96b-869b-4e99-8b10-2a443e1fe2ec",
   "metadata": {},
   "source": [
    "![Photo by DATAIDEA](../assets/banner4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5514778-b960-4935-b201-53fbae595a54",
   "metadata": {},
   "source": [
    "#### Introduction:\n",
    "In data analysis and machine learning, preprocessing steps such as data normalization and standardization are crucial for improving the performance and interpretability of models.\n",
    "This Jupyter Notebook provides an overview of the importance of data normalization and standardization in preparing data for analysis and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100cdab0-f47f-46a0-a765-3051a6a1f983",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dataidea as di"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e49d6a-8c7f-4848-af29-eda2423ffb5e",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd830ab7-961d-48c1-b79f-963d1ade45e6",
   "metadata": {},
   "source": [
    "1. **Normalization**: Normalization typically refers to scaling numerical features to a common scale, often between 0 and 1. This is usually done by subtracting the minimum value and then dividing by the range (maximum - minimum). Normalization is useful when the distribution of the data does not follow a Gaussian distribution (Normal Distribution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc35ed7-0dac-417e-aa48-d3cc6e2c3e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Normalization without libraries:\n",
    "def minMaxScaling(data):\n",
    "    min_val = min(data)\n",
    "    max_val = max(data)\n",
    "    \n",
    "    scaled_data = []\n",
    "    for value in data:\n",
    "        scaled = (value - min_val) / (max_val - min_val)\n",
    "        scaled_data.append(scaled)\n",
    "    return scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e3edee-617f-4f35-ba08-5e8b35328f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized data (Min-Max Scaling): [0.0, 0.25, 0.5, 0.75, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# Example data\n",
    "data = np.array([10, 20, 30, 40, 50])\n",
    "normalized_data = minMaxScaling(data)\n",
    "print(\"Normalized data (Min-Max Scaling):\", normalized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edba20e-10a2-4955-b84e-73903a239a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data:\n",
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n",
      "\n",
      "Normalized data:\n",
      "[[0.  0. ]\n",
      " [0.5 0.5]\n",
      " [1.  1. ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Sample data\n",
    "data = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "\n",
    "# Create the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler to the data and transform the data\n",
    "normalized_data = scaler.fit_transform(data)\n",
    "\n",
    "print(\"Original data:\")\n",
    "print(data)\n",
    "print(\"\\nNormalized data:\")\n",
    "print(normalized_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36f8030-d9e2-403a-83ab-0a35b9731d2a",
   "metadata": {},
   "source": [
    "Let's now try on a real world dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8f6888-b324-4442-a523-21f2931899c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296.0   \n",
       "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242.0   \n",
       "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242.0   \n",
       "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222.0   \n",
       "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  MEDV  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     17.8  392.83   4.03  34.7  \n",
       "3     18.7  394.63   2.94  33.4  \n",
       "4     18.7  396.90   5.33  36.2  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston_data = di.loadDataset('boston')\n",
    "boston_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd90b6f-f20d-42c3-971e-63c11bab4192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.64160659, 0.20801527],\n",
       "       [0.00023592, 0.78269825, 0.10496183],\n",
       "       [0.0002357 , 0.59938208, 0.10496183],\n",
       "       ...,\n",
       "       [0.00061189, 0.90731205, 0.16412214],\n",
       "       [0.00116073, 0.88980433, 0.16412214],\n",
       "       [0.00046184, 0.80226571, 0.16412214]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston_scaler = MinMaxScaler()\n",
    "normalized_data = boston_scaler.fit_transform(boston_data[['CRIM', 'AGE', 'TAX']])\n",
    "np.set_printoptions(suppress=True)\n",
    "normalized_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7f87d8-1941-4511-aff9-084228c01199",
   "metadata": {},
   "source": [
    "## Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b535aa4-4db2-4e93-8fe6-6c9b0a68b482",
   "metadata": {},
   "source": [
    "2. **Standardization**: Standardization, often implemented with a method like z-score standardization, transforms the data to have a mean of 0 and a standard deviation of 1. This means that the data will have a Gaussian distribution (if the original data had a Gaussian distribution). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcaf240-2f90-47ba-862f-90b322135345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zScoreNormalization(data):\n",
    "    mean = sum(data) / len(data)\n",
    "    variance = sum((x - mean) ** 2 for x in data) / len(data)\n",
    "    std_dev = variance ** 0.5\n",
    "    standardized_data = [(x - mean) / std_dev for x in data]\n",
    "    return standardized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18f142e-6f09-45c2-977c-8b19d3fdb2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized data (Z-Score Normalization): [-1.414213562373095, -0.7071067811865475, 0.0, 0.7071067811865475, 1.414213562373095]\n"
     ]
    }
   ],
   "source": [
    "# Example data\n",
    "data = [10, 20, 30, 40, 50]\n",
    "standardized_data = zScoreNormalization(data)\n",
    "print(\"Standardized data (Z-Score Normalization):\", standardized_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535edf12-ec08-4bef-9548-785c1f6e618b",
   "metadata": {},
   "source": [
    "In Python, we can also typically use the `StandardScaler` from the `sklearn.preprocessing` module to standardize data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9daf02b4-7b31-4b95-84fd-770124ac0474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data:\n",
      "[[1 2 3]\n",
      " [3 4 5]\n",
      " [5 6 7]]\n",
      "\n",
      "Standardized data:\n",
      "[[-1.22474487 -1.22474487 -1.22474487]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 1.22474487  1.22474487  1.22474487]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Sample data\n",
    "data = np.array([[1, 2, 3], [3, 4, 5], [5, 6, 7]])\n",
    "\n",
    "# Create the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the data and transform the data\n",
    "standardized_data = scaler.fit_transform(data)\n",
    "\n",
    "print(\"Original data:\")\n",
    "print(data)\n",
    "print(\"\\nStandardized data:\")\n",
    "print(standardized_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3bd786-fcb4-41c5-9911-c142c16c4c3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.41978194, -0.12001342, -0.66660821],\n",
       "       [-0.41733926,  0.36716642, -0.98732948],\n",
       "       [-0.41734159, -0.26581176, -0.98732948],\n",
       "       ...,\n",
       "       [-0.41344658,  0.79744934, -0.80321172],\n",
       "       [-0.40776407,  0.73699637, -0.80321172],\n",
       "       [-0.41500016,  0.43473151, -0.80321172]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston_scaler = StandardScaler()\n",
    "standardized_data = boston_scaler.fit_transform(boston_data[['CRIM', 'AGE', 'TAX']])\n",
    "np.set_printoptions(suppress=True)\n",
    "standardized_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f054ad3e-d334-4696-8eea-8b3b5f08a132",
   "metadata": {},
   "source": [
    "#### Importance:\n",
    "\n",
    "1. Data Normalization:\n",
    "   - Uniform Scaling: Ensures all features are scaled to a similar range, preventing dominance by features with larger scales.\n",
    "   - Improved Convergence: Facilitates faster convergence in optimization algorithms by making the loss surface more symmetric.\n",
    "   - Interpretability: Easier interpretation as values are on a consistent scale, aiding in comparison and understanding of feature importance.\n",
    "\n",
    "\n",
    "2. Data Standardization:\n",
    "   - Mean Centering: Transforms data to have a mean of 0 and a standard deviation of 1, simplifying interpretation of coefficients in linear models.\n",
    "   - Handling Different Scales: Useful when features have different scales or units, making them directly comparable.\n",
    "   - Reducing Sensitivity to Outliers: Less affected by outliers compared to normalization, leading to more robust models.\n",
    "   - Maintaining Information: Preserves relative relationships between data points without altering the distribution shape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadaa5ee-e993-4a43-9c35-d43dff83dc17",
   "metadata": {},
   "source": [
    "## Which one? \n",
    "The choice between normalization and standardization depends on your data and the requirements of your analysis. Here are some guidelines to help you decide:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4634a023-b35a-4b1a-a3c6-1369cc8ed853",
   "metadata": {},
   "source": [
    "1. **Normalization**:\n",
    "   - Use normalization when the scale of features is meaningful and should be preserved.\n",
    "   - Normalize data when you're working with algorithms that require input features to be on a similar scale, such as algorithms using distance metrics like k-nearest neighbors or clustering algorithms like K-means.\n",
    "   - If the distribution of your data is not Gaussian and you want to scale the features to a fixed range, normalization might be a better choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8daec5-e4ed-4798-b891-49f2f0b8d316",
   "metadata": {},
   "source": [
    "2. **Standardization**:\n",
    "   - Use standardization when the distribution of your data is Gaussian or when you're unsure about the distribution.\n",
    "   - Standardization is less affected by outliers compared to normalization, making it more suitable when your data contains outliers.\n",
    "   - If you're working with algorithms that assume your data is normally distributed, such as linear regression, logistic regression, standardization is typically preferred."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168163c2-5155-4c7d-8c57-2becf29317ec",
   "metadata": {},
   "source": [
    "In some cases, you might experiment with both approaches and see which one yields better results for your specific dataset and analysis. Additionally, it's always a good practice to understand your data and the underlying assumptions of the algorithms you're using to make informed decisions about data preprocessing techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5e2d3e",
   "metadata": {},
   "source": [
    "<script async src=\"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-8076040302380238\"\n",
    "     crossorigin=\"anonymous\"></script>\n",
    "<!-- inline_horizontal -->\n",
    "<ins class=\"adsbygoogle\"\n",
    "     style=\"display:block\"\n",
    "     data-ad-client=\"ca-pub-8076040302380238\"\n",
    "     data-ad-slot=\"9021194372\"\n",
    "     data-ad-format=\"auto\"\n",
    "     data-full-width-responsive=\"true\"></ins>\n",
    "<script>\n",
    "     (adsbygoogle = window.adsbygoogle || []).push({});\n",
    "</script>\n",
    "\n",
    "<p class=pb-1>\n",
    "To be among the first to hear about future updates, simply enter your email below, follow us on <a href=\"https://x.com/dataideaorg\"><i class=\"bi bi-twitter-x\"></i>\n",
    " (formally Twitter)</a>, or subscribe to our <a href=\"https://www.youtube.com/@dataideaorg\"><i class=\"bi bi-youtube\"></i> YouTube channel</a>.\n",
    "</p>\n",
    "<iframe src=\"https://embeds.beehiiv.com/5fc7c425-9c7e-4e08-a514-ad6c22beee74?slim=true\" data-test-id=\"beehiiv-embed\" height=\"52\" frameborder=\"0\" scrolling=\"no\" style=\"margin: 0; border-radius: 0px !important; background-color: transparent; width: 100%;\" ></iframe>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
