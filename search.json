[
  {
    "objectID": "Python-Data-Analysis/Week9-Time-Series/92_forecasting.html",
    "href": "Python-Data-Analysis/Week9-Time-Series/92_forecasting.html",
    "title": "DATAIDEA",
    "section": "",
    "text": "Time_Series_Header",
    "crumbs": [
      "Python-Data-Analysis",
      "Week9-Time-Series",
      "Time Series Forecasting"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week9-Time-Series/92_forecasting.html#time-series-forecasting",
    "href": "Python-Data-Analysis/Week9-Time-Series/92_forecasting.html#time-series-forecasting",
    "title": "DATAIDEA",
    "section": "Time Series Forecasting",
    "text": "Time Series Forecasting\nForecasting refers to the future predictions based on the time series data analysis. Below are the steps performed during time series forecasting\n\nStep 1: Understand the time series characteristics like trend, seasonality etc\nStep 2: Do the analysis and identify the best method to make the time series stationary\nStep 3: Note down the transformation steps performed to make the time series stationary and make sure that the reverse transformation of data is possible to get the original scale back\nStep 4: Based on data analysis choose the appropriate model for time series forecasting\nStep 5: We can assess the performance of a model by applying simple metrics such as residual sum of squares(RSS). Make sure to use whole data for prediction.\nStep 6: Now we will have an array of predictions which are in transformed scale. We just need to apply the reverse transformation to get the prediction values in original scale.\nStep 7: At the end we can do the future forecasting and get the future forecasted values in original scale.",
    "crumbs": [
      "Python-Data-Analysis",
      "Week9-Time-Series",
      "Time Series Forecasting"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week9-Time-Series/92_forecasting.html#models-used-for-time-series-forecasting",
    "href": "Python-Data-Analysis/Week9-Time-Series/92_forecasting.html#models-used-for-time-series-forecasting",
    "title": "DATAIDEA",
    "section": "Models Used For Time Series Forecasting",
    "text": "Models Used For Time Series Forecasting\n\nAutoregression (AR)\nMoving Average (MA)\nAutoregressive Moving Average (ARMA)\nAutoregressive Integrated Moving Average (ARIMA)\nSeasonal Autoregressive Integrated Moving-Average (SARIMA)\nSeasonal Autoregressive Integrated Moving-Average with Exogenous Regressors (SARIMAX)\nVector Autoregression (VAR)\nVector Autoregression Moving-Average (VARMA)\nVector Autoregression Moving-Average with Exogenous Regressors (VARMAX)\nSimple Exponential Smoothing (SES)\nHolt Winter’s Exponential Smoothing (HWES)\n\nNext part of this article we are going to analyze and forecast air passengers time series data using ARIMA model. Brief introduction of ARIMA model is as below\n[ad]",
    "crumbs": [
      "Python-Data-Analysis",
      "Week9-Time-Series",
      "Time Series Forecasting"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week9-Time-Series/92_forecasting.html#arima",
    "href": "Python-Data-Analysis/Week9-Time-Series/92_forecasting.html#arima",
    "title": "DATAIDEA",
    "section": "ARIMA",
    "text": "ARIMA\n\nARIMA stands for Auto-Regressive Integrated Moving Averages. It is actually a combination of AR and MA model.\nARIMA has three parameters ‘p’ for the order of Auto-Regressive (AR) part, ‘q’ for the order of Moving Average (MA) part and ‘d’ for the order of integrated part.\n\n\nAuto-Regressive (AR) Model:\n\nAs the name indicates, its the regression of the variables against itself. In this model linear combination of the past values are used to forecast the future values.\nTo figure out the order of AR model we will use PACF function\n\n\n\nIntegration(I):\n\nUses differencing of observations (subtracting an observation from observation at the previous time step) in order to make the time series stationary. Differencing involves the subtraction of the current values of a series with its previous values \\(d\\) number of times.\nMost of the time value of \\(d = 1\\), means first order of difference.\n\n\n\nMoving Average (MA) Model:\n\nRather than using past values of the forecast variable in a regression, a moving average model uses linear combination of past forecast errors\nTo figure out the order of MA model we will use ACF function\n\n[ad]",
    "crumbs": [
      "Python-Data-Analysis",
      "Week9-Time-Series",
      "Time Series Forecasting"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week9-Time-Series/93_python_example.html",
    "href": "Python-Data-Analysis/Week9-Time-Series/93_python_example.html",
    "title": "Time Series Analysis",
    "section": "",
    "text": "Photo by DATAIDEA",
    "crumbs": [
      "Python-Data-Analysis",
      "Week9-Time-Series",
      "Time Series Analysis"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week9-Time-Series/93_python_example.html#python-example",
    "href": "Python-Data-Analysis/Week9-Time-Series/93_python_example.html#python-example",
    "title": "Time Series Analysis",
    "section": "Python Example",
    "text": "Python Example\nWe have a monthly time series data of the air passengers from 1 Jan 1949 to 1 Dec 1960. Each row contains the air passenger number for a month of that particular year. Objective is to build a model to forecast the air passenger traffic for future months.",
    "crumbs": [
      "Python-Data-Analysis",
      "Week9-Time-Series",
      "Time Series Analysis"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week9-Time-Series/93_python_example.html#import-the-library",
    "href": "Python-Data-Analysis/Week9-Time-Series/93_python_example.html#import-the-library",
    "title": "Time Series Analysis",
    "section": "Import The Library ",
    "text": "Import The Library \n\ndataidea: allows us to quickly load most common packages and datasets for the course\nstatsmodels: Using statsmodels module classes and functions for time series analysis and forecasting\n\nadfuller: Augmented Dickey-Fuller\nACF: Auto Correlation Function\nPACF: Partial Auto Correlation Function\nARIMA: Autoregressive Integrated Moving Average ARIMA(p,d,q) Model\nsm.tsa.seasonal.seasonal_decompose: For decomposition of time series\n\nrcParams: To change the matplotlib properties like figure size\n\n\nfrom dataidea.packages import np, pd, plt, sns, sm\nfrom dataidea.datasets import loadDataset # allows us to load datasets\nfrom statsmodels.tsa.stattools import adfuller,acf, pacf\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom pylab import rcParams\n\n\n# Set plot size \nrcParams['figure.figsize'] = 10, 6\n\n\n# load the dataset\ndata = loadDataset('air_passengers')\n\n# Print the first 5 rows\ndata.head()\n\n\n\n\n\n\n\n\nMonth\nPassengers\n\n\n\n\n0\n1949-01\n112\n\n\n1\n1949-02\n118\n\n\n2\n1949-03\n132\n\n\n3\n1949-04\n129\n\n\n4\n1949-05\n121",
    "crumbs": [
      "Python-Data-Analysis",
      "Week9-Time-Series",
      "Time Series Analysis"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week9-Time-Series/93_python_example.html#understanding-the-data",
    "href": "Python-Data-Analysis/Week9-Time-Series/93_python_example.html#understanding-the-data",
    "title": "Time Series Analysis",
    "section": "Understanding The Data ",
    "text": "Understanding The Data \n\nDataframe data contains the time series data. There are two columns Month and Passengers. Month column contains the value of month in that year and passenger column contains the number of air passengers for that particular month.\nAs you may have noticed Month column datatype is Object, so we are going to convert it to datetime\nTo make plotting easier, we set the index of pandas dataframe data to the Month column so that it will act as x-axis & Passenger column as y-axis\n\n\n# convert month to datetime\ndata['Month'] = pd.to_datetime(data.Month)\n\n# set month as index\ndata = data.set_index(data.Month)\ndata.drop('Month', axis = 1, inplace = True)\n\n# look at the first 5 rows\ndata.head()\n\n\n\n\n\n\n\n\nPassengers\n\n\nMonth\n\n\n\n\n\n1949-01-01\n112\n\n\n1949-02-01\n118\n\n\n1949-03-01\n132\n\n\n1949-04-01\n129\n\n\n1949-05-01\n121",
    "crumbs": [
      "Python-Data-Analysis",
      "Week9-Time-Series",
      "Time Series Analysis"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week9-Time-Series/93_python_example.html#time-series-characteristics",
    "href": "Python-Data-Analysis/Week9-Time-Series/93_python_example.html#time-series-characteristics",
    "title": "Time Series Analysis",
    "section": "Time Series Characteristics ",
    "text": "Time Series Characteristics \n\nTrend\n\n# plt.figure(figsize= (10,6))\nplt.plot(data)\nplt.xlabel('Years')\nplt.ylabel('No of Air Passengers')\nplt.title('Trend of the Time Series')\nplt.show()\n\n\n\n\n\n\n\n\nAs you can see from above plot there is upward trend of number of passenger for every year.\n\n\nVariance\nIn above graph you can clearly see that the variation is also increasing with the level of the series. You will see in the later part of this exercise how we handle the variance to increase the stationarity of the series.\n\n\nSeasonality\nWe can also see the graph going up and down at regular interval, that is the sign of seasonality. Let’s plot the graph for few months to visualize for seasonality.\n\n# To plot the seasonality we are going to create a temp dataframe \n# and add columns for Month and Year values\ndata_temp = data.copy()\ndata_temp['Year'] = pd.DatetimeIndex(data_temp.index).year\ndata_temp['Month'] = pd.DatetimeIndex(data_temp.index).month\n\n# Stacked line plot\nplt.figure(figsize=(10,10))\nplt.title('Seasonality of the Time Series')\nsns.pointplot(x='Month',y='Passengers',hue='Year',data=data_temp)\n\n\n\n\n\n\n\n\nFrom above graph we can say that every year in month of July we observe maximum number of passengers and similarly minimum number of passenger in the month of November.\n[ad]\n\n\n\n\n\n\n\nDecomposition of Time Series\nLet’s now use the decomposition technique to deconstruct the time series data into several component like trend and seasonality for visualization of time series characteristics.\nHere we are going to use ‘additive’ model because it is quick to develop, fast to train, and provide interpretable patterns.\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\ndecomposition = seasonal_decompose(\n    data, \n    model='additive')\n \nfig = decomposition.plot()\n\n\n\n\n\n\n\n\n[ad]",
    "crumbs": [
      "Python-Data-Analysis",
      "Week9-Time-Series",
      "Time Series Analysis"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week9-Time-Series/93_python_example.html#time-series-analysis",
    "href": "Python-Data-Analysis/Week9-Time-Series/93_python_example.html#time-series-analysis",
    "title": "Time Series Analysis",
    "section": "Time Series Analysis ",
    "text": "Time Series Analysis \nSo our time series has variance, trend and seasonality characteristics. During our analysis we are going to try multiple techniques to make time series stationary and record the stationarity scores for each method. Finally, we will select the method, which is easy for inverse transformation easy and give best stationarity score.\n\nCheck for Stationarity\nWe are going to use rolling statistics and Dickey-Fuller test to check the stationarity of the time series\n\ndef stationarity_test(timeseries):\n    # Get rolling statistics for window = 12 i.e. yearly statistics\n    rolling_mean = timeseries.rolling(window=12).mean()\n    rolling_std = timeseries.rolling(window=12).std()\n    \n    # Plot rolling statistic\n    plt.figure(figsize= (10,6))\n    plt.xlabel('Years')\n    plt.ylabel('No of Air Passengers')    \n    plt.title('Stationary Test: Rolling Mean and Standard Deviation')\n    plt.plot(timeseries, color= 'blue', label= 'Original')\n    plt.plot(rolling_mean, color= 'green', label= 'Rolling Mean')\n    plt.plot(rolling_std, color= 'red', label= 'Rolling Std')   \n    plt.legend()\n    plt.show()\n    \n    # Dickey-Fuller test\n    print('Results of Dickey-Fuller Test')\n    df_test = adfuller(timeseries)\n    df_output = pd.Series(df_test[0:4], index = ['Test Statistic', 'p-value', '#Lags Used', 'Number of Observations Used'])\n    for key, value in df_test[4].items():\n        df_output['Critical Value (%s)' %key] = value\n    print(df_output)\n\n\n# Lets test the stationarity score with original series data\nstationarity_test(data)\n\n\n\n\n\n\n\n\nResults of Dickey-Fuller Test\nTest Statistic                   0.815369\np-value                          0.991880\n#Lags Used                      13.000000\nNumber of Observations Used    130.000000\nCritical Value (1%)             -3.481682\nCritical Value (5%)             -2.884042\nCritical Value (10%)            -2.578770\ndtype: float64\n\n\nThough it’s clear from visual observation that it’s not a stationary series, but still lets have look at the rolling statistics and Duckey Fuller test results\n\nRolling statistics: Standard deviation has very less variation but mean is increasing continuously.\nDuckey Fuller Test: Test statistic is way more than the critical values.\n\n[ad]\n\n\n\n\n\n\n\nConvert Non-Stationary Data to Stationary Data\nLet’s first use the differencing technique to obtain the stationarity.\n\nDifferencing\nTo transform the series using Differencing we will use the diff() method of pandas. A benefit of using the Pandas function, in addition to requiring less code, is that it maintains the date-time information for the differenced series\n\\[Y_t' = Y_t - Y_{t-1}\\]\n\ndata_diff = data.diff(periods = 1) # First order differencing\nplt.xlabel('Years')\nplt.ylabel('No of Air Passengers')    \nplt.title('Convert Non Stationary Data to Stationary Data using Differencing ')\nplt.plot(data_diff)\n\n\n\n\n\n\n\n\nSo from above graph its clear that differencing technique removed the trend from the time series, but variance is still there Now lets run the stationarity_test() to check the effectiveness of the ‘Differencing’ technique\n\ndata_diff.dropna(inplace = True)# Data transformation may add na values\nstationarity_test(data_diff)\n\n\n\n\n\n\n\n\nResults of Dickey-Fuller Test\nTest Statistic                  -2.829267\np-value                          0.054213\n#Lags Used                      12.000000\nNumber of Observations Used    130.000000\nCritical Value (1%)             -3.481682\nCritical Value (5%)             -2.884042\nCritical Value (10%)            -2.578770\ndtype: float64\n\n\nThe rolling values appear to be varying slightly, and we can see there is slight upward trend in standard deviation. Also, the test statistic is smaller than the 10% critical but since p-value is greater than 0.05 it is not a stationary series.\nNote that variance in the series is also affecting above results, which can be removed using transformation technique.\nLet’s also check with transformation technique\n\n\nTransformation\nSince variance is proportional to the levels, we are going to use the log transformation.\n\n# apply the log transformation\ndata_log = np.log(data)\n\n# let's plot the results\nplt.subplot(211)\nplt.plot(data, label= 'Time Series with Variance')\nplt.legend()\nplt.subplot(212)\nplt.plot(data_log, label='Time Series without Variance (Log Transformation)')\nplt.legend()  \nplt.show()\n\n\n\n\n\n\n\n\nSince log transformation has removed the variance from series, lets use this transformed data hence forward. Note that, Since we are using log transformation, we can use the exponential of the series to get the original scale back.\n    df = exp(df_log)\nLet cross-check the differencing method scores with this log transformed data again.\n\n# First order differencing\ndata_log_diff = data_log.diff(periods = 1) \n\n# Data transformation may add na values\ndata_log_diff.dropna(inplace = True)\nstationarity_test(data_log_diff)\n\n\n\n\n\n\n\n\nResults of Dickey-Fuller Test\nTest Statistic                  -2.717131\np-value                          0.071121\n#Lags Used                      14.000000\nNumber of Observations Used    128.000000\nCritical Value (1%)             -3.482501\nCritical Value (5%)             -2.884398\nCritical Value (10%)            -2.578960\ndtype: float64\n\n\nThe rolling mean and standard deviation values are okay now. The test statistic is smaller than the 10% critical values but since p-value is greater than 0.05 it is not a stationary series.\nLet’s also check with Moving Average technique…\n\n\nMoving Average\nSince we have time series data from 1 Jan 1949 to 1 Dec 1960, we will define a yearly window for moving average. Window size = 12. Note that we are going to use Log transformed data.\n\ndata_log_moving_avg = data_log.rolling(window = 12).mean()\n\n# let's plot the results\nplt.xlabel('Years')\nplt.ylabel('No of Air Passengers')    \nplt.title('Convert Non Stationary Data to Stationary Data using Moving Average')\nplt.plot(data_log, color= 'blue', label='Orignal')\nplt.plot(data_log_moving_avg, color= 'red', label='Moving Average')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nAs you can see from above graph that data is more smooth without any variance. If we use the differencing technique with log transformed data and mean average data then we should get better stationarity scores\n\ndata_log_moving_avg_diff = data_log - data_log_moving_avg\ndata_log_moving_avg_diff.dropna(inplace = True)\nstationarity_test(data_log_moving_avg_diff)\n\n\n\n\n\n\n\n\nResults of Dickey-Fuller Test\nTest Statistic                  -3.162908\np-value                          0.022235\n#Lags Used                      13.000000\nNumber of Observations Used    119.000000\nCritical Value (1%)             -3.486535\nCritical Value (5%)             -2.886151\nCritical Value (10%)            -2.579896\ndtype: float64\n\n\nAs expected now we are able to see some real improvements. p-value is less than 0.05 that means our series is stationary, but we can only say this with 95% of confidence, as test statistics is less than 5% critical value.\nIn order to increase the stationarity of the series lets try to use ‘Weighted Moving Average’ technique\n\n\nWeighted Moving Average (WMA)\nHere we are going to use exponentially weighted moving average with parameter ‘halflife = 12’. This parameter defines the amount of exponential decay. This is just an assumption here and would depend largely on the business domain.\n\ndata_log_weighted_avg = data_log.ewm(halflife = 12).mean()\nplt.plot(data_log)\nplt.plot(data_log_weighted_avg, color = 'red')\n\n\n\n\n\n\n\n\nNotice that WMA follow’s no of passenger values more closely than a corresponding Simple Moving Average which also results in more accurate trend direction. Now lets check, the effect of this on stationarity scores!\n\ndata_log_weighted_avg_diff = data_log - data_log_weighted_avg\nstationarity_test(data_log_weighted_avg_diff)\n\n\n\n\n\n\n\n\nResults of Dickey-Fuller Test\nTest Statistic                  -3.601262\np-value                          0.005737\n#Lags Used                      13.000000\nNumber of Observations Used    130.000000\nCritical Value (1%)             -3.481682\nCritical Value (5%)             -2.884042\nCritical Value (10%)            -2.578770\ndtype: float64\n\n\nTest statistic is smaller than the 1% critical value, which is better than the previous case. Note that in this case there will be no missing values as all values from starting are given weights. So it’ll work even with no previous values.\nThere is one more way to obtain better stationarity is by using the residual data from time series decomposition.\n\n\nDecomposition of Time Series\nLet’s now use the decomposition technique to deconstruct the log transformed time series data, so that we can check the stationarity using residual data.\n\nimport statsmodels.api as smapi\n\ndecomposition = smapi.tsa.seasonal_decompose(data_log,period =12)\nfig = decomposition.plot()\n\n\n\n\n\n\n\n\nHere we can see that the trend and seasonality are separated out from log transformed data, and we can now check the stationarity of the residuals\n\ndata_log_residual = decomposition.resid\ndata_log_residual.dropna(inplace = True)\nstationarity_test(data_log_residual)\n\n\n\n\n\n\n\n\nResults of Dickey-Fuller Test\nTest Statistic                -6.332387e+00\np-value                        2.885059e-08\n#Lags Used                     9.000000e+00\nNumber of Observations Used    1.220000e+02\nCritical Value (1%)           -3.485122e+00\nCritical Value (5%)           -2.885538e+00\nCritical Value (10%)          -2.579569e+00\ndtype: float64\n\n\nThe Dickey-Fuller test statistic is significantly lower than the 1% critical value and p-value is almost 0. So this time series is very close to stationary. This concludes our time series analysis and data transformation to get the stationary series. Now we can start modeling it for forecast.\n[ad]",
    "crumbs": [
      "Python-Data-Analysis",
      "Week9-Time-Series",
      "Time Series Analysis"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week9-Time-Series/93_python_example.html#forecasting",
    "href": "Python-Data-Analysis/Week9-Time-Series/93_python_example.html#forecasting",
    "title": "Time Series Analysis",
    "section": "Forecasting",
    "text": "Forecasting\n\nThough using residual values gives us very good results, but it’s relatively difficult to add noise and seasonality back into predicted residuals in this case.\nSo we are going to make model on the time series(df_log_diff), where we have used log transformation and differencing technique. This is one of the most popular and beginner-friendly technique. As per our time series analysis df_log_diff is not a perfectly stationary series, that’s why we are going to use statistical models like ARIMA to forecast the data.\nRemember that ARIMA model uses three parameters, p for the order of Auto-Regressive (AR) part, q for the order of Moving Average (MA) part and d for the order of integrated part. We are going to use d=1 but to find the value for p and q lets plot ACF and PACF.\nNote that since we are using d=1, first order of differencing will be performed on given series. Since first value of time series don’t have any value to subtract from resulting series will have one less value from original series\n\n\nACF and PACF Plots\n\nTo figure out the order of AR model(p) we will use PACF function. p = the lag value where the PACF chart crosses the upper confidence interval for the first time\nTo figure out the order of MA model(q) we will use ACF function. q = the lag value where the ACF chart crosses the upper confidence interval for the first time\n\n\nlag_acf = acf(data_log_diff, nlags=20)\nlag_pacf = pacf(data_log_diff, nlags=20, method='ols')\n\n# Plot ACF: \nplt.subplot(121) \nplt.plot(lag_acf)\nplt.axhline(y=0,linestyle='--',color='gray')\n# Draw 95% confidence interval line\nplt.axhline(y=-1.96/np.sqrt(len(data_log_diff)),linestyle='--',color='red')\nplt.axhline(y=1.96/np.sqrt(len(data_log_diff)),linestyle='--',color='red')\nplt.xlabel('Lags')\nplt.title('Autocorrelation Function')\n\n#Plot PACF:\nplt.subplot(122)\nplt.plot(lag_pacf)\nplt.axhline(y=0,linestyle='--',color='gray')\n# Draw 95% confidence interval line\nplt.axhline(y=-1.96/np.sqrt(len(data_log_diff)),linestyle='--',color='red')\nplt.axhline(y=1.96/np.sqrt(len(data_log_diff)),linestyle='--',color='red')\nplt.xlabel('Lags')\nplt.title('Partial Autocorrelation Function')\nplt.tight_layout()\n\n\n\n\n\n\n\n\nFrom above graph its clear that p=2 and q=2. Now we have the ARIMA parameters values, lets make 3 different ARIMA models considering individual as well as combined effects. We will also print the RSS(Residual Sum of Square) metric for each. Please note that here RSS is for the values of residuals and not actual series.\n\n\nAR Model\nSince q is MA model parameter we will keep its value as 0.\n\n# # freq = 'MS' &gt; The frequency of the time-series MS = calendar month begin\n# # The (p,d,q) order of the model for the number of AR parameters, differences, and MA parameters to use\n# model = ARIMA(data_log, order=(2, 1, 0), freq = 'MS')  \n# results_AR = model.fit()\n# plt.plot(data_log_diff)\n# plt.plot(results_AR.fittedvalues, color='red')\n# plt.title('AR Model, RSS: %.4f'% sum((results_AR.fittedvalues - data_log_diff['Passengers'])**2))\n# plt.show()\n\n\n\nMA Model\nSince ‘p’ is AR model parameter we will keep its value as ‘0’.\n\n# model = ARIMA(data_log, order=(0, 1, 2), freq = 'MS')  \n# results_MA = model.fit()  \n# plt.plot(data_log_diff)\n# plt.plot(results_MA.fittedvalues, color='red')\n# plt.title('MA Model, RSS: %.4f'% sum((results_MA.fittedvalues-data_log_diff['Passengers'])**2))\n\n\n\nCombined Model\n\n# model = ARIMA(data_log, order=(2, 1, 2), freq = 'MS')  \n# results_ARIMA = model.fit(disp=-1)  \n# plt.plot(data_log_diff)\n# plt.plot(results_ARIMA.fittedvalues, color='red')\n# plt.title('Combined Model, RSS: %.4f'% sum((results_ARIMA.fittedvalues-data_log_diff['Passengers'])**2))\n\nHere we can see that the AR and MA models have almost the same RSS score but combined is significantly better. So we will go ahead with combined ARIMA model and use it for predictions.\n\n\nPrediction and Reverse Transformation\n\nWe will create a separate series of predicted values using ARIMA model\nReverse transform the predicted values to get the original scale back\nCompare the predicted values with original values and plot them\n\n\n# # Create a separate series of predicted values\n# predictions_diff = pd.Series(results_ARIMA.fittedvalues, copy=True)\n\n# print('Total no of predictions: ', len(predictions_diff))\n# predictions_diff.head()\n\nSince we are using first order of differencing(d =1), there is no prediction available for first value (1949-02-01) of original series. In order to remove ‘differencing transformation’ from the prediction values we are going to add these differences consecutively to the base number. An easy way to do it is to first determine the cumulative sum at index and then add it to the base number. We are going to use pandas cumsum() function for it.\n\n# predictions_diff_cumsum = predictions_diff.cumsum()\n# predictions_diff_cumsum.head()\n\nAbove values once added to the base number will completely remove the differencing transformation. For this, lets create a series with all values as base number and add the ‘predictions_diff_cumsum’ to it.\n\n# predictions_log = pd.Series(df_log['Passengers'].iloc[0], index=df_log.index) # Series of base number\n# predictions_log = predictions_log.add(predictions_diff_cumsum,fill_value=0)\n# predictions_log.head()\n\nSo as of now we have removed the differencing transformation, now lets remove the log transformation to get the original scale back.\n\n# predictions = np.exp(predictions_log)\n# plt.plot(df)\n# plt.plot(predictions)\n\n\n# df_predictions =pd.DataFrame(predictions, columns=['Predicted Values'])\n# pd.concat([df,df_predictions],axis =1).T\n\n\n\nFuture Forecasting\n\nWe have data from 1 Jan 1949 to 1 Dec 1960. 12 years of data with passenger number observation for each month i.e. 144 total observations.\nIf we want to forecast for next 5 years or 60 months then, ‘end’ count will be &gt; 144 + 60 = 204.\nWe are going to use statsmodels plot_predict() method for it\n\n\n# results_ARIMA.plot_predict(start = 1, end= 204)\n\n\n# # Forecasted values in original scale will be\n# forecast_values_log_scale = results_ARIMA.forecast(steps = 60)\n# forecast_values_original_scale = np.exp(forecast_values_log_scale[0])\n\n# forecast_date_range= pd.date_range(\"1961-01-01\", \"1965-12-01\", freq=\"MS\")\n\n# df_forecast =pd.DataFrame(forecast_values_original_scale, columns=['Forecast'])\n# df_forecast['Month'] = forecast_date_range\n\n# df_forecast[['Month', 'Forecast']]\n\n\nTo be among the first to hear about future updates, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel.\n\n\n\n\n\n\n\n\n[ad]",
    "crumbs": [
      "Python-Data-Analysis",
      "Week9-Time-Series",
      "Time Series Analysis"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week6-Visualization2/62_seaborn_part2.html",
    "href": "Python-Data-Analysis/Week6-Visualization2/62_seaborn_part2.html",
    "title": "DATAIDEA",
    "section": "",
    "text": "Photo by DATAIDEA",
    "crumbs": [
      "Python-Data-Analysis",
      "Week6-Visualization2",
      "Seaborn Part 2"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week6-Visualization2/62_seaborn_part2.html#seaborn-part-2",
    "href": "Python-Data-Analysis/Week6-Visualization2/62_seaborn_part2.html#seaborn-part-2",
    "title": "DATAIDEA",
    "section": "Seaborn Part 2",
    "text": "Seaborn Part 2\nThe Python visualization library Seaborn is based on matplotlib and provides a high-level interface for drawing attractive statistical graphics.\nMake use of the following aliases to import the libraries:\n\nfrom dataidea.packages import plt, sns, np, pd\n\nThe basic steps to creating plots with Seaborn are\n\nPrepare some data\nControl figure aesthetics\nPlot with Seaborn\nFurther customize your plot\n\n\ntips = sns.load_dataset('tips')\n\n\ntips.head()\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4\n\n\n\n\n\n\n\n\nsns.set_style(\"whitegrid\")\n\n\ng = sns.lmplot(x=\"tip\", y=\"total_bill\", data=tips, aspect=2)\ng.set_axis_labels(\"Tip\",\"Total bill(USD)\")\nplt.show()",
    "crumbs": [
      "Python-Data-Analysis",
      "Week6-Visualization2",
      "Seaborn Part 2"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week6-Visualization2/62_seaborn_part2.html#data",
    "href": "Python-Data-Analysis/Week6-Visualization2/62_seaborn_part2.html#data",
    "title": "DATAIDEA",
    "section": "Data",
    "text": "Data\nSeaborn also offers built-in data sets:\n\nuniform_data = np.random.rand(10, 12)\n\n\ndata = pd.DataFrame({'x':np.arange(1,101),\n'y':np.random.normal(0,4,100)})\n\n\ntitanic = sns.load_dataset(\"titanic\")\niris = sns.load_dataset(\"iris\")\n\n\nfig, ax = plt.subplots()",
    "crumbs": [
      "Python-Data-Analysis",
      "Week6-Visualization2",
      "Seaborn Part 2"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week6-Visualization2/62_seaborn_part2.html#plotting-with-seaborn",
    "href": "Python-Data-Analysis/Week6-Visualization2/62_seaborn_part2.html#plotting-with-seaborn",
    "title": "DATAIDEA",
    "section": "Plotting with Seaborn",
    "text": "Plotting with Seaborn\n\nAxis Grids\n\n# Subplot grid for plotting conditional relationships\ng = sns.FacetGrid(titanic, col=\"survived\", row=\"sex\")\ng = g.map(plt.hist,\"age\")\n#Draw a categorical plot onto a Facetgrid\n\n\n\n\n\n\n\n\nSubplot grid for plotting pairwise relationships\n\nh = sns.PairGrid(iris)\nh = h.map(plt.scatter)\n\n\n\n\n\n\n\n\n\nsns.pairplot(iris)\nplt.show()\n\n\n\n\n\n\n\n\nGrid for bivariate plot with marginal univariate plots\n\ni = sns.JointGrid(x=\"x\",\ny=\"y\",\ndata=data)\ni = i.plot(sns.regplot,\nsns.distplot)\n\n/home/jumashafara/venvs/programming_for_data_science/lib/python3.10/site-packages/seaborn/axisgrid.py:1886: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  func(self.x, **orient_kw_x, **kwargs)\n/home/jumashafara/venvs/programming_for_data_science/lib/python3.10/site-packages/seaborn/axisgrid.py:1892: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  func(self.y, **orient_kw_y, **kwargs)\n\n\n\n\n\n\n\n\n\n\nsns.lmplot(data=iris[iris.species == 'setosa'], y='sepal_length', x='sepal_width')\n\n\n\n\n\n\n\n\n\n# Plot data and regression model fitsacross a FacetGrid\nsns.lmplot(x=\"sepal_width\",\ny=\"sepal_length\",\nhue=\"species\",\nx_ci = 'sd',\ndata=iris)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCategorical Plots\n\nScatterplot\nScatterplot with one categorical variable\n\nsns.stripplot(x=\"species\",\ny=\"petal_length\",\ndata=iris, hue='species')\nplt.show()\n\n\n\n\n\n\n\n\nIn this situation we are able to visualize the distribution of the petal lengths for each of the classes in the species(categorical) column.\nForexample we can observe that setosa has petal lengths lying between 1 and 2 units, whereas versicolor has petal lengths lying between 3 and 5 units inclusive and so on.\nWe can also quickly observe that virginica petals are generally longest(4.5-7) compared to the other two ie setosa(1-2) and versicolor(3-5)\nCategorical scatterplot with non-overlapping points\n\nsns.swarmplot(x=\"species\",\ny=\"petal_length\",\ndata=iris, hue='species')\nplt.show()\n\n/home/jumashafara/venvs/programming_for_data_science/lib/python3.10/site-packages/seaborn/categorical.py:3399: UserWarning: 12.0% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n\n\n\n\n\n\n\n\n\n\n\nBar Chart\nShow point estimates and confidence intervals with scatterplot glyphs\n\nsns.barplot(x=\"sex\",\ny=\"survived\",\nhue=\"class\",\npalette='Greens_d',\ndata=titanic)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCount Plot\nShow count of observations\n\nsns.countplot(\n    x=\"deck\", \n    data=titanic, \n    palette=\"Greens_d\", \n    hue='survived')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nPoint Plot\nShow point estimates and confidence intervals as rectangular bars\n\nsns.pointplot(x=\"class\", y=\"survived\", hue=\"sex\", data=titanic,\n            palette={\"male\":\"g\",\"female\":\"b\"}, markers=[\"^\",\"o\"], linestyles=[\"-\",\"--\"])\nplt.show()\n\n\n\n\n\n\n\n\n\n\nBoxplot\n\nsns.boxplot(x=\"alive\", y=\"age\", hue=\"adult_male\", data=titanic)\nplt.show()\n\n\n\n\n\n\n\n\nThe boxplots help us visualize the distribution of the titanic passengers conditionally ie, we can see the distribution men and women given that they’re alive or not alive\n\nsns.boxplot(data=iris, orient=\"h\")\nplt.show()\n\n\n\n\n\n\n\n\nAbove we can observe the distribution of the features in the entire dataset by passing in the entire dataset as the value for the data parameter.\n\n\nViolin Plot\n\nsns.violinplot(x=\"age\",\ny=\"sex\",\nhue=\"survived\",\ndata=titanic)\n\n\n\n\n\n\n\n\n\n\n\nDistribution Plots\nPlot univariate distribution\n\nplot = sns.displot(data.y, kde=True, color=\"g\")\n\n\n\n\n\n\n\n\n\n\nMatrix Plots\nHeatmap\n\n# Exclude non-numeric columns from correlation calculation\nnumeric_cols = tips.select_dtypes(include=['float64', 'int64'])\ncorrelation_matrix = numeric_cols.corr()\n\n# Plotting the heatmap\nsns.heatmap(correlation_matrix, annot=True)\nplt.title('Correlation Heatmap')\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.show()\nplt.savefig(\"foo.png\")\n\n# You can save a transparent figure\nplt.savefig(\"foo.png\", transparent=True)\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\nfig3, ax = plt.subplots()\n# Create the regression plot\nsns.regplot(x=\"petal_width\", y=\"petal_length\", data=iris, ax=ax)\n\n# Display the plot\nplt.show()\n\n\n\n\n\n\n\n\nA few ads maybe displayed for income as resources are now offered freely. 🤝🤝🤝",
    "crumbs": [
      "Python-Data-Analysis",
      "Week6-Visualization2",
      "Seaborn Part 2"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week6-Visualization2/61_matplotlib.html",
    "href": "Python-Data-Analysis/Week6-Visualization2/61_matplotlib.html",
    "title": "DATAIDEA",
    "section": "",
    "text": "Photo by DATAIDEA",
    "crumbs": [
      "Python-Data-Analysis",
      "Week6-Visualization2",
      "Matplotlib"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week6-Visualization2/61_matplotlib.html#matplotlib",
    "href": "Python-Data-Analysis/Week6-Visualization2/61_matplotlib.html#matplotlib",
    "title": "DATAIDEA",
    "section": "Matplotlib",
    "text": "Matplotlib\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom dataidea.datasets import loadDataset\n\n\n# Load the Titanic dataset\ndemo_df = loadDataset('../assets/demo_cleaned.csv', inbuilt=False, file_type='csv')\n\n\nCreate a plot\n\n?fig.add_axes\n\nSignature: fig.add_axes(*args, **kwargs)\nDocstring:\nAdd an `~.axes.Axes` to the figure.\n\nCall signatures::\n\n    add_axes(rect, projection=None, polar=False, **kwargs)\n    add_axes(ax)\n\nParameters\n----------\nrect : tuple (left, bottom, width, height)\n    The dimensions (left, bottom, width, height) of the new\n    `~.axes.Axes`. All quantities are in fractions of figure width and\n    height.\n\nprojection : {None, 'aitoff', 'hammer', 'lambert', 'mollweide', 'polar', 'rectilinear', str}, optional\n    The projection type of the `~.axes.Axes`. *str* is the name of\n    a custom projection, see `~matplotlib.projections`. The default\n    None results in a 'rectilinear' projection.\n\npolar : bool, default: False\n    If True, equivalent to projection='polar'.\n\naxes_class : subclass type of `~.axes.Axes`, optional\n    The `.axes.Axes` subclass that is instantiated.  This parameter\n    is incompatible with *projection* and *polar*.  See\n    :ref:`axisartist_users-guide-index` for examples.\n\nsharex, sharey : `~matplotlib.axes.Axes`, optional\n    Share the x or y `~matplotlib.axis` with sharex and/or sharey.\n    The axis will have the same limits, ticks, and scale as the axis\n    of the shared axes.\n\nlabel : str\n    A label for the returned Axes.\n\nReturns\n-------\n`~.axes.Axes`, or a subclass of `~.axes.Axes`\n    The returned axes class depends on the projection used. It is\n    `~.axes.Axes` if rectilinear projection is used and\n    `.projections.polar.PolarAxes` if polar projection is used.\n\nOther Parameters\n----------------\n**kwargs\n    This method also takes the keyword arguments for\n    the returned Axes class. The keyword arguments for the\n    rectilinear Axes class `~.axes.Axes` can be found in\n    the following table but there might also be other keyword\n    arguments if another projection is used, see the actual Axes\n    class.\n\n    Properties:\n    adjustable: {'box', 'datalim'}\n    agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array and two offsets from the bottom left corner of the image\n    alpha: scalar or None\n    anchor: (float, float) or {'C', 'SW', 'S', 'SE', 'E', 'NE', ...}\n    animated: bool\n    aspect: {'auto', 'equal'} or float\n    autoscale_on: bool\n    autoscalex_on: unknown\n    autoscaley_on: unknown\n    axes_locator: Callable[[Axes, Renderer], Bbox]\n    axisbelow: bool or 'line'\n    box_aspect: float or None\n    clip_box: `~matplotlib.transforms.BboxBase` or None\n    clip_on: bool\n    clip_path: Patch or (Path, Transform) or None\n    facecolor or fc: color\n    figure: `~matplotlib.figure.Figure`\n    frame_on: bool\n    gid: str\n    in_layout: bool\n    label: object\n    mouseover: bool\n    navigate: bool\n    navigate_mode: unknown\n    path_effects: list of `.AbstractPathEffect`\n    picker: None or bool or float or callable\n    position: [left, bottom, width, height] or `~matplotlib.transforms.Bbox`\n    prop_cycle: `~cycler.Cycler`\n    rasterization_zorder: float or None\n    rasterized: bool\n    sketch_params: (scale: float, length: float, randomness: float)\n    snap: bool or None\n    subplotspec: unknown\n    title: str\n    transform: `~matplotlib.transforms.Transform`\n    url: str\n    visible: bool\n    xbound: (lower: float, upper: float)\n    xlabel: str\n    xlim: (left: float, right: float)\n    xmargin: float greater than -0.5\n    xscale: unknown\n    xticklabels: unknown\n    xticks: unknown\n    ybound: (lower: float, upper: float)\n    ylabel: str\n    ylim: (bottom: float, top: float)\n    ymargin: float greater than -0.5\n    yscale: unknown\n    yticklabels: unknown\n    yticks: unknown\n    zorder: float\n\nNotes\n-----\nIn rare circumstances, `.add_axes` may be called with a single\nargument, an Axes instance already created in the present figure but\nnot in the figure's list of Axes.\n\nSee Also\n--------\n.Figure.add_subplot\n.pyplot.subplot\n.pyplot.axes\n.Figure.subplots\n.pyplot.subplots\n\nExamples\n--------\nSome simple examples::\n\n    rect = l, b, w, h\n    fig = plt.figure()\n    fig.add_axes(rect)\n    fig.add_axes(rect, frameon=False, facecolor='g')\n    fig.add_axes(rect, polar=True)\n    ax = fig.add_axes(rect, projection='polar')\n    fig.delaxes(ax)\n    fig.add_axes(ax)\nFile:      ~/venvs/dataidea/lib/python3.10/site-packages/matplotlib/figure.py\nType:      method\n\n\n\nfig = plt.figure()\n# All plotting is done with respect to an Axes. \nfig.add_axes([0.1, 0.1, 0.5, 0.5])\n\n\n\n\n\n\n\n\nIn most cases, a subplot will fit your needs. A subplot is an axes on a grid system.\n\nfig1, ax = plt.subplots()\nax.hist(demo_df.income)\nplt.show()\n\n\n\n\n\n\n\n\n\nfig2, ax = plt.subplots()\nax.hist(demo_df.income)\n# plt.grid(True)\nax2 = fig2.add_subplot(222) # row-col-num\nax2.hist(demo_df['age'])\nplt.show()\n\n\n\n\n\n\n\n\n\nfig3, axes = plt.subplots(nrows=2,ncols=2)\n\n\n\n\n\n\n\n\n\nfig4, axes = plt.subplots(nrows=2,ncols=2)\n# add bar graph\ngender_counts = demo_df.gender.value_counts()\naxes[0,0].bar(gender_counts.index, gender_counts.values)\n# add histogram\naxes[0,1].hist(demo_df.age, bins=20, edgecolor='black')\n# # add box plot\naxes[1, 0].boxplot(demo_df.income, vert=0)\n# # add scatter plot\naxes[1, 1].scatter(demo_df.age, demo_df.income)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n2D Data or Images\n\nfrom PIL import Image\n\nimage = Image.open('../assets/dataidea-logo.png')\n\nfig4, ax = plt.subplots()\nax.imshow(image)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nSave Figure\n\nfig4, axes = plt.subplots(nrows=2,ncols=2)\n# add bar graph\ngender_counts = demo_df.gender.value_counts()\naxes[0,0].bar(gender_counts.index, gender_counts.values)\n# add histogram\naxes[0,1].hist(demo_df.age, bins=20, edgecolor='black')\n# add box plot\naxes[1, 0].boxplot(demo_df.income, vert=0)\n# add scatter plot\naxes[1, 1].scatter(demo_df.age, demo_df.income)\n\nplt.savefig('figure.pdf')\nplt.show()\n\n\n\n\n\n\n\n\nA few ads maybe displayed for income as resources are now offered freely. 🤝🤝🤝",
    "crumbs": [
      "Python-Data-Analysis",
      "Week6-Visualization2",
      "Matplotlib"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week4-ML-Intro/42_training_models_meaning.html",
    "href": "Python-Data-Analysis/Week4-ML-Intro/42_training_models_meaning.html",
    "title": "Training a model: Meaning",
    "section": "",
    "text": "For one independent variable (feature)\n\nUnderstanding Linear Regression:\n\nLinear regression is a statistical method used to model the relationship between a dependent variable (often denoted as \\(y\\)) and one or more independent variables (often denoted as \\(x\\)).\nThe relationship is modeled as a straight line equation: \\(y = mx + b\\), where \\(m\\) is the slope of the line and \\(b\\) is the y-intercept.\n\nFitting a Model:\n\nWhen we say we’re “fitting a model” in the context of linear regression, it means we’re determining the best-fitting line (or plane in higher dimensions) that represents the relationship between the independent variable(s) and the dependent variable.\nThis process involves finding the values of \\(m\\) and \\(b\\) that minimize the difference between the actual observed values of the dependent variable and the values predicted by the model.\nIn simpler terms, fitting a model means finding the line that best describes the relationship between our input data and the output we want to predict.\n\nPython Code Example:\n\nHere’s a simple example of how you might fit a linear regression model using Python, particularly with the scikit-learn library:\n\n\n\n\n\n\n\n\n# Importing necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n\n# Creating sample data\n # Independent variable (feature)\nX = np.array([[1], [2], [3], [4], [5]]) \n\n# Dependent variable (target)\ny = np.array([2, 4, 5, 4, 5])\n\n\n# Creating a linear regression model\nlinear_regression_model = LinearRegression()\n\n# Fitting the linear_regression_model to our data\nlinear_regression_model.fit(X, y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression() \n\n\n\n# Printing the slope (coefficient) and intercept of the best-fitting line\nprint(\"Slope (m):\", linear_regression_model.coef_[0])\nprint(\"Intercept (b):\", linear_regression_model.intercept_)\n\nSlope (m): 0.6\nIntercept (b): 2.2\n\n\n\n# Predictions\ny_predicted = linear_regression_model.predict(X)\n\n\ny_predicted\n\narray([2.8, 3.4, 4. , 4.6, 5.2])\n\n\n\nIn this code:\n\nX represents the independent variable (feature), which is a column vector in this case.\ny represents the dependent variable (target).\nWe create a LinearRegression model object.\nWe fit the model to our data using the .fit() method.\nFinally, we print out the slope (coefficient) and intercept of the best-fitting line.\n\n\n\n\n\n\n\nLet’s do some visualization\n\n# Plotting the linear regression line\nplt.scatter(X, y, color='blue', label='Data Points')\nplt.plot(X, y_predicted, color='red', label='Linear Regression Line')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Linear Regression')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nSo, in summary, “fitting a model” means finding the best parameters (like slope and intercept in the case of linear regression) that describe the relationship between our input data and the output we want to predict.\n\n\n\n\n\n\n\nFor Multiple Independent Variables\nIf we have multiple independent variables (features) in \\(X\\), the process is still the same, but the equation becomes more complex. This is known as multiple linear regression.\nHere’s how it works:\nHere are the corrected texts with LaTeX formulas suitable for Jupyter notebooks:\n\nUnderstanding Multiple Linear Regression:\n\nInstead of a single independent variable \\(x\\), we have multiple independent variables represented as a matrix \\(X\\).\nThe relationship between the dependent variable \\(y\\) and the independent variables \\(X\\) is modeled as: \\[y = b_0 + b_1 x_1 + b_2 x_2 + \\cdots + b_n x_n\\] where \\(b_0\\) is the intercept, \\(b_1, b_2, \\ldots, b_n\\) are the coefficients corresponding to each independent variable \\(x_1, x_2, \\ldots, x_n\\).\n\nFitting a Model with Many Variables:\n\nFitting the model involves finding the values of the coefficients \\(b_0, b_1, \\ldots, b_n\\) that minimize the difference between the actual observed values of the dependent variable and the values predicted by the model.\nThe process is essentially the same as in simple linear regression, but with more coefficients to estimate.\n\nPython Code Example:\n\nHere’s how you might fit a multiple linear regression model using Python:\n\n\n\n\n\n\n\n\n# Creating sample data with multiple variables\n# Independent variables (features)\nX = np.array([[1, 2], [2, 4], [3, 6], [4, 8], [5, 10]])  \n\n# Dependent variable (target)\ny = np.array([2, 4, 5, 4, 5])\n\n\n# Creating a multiple linear regression model\nmultiple_linear_regression_model = LinearRegression()\n\n# Fitting the multiple_linear_regression_model to our data\nmultiple_linear_regression_model.fit(X, y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression() \n\n\n\n# Printing the coefficients (intercept and slopes) of the best-fitting line\nprint(\"Intercept (b0):\", multiple_linear_regression_model.intercept_)\nprint(\"Coefficients (b1, b2):\", multiple_linear_regression_model.coef_)\n\nIntercept (b0): 2.200000000000001\nCoefficients (b1, b2): [0.12 0.24]\n\n\n\nIn this code:\n\nX represents the independent variables (features), where each row is a data point and each column represents a different feature.\ny represents the dependent variable (target).\nWe create a LinearRegression model object.\nWe fit the model to our data using the .fit() method.\nFinally, we print out the intercept and coefficients of the best-fitting line.\n\n\nSo, fitting a model with many variables involves finding the best parameters (coefficients) that describe the relationship between our input data (multiple independent variables) and the output we want to predict.\n\n\n\n\n\n\n\nWhat about Logistic Regression?\nLogistic regression is a type of regression analysis used for predicting the probability of a binary outcome based on one or more predictor variables. Here’s how the fitting process works with logistic regression:\n\nUnderstanding Logistic Regression:\n\nLogistic regression models the probability that a given input belongs to a particular category (binary classification problem).\nInstead of fitting a straight line or plane like in linear regression, logistic regression uses the logistic function (also known as the sigmoid function) to model the relationship between the independent variables and the probability of the binary outcome.\nThe logistic function is defined as:\n\n\\[P(y=1 \\,|\\, X) = \\frac{1}{1 + e^{-(b_0 + b_1x_1 + ... + b_nx_n)}}\\] where \\(P(y=1 \\,|\\, X)\\) is the probability of the positive outcome given the input \\(X\\), \\(b_0\\) is the intercept, \\(b_1, b_2, ..., b_n\\) are the coefficients, and \\(x_1, x_2, ..., x_n\\) are the independent variables.\n\n\n# z = b_0 + b_1x_1 + ... + b_nx_n\nsigmoid = lambda z: 1 / (1 + np.exp(-z))\n\n\ndef sigmoid(z):\n    prob = 1 / (1 + np.exp(-z))\n    return prob\n\n\nprint('z = 1000000:', sigmoid(z=1000000))\nprint('z = 0.0000001:', sigmoid(z=0.0000001))\n\nz = 1000000: 1.0\nz = 0.0000001: 0.50000025\n\n\n\n\n\n\n\n\nFitting a Logistic Regression Model:\n\nFitting the logistic regression model involves finding the values of the coefficients \\(b_0, b_1, ..., b_n\\) that maximize the likelihood of observing the given data under the assumed logistic regression model.\nThis is typically done using optimization techniques such as gradient descent or other optimization algorithms.\nThe goal is to find the set of coefficients that best separates the two classes or minimizes the error between the predicted probabilities and the actual binary outcomes in the training data.\n\nPython Code Example:\n\nHere’s how you might fit a logistic regression model using Python with the scikit-learn library:\n\n\n\n# import the logistic regression model\nfrom sklearn.linear_model import LogisticRegression\n\n\n# Creating sample data\nX = np.array([[1], [2], [3], [4], [5], [6], [7], [8], [9]])  # Independent variable\ny = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1])             # Binary outcome (0 or 1)\n\n\n# Creating a logistic regression model\nlogistic_regression_model = LogisticRegression()\n\n# Fitting the model to our data\nlogistic_regression_model.fit(X, y)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression() \n\n\n\n# Printing the intercept and coefficient(s) of the best-fitting logistic curve\nprint(\"Intercept (b0):\", logistic_regression_model.intercept_)\nprint(\"Coefficient (b1):\", logistic_regression_model.coef_)\n\nIntercept (b0): [-5.29559243]\nCoefficient (b1): [[1.17808562]]\n\n\n\nIn this code:\n\nX represents the independent variable.\ny represents the binary outcome.\nWe create a LogisticRegression model object.\nWe fit the model to our data using the .fit() method.\nFinally, we print out the intercept and coefficient(s) of the best-fitting logistic curve.\n\n\n\n# Predicted probabilities\nprobabilities = logistic_regression_model.predict_proba(X)\n\n\nprobability_of_1 = logistic_regression_model.predict_proba(X)[:, 1]\n\n\n# Plotting the logistic regression curve\nplt.scatter(X, y, color='blue', label='Data Points')\nplt.plot(probability_of_1, color='red', label='Logistic Regression Curve')\nplt.xlabel('X')\nplt.ylabel('Probability')\nplt.title('Logistic Regression')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nSo, fitting a logistic regression model involves finding the best parameters (coefficients) that describe the relationship between our input data and the probability of the binary outcome.\n\n\nDon’t miss out on any updates and developments! Subscribe to the DATAIDEA Newsletter it’s easy and safe.\n\n\n\n\n\nfrom dataidea import np,pd\n\n\ndataf = pd.DataFrame(\n    data={\n        'income': [1000, 2000, 3000],\n        'age': [10, 20, 30]\n    }\n)\n\ndataf\n\n\n\n\n\n\n\n\nincome\nage\n\n\n\n\n0\n1000\n10\n\n\n1\n2000\n20\n\n\n2\n3000\n30\n\n\n\n\n\n\n\n\ndef model (gradient, intercept, x_values):\n    y_pred = [gradient * x_value + intercept for x_value in x_values]\n    return y_pred\n\n\ny_pred = model(gradient=100, intercept=0.1, x_values=dataf.age)\n\n\ndataf['pred'] = y_pred\ndataf['loss'] = pow(dataf.income - y_pred, 2)\n\n\ndataf\n\n\n\n\n\n\n\n\nincome\nage\npred\nloss\n\n\n\n\n0\n1000\n10\n1000.1\n0.01\n\n\n1\n2000\n20\n2000.1\n0.01\n\n\n2\n3000\n30\n3000.1\n0.01\n\n\n\n\n\n\n\n\ncost1 = np.mean(dataf.loss)\n\n\ncost1\n\n0.009999999999989387\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Python-Data-Analysis",
      "Week4-ML-Intro",
      "Training a model: Meaning"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week4-ML-Intro/41_overview_of_machine_learning.html",
    "href": "Python-Data-Analysis/Week4-ML-Intro/41_overview_of_machine_learning.html",
    "title": "Overview of Machine Learning",
    "section": "",
    "text": "Photo by DATAIDEA",
    "crumbs": [
      "Python-Data-Analysis",
      "Week4-ML-Intro",
      "Overview of Machine Learning"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week4-ML-Intro/41_overview_of_machine_learning.html#references",
    "href": "Python-Data-Analysis/Week4-ML-Intro/41_overview_of_machine_learning.html#references",
    "title": "Overview of Machine Learning",
    "section": "References",
    "text": "References\n\nDATAIDEA - What is Demographic Data\nIBM - What is Machine Learning\nData Camp - What is Machine Learning",
    "crumbs": [
      "Python-Data-Analysis",
      "Week4-ML-Intro",
      "Overview of Machine Learning"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week3-Visualization/32_data_exploration_and_cleaning_exercise.html",
    "href": "Python-Data-Analysis/Week3-Visualization/32_data_exploration_and_cleaning_exercise.html",
    "title": "Data Exploration and Cleaning Exercise",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\n\nLoad demo.xlsx dataset\n\n\n# your solution\n\n\nRename the columns as suggested below\n\n\n\nOld name\nNew name\n\n\n\n\nAge\nage\n\n\nGender\ngender\n\n\nMarital Status\nmarital_status\n\n\nAddress\naddress\n\n\nIncome\nincome\n\n\nIncome Category\nincome_category\n\n\nJob Category\njob_category\n\n\n\n\n\n# your solution\n\n\nDisplay all the columns in the dataset\n\n\n# your solution\n\n\nDisplay some basic statistics about the numeric variables in the dataset\n\n\n# your solution\n\n\nDisplay some basic statistics about the categorical variables in the dataset\n\n\n# your solution\n\n\n\n\n\n\n\nWhat are the unique observations under gender?\n\n\n# your solution\n\n\nCan you fix any problems observed under the gender, give brief explanations why and how\n\n\n# your solution\n\n\nHow many observations have ‘no answer’ for marital status?\n\n\n# your solution\n\n\nWrite some piece of code to return only numeric variables from the dataset\n\n\n# your solution\n\n\nAre there any missing values in the dataset?\n\n\n# your solution\n\n\n\n\n\n\n\nAre there any outliers in the income variable?\n\n\n# your solution\n\n\nInvestigate the relationship between age and income\n\n\n# your solution\n\n\nHow many people earn more than 300 units?\n\n\n# your solution\n\n\nWhat data type is the marital status?\n\n\n# your solution\n\n\nCreate dummy variables for gender\n\n\n# your solution\n\n\nSubscribe to our Newsletter so that you don’t miss out on new updates \n\n\n\n\n\n\nEND\n\n\n\n Back to top",
    "crumbs": [
      "Python-Data-Analysis",
      "Week3-Visualization",
      "Data Exploration and Cleaning Exercise"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week3-Visualization/31_matplotlib_refined.html",
    "href": "Python-Data-Analysis/Week3-Visualization/31_matplotlib_refined.html",
    "title": "Matplotlib",
    "section": "",
    "text": "Photo by DATAIDEA",
    "crumbs": [
      "Python-Data-Analysis",
      "Week3-Visualization",
      "Matplotlib"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week3-Visualization/31_matplotlib_refined.html#what-is-matploblib",
    "href": "Python-Data-Analysis/Week3-Visualization/31_matplotlib_refined.html#what-is-matploblib",
    "title": "Matplotlib",
    "section": "What is Matploblib",
    "text": "What is Matploblib\nMatplotlib is a powerful plotting library in Python commonly used for data visualization. When working with datasets, you can use Matplotlib to create various plots to explore and visualize the data. Here are some major plots you can create using Matplotlib with the Titanic dataset:\n\n# # Uncomment and run this cell to install the libraries\n# !pip install pandas matplotlib dataidea\n\n\n# import the libraries, packages and modules\nfrom dataidea.packages import pd, plt\nfrom dataidea.datasets import loadDataset\n\nLet’s demonstrate each of the plots using the Titanic dataset. We’ll first load the dataset and then create each plot using Matplotlib.\n\n# Load the Titanic dataset\ntitanic_df = loadDataset('titanic')\n\nWe can load this dataset like this because it is inbuilt in the dataidea package\n\ntitanic_df.head(n=5)\n\n\n\n\n\n\n\n\npclass\nsurvived\nname\nsex\nage\nsibsp\nparch\nticket\nfare\ncabin\nembarked\nboat\nbody\nhome.dest\n\n\n\n\n0\n1.0\n1.0\nAllen, Miss. Elisabeth Walton\nfemale\n29.0000\n0.0\n0.0\n24160\n211.3375\nB5\nS\n2\nNaN\nSt Louis, MO\n\n\n1\n1.0\n1.0\nAllison, Master. Hudson Trevor\nmale\n0.9167\n1.0\n2.0\n113781\n151.5500\nC22 C26\nS\n11\nNaN\nMontreal, PQ / Chesterville, ON\n\n\n2\n1.0\n0.0\nAllison, Miss. Helen Loraine\nfemale\n2.0000\n1.0\n2.0\n113781\n151.5500\nC22 C26\nS\nNaN\nNaN\nMontreal, PQ / Chesterville, ON\n\n\n3\n1.0\n0.0\nAllison, Mr. Hudson Joshua Creighton\nmale\n30.0000\n1.0\n2.0\n113781\n151.5500\nC22 C26\nS\nNaN\n135.0\nMontreal, PQ / Chesterville, ON\n\n\n4\n1.0\n0.0\nAllison, Mrs. Hudson J C (Bessie Waldo Daniels)\nfemale\n25.0000\n1.0\n2.0\n113781\n151.5500\nC22 C26\nS\nNaN\nNaN\nMontreal, PQ / Chesterville, ON\n\n\n\n\n\n\n\n\n\n\n\n\n\nBar Plot: You can create a bar plot to visualize categorical data such as the number of passengers in each class (first class, second class, third class), the number of survivors vs. non-survivors, or the number of passengers embarked from each port (Cherbourg, Queenstown, Southampton).\n\n\n# 1. Bar Plot - Number of passengers in each class\nclass_counts = titanic_df.pclass.value_counts()\nclasses = class_counts.index\ncounts = class_counts.values\n\nplt.bar(x=classes, height=counts)\nplt.title('Number of Passengers Per Passenger Class')\nplt.xlabel('Passenger Class')\nplt.ylabel('Number of Passengers')\n\nplt.show()\n\n\n\n\n\n\n\n\nIt’s easy to see from the graph that the 3rd class had the largest number of passengers, followed by the 1st class and 2nd class comes last\n\nHistogram: Histograms are useful for visualizing the distribution of continuous variables such as age or fare. You can create histograms to see the age distribution of passengers or the fare distribution.\n\n\n# 2. Histogram - Age distribution of passengers\nages = titanic_df.age\nplt.hist(x=ages, bins=20, edgecolor='black')\nplt.title('Age Distribution of Passengers')\nplt.ylabel('Frequency')\nplt.xlabel('Age')\nplt.show()\n\n\n\n\n\n\n\n\nFrom the histogram we can observe that:\n\nThe majority of the people we of ages between 15 and 35\nFewer older people(above 60 years) boarded the titanic (below 20)\n\n\n\n\n\n\n\nBox Plot: A box plot can be used to show the distribution of a continuous variable across different categories. For example, you can create a box plot to visualize the distribution of age or fare across different passenger classes.\n\n3.1. Age distribution boxplot\n\n# 3.1 Age distribution boxplot\nages = titanic_df.age.dropna()\nplt.boxplot(x=ages, vert=False)\nplt.title('Age Distribution of Passengers')\nplt.xlabel('Age')\nplt.show()\n\n\n\n\n\n\n\n\nFeatures of a box plot:\n\nBox: The box in a boxplot represents the interquartile range (IQR), which contains the middle 50% of the data. The top and bottom edges of the box are the third quartile (Q3) and the first quartile (Q1), respectively.\nMedian Line: A line inside the box indicates the median (Q2) of the data, which is the middle value of the dataset.\nWhiskers: The whiskers extend from the edges of the box to the smallest and largest values within 1.5 times the IQR from Q1 and Q3. They represent the range of the bulk of the data.\nOutliers: Data points that fall outside the whiskers are considered outliers. They are typically plotted as individual points. Outliers can be indicative of variability or errors in the data.\nMinimum and Maximum: The ends of the whiskers show the minimum and maximum values within the range of 1.5 times the IQR from the first and third quartiles.\n\nMeaning: A boxplot provides a visual summary of several important aspects of a dataset:\n\nCentral Tendency: The median line shows the central point of the data.\nSpread: The IQR (the length of the box) shows the spread of the middle 50% of the data.\nSymmetry and Skewness: The relative position of the median within the box and the length of the whiskers can indicate whether the data is symmetric or skewed.\nOutliers: Individual points outside the whiskers highlight potential outliers.\n\nBoxplots are particularly useful for comparing distributions between several groups or datasets and identifying outliers and potential anomalies.\n3.2 Age Distribution Across Passenger Classes\n\n# 3. Box Plot - Distribution of age across passenger classes\nplt.boxplot([titanic_df[titanic_df['pclass'] == 1]['age'].dropna(),\n             titanic_df[titanic_df['pclass'] == 2]['age'].dropna(),\n             titanic_df[titanic_df['pclass'] == 3]['age'].dropna()],\n            labels=['1st Class', '2nd Class', '3rd Class'])\nplt.xlabel('Passenger Class')\nplt.ylabel('Age')\nplt.title('Distribution of Age Across Passenger Classes')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScatter Plot: Scatter plots are helpful for visualizing the relationship between two continuous variables. You can create scatter plots to explore relationships such as age vs. fare. Read more about the scatter plot from the Matplotlib documentation\n\n\n# 4. Scatter Plot - Age vs. Fare\nplt.scatter(\n    x=titanic_df['age'], \n    y=titanic_df['fare'], \n    alpha=.5, \n    c=titanic_df['survived'], \n    cmap='coolwarm'\n)\nplt.xlabel('Age')\nplt.ylabel('Fare')\nplt.title('Age vs. Fare')\nplt.colorbar(label='Survived')  \nplt.show()\n\n\n\n\n\n\n\n\nI don’t about you but for me I don’t see a linear relationship between the age and fare of the titanic passengers\n\nPie Chart: Pie charts can be used to visualize the proportion of different categories within a dataset. For example, you can create a pie chart to show the proportion of male vs. female passengers or the proportion of survivors vs. non-survivors.\n\n\n# 5. Pie Chart - Proportion of male vs. female passengers\ngender_counts = titanic_df['sex'].value_counts()\nplt.pie(gender_counts, labels=gender_counts.index, autopct='%1.1f%%', startangle=90)\nplt.title('Proportion of Male vs. Female Passengers')\nplt.legend(loc='lower right')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStacked Bar Plot: Stacked bar plots can be used to compare the composition of different categories across groups. For example, you can create a stacked bar plot to compare the proportion of survivors and non-survivors within each passenger class.\n\n\n# 6. Stacked Bar Plot - Survival status within each passenger class\nsurvival_counts = titanic_df.groupby(['pclass', 'survived']).size().unstack()\nsurvival_counts.plot(kind='bar', stacked=True)\nplt.xlabel('Passenger Class')\nplt.ylabel('Number of Passengers')\nplt.title('Survival Status Within Each Passenger Class')\nplt.legend(['Did not survive', 'Survived'])\nplt.show()\n\n\n\n\n\n\n\n\n\ntitanic_df.groupby(['pclass', 'survived']).size().unstack()\n\n\n\n\n\n\n\nsurvived\n0.0\n1.0\n\n\npclass\n\n\n\n\n\n\n1.0\n123\n200\n\n\n2.0\n158\n119\n\n\n3.0\n528\n181\n\n\n\n\n\n\n\nWe observe that:\n\nMore passengers in class 1 survived than those that did not survive (200 vs 123)\nMost of the passengers in class 3 did not survive (528 vs 181)\nSlightly more passengers did not survive as compared to those that survived in class 2 (152 vs 119)\n\n\nLine Plot: Line plots can be useful for visualizing trends over time or continuous variables. While the Titanic dataset may not have explicit time data, you can still use line plots to visualize trends such as the change in survival rate with increasing age or fare.\n\n\n# 7. Line Plot - Mean age of passengers by passenger class\nmean_age_by_class = titanic_df.groupby('pclass')['age'].mean()\nplt.plot(mean_age_by_class.index, mean_age_by_class.values, marker='o')\nplt.xlabel('Passenger Class')\nplt.ylabel('Mean Age')\nplt.title('Mean Age of Passengers by Passenger Class')\nplt.show()\n\n\n\n\n\n\n\n\nWe can quickly see the average ages for each passenger class, ie: - Around 39 for first class - Around 30 for second class - Around 25 for third class\nThese are some of the major plots you can create using Matplotlib. Each plot serves a different purpose and can help you gain insights into the data and explore relationships between variables.\n\n\n\n\n\n\nair_passengers_data = loadDataset('air_passengers')\nair_passengers_data.head()\n\n\n\n\n\n\n\n\nMonth\nPassengers\n\n\n\n\n0\n1949-01\n112\n\n\n1\n1949-02\n118\n\n\n2\n1949-03\n132\n\n\n3\n1949-04\n129\n\n\n4\n1949-05\n121\n\n\n\n\n\n\n\n\nair_passengers_data['Month'] = pd.to_datetime(air_passengers_data.Month)\nplt.plot('Month', 'Passengers', data=air_passengers_data)\nplt.xlabel('Years')\nplt.ylabel('Number of Passengers')\nplt.show()\n\n\n\n\n\n\n\n\nWe can observe that the number of passengers seems to increase with time",
    "crumbs": [
      "Python-Data-Analysis",
      "Week3-Visualization",
      "Matplotlib"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/WeekX-Extras/handling_imbalanced_data.html",
    "href": "Python-Data-Analysis/WeekX-Extras/handling_imbalanced_data.html",
    "title": "Imbalanced Datasets",
    "section": "",
    "text": "Photo by DATAIDEA",
    "crumbs": [
      "Python-Data-Analysis",
      "WeekX-Extras",
      "Imbalanced Datasets"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/WeekX-Extras/handling_imbalanced_data.html#handling-imbalanced-dataset",
    "href": "Python-Data-Analysis/WeekX-Extras/handling_imbalanced_data.html#handling-imbalanced-dataset",
    "title": "Imbalanced Datasets",
    "section": "Handling Imbalanced Dataset",
    "text": "Handling Imbalanced Dataset\nHandling imbalanced datasets is a common challenge in machine learning, especially in classification tasks where one class significantly outnumbers the other(s). Let’s go through a simple example using the popular Iris dataset, which we’ll artificially imbalance for demonstration purposes.\nThe Iris dataset consists of 150 samples, each belonging to one of three classes: Iris Setosa, Iris Versicolour, and Iris Virginica. We’ll create an imbalanced version of this dataset where one class is underrepresented.\nFirst, let’s load the dataset and create the imbalance:\n\n# !pip install imbalanced-learn\n# !pip install --upgrade dataidea\n\n\nfrom dataidea.packages import pd, plt, np\nfrom sklearn.datasets import load_iris\n\n\n# Load Iris dataset\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Convert to DataFrame for manipulation\ndf = pd.DataFrame(data=np.c_[X, y], \n                  columns=iris.feature_names + ['target'])\n\ndf.head()\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\ntarget\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n0.0\n\n\n1\n4.9\n3.0\n1.4\n0.2\n0.0\n\n\n2\n4.7\n3.2\n1.3\n0.2\n0.0\n\n\n3\n4.6\n3.1\n1.5\n0.2\n0.0\n\n\n4\n5.0\n3.6\n1.4\n0.2\n0.0\n\n\n\n\n\n\n\n\nIntroducing Imbalance\n\n# Introduce imbalance by removing samples from one class\nclass_to_remove = 2  # Iris Virginica\nimbalance_ratio = 0.5  # Ratio of samples to be removed\nindices_to_remove = np.random.choice(df[df['target'] == class_to_remove].index,\n                                     size=int(imbalance_ratio * len(df[df['target'] == class_to_remove])),\n                                     replace=False)\ndf_imbalanced = df.drop(indices_to_remove)\n\n# Check the class distribution\nvalue_counts = df_imbalanced['target'].value_counts()\nprint(value_counts)\n\ntarget\n0.0    50\n1.0    50\n2.0    25\nName: count, dtype: int64\n\n\n\nplt.bar(value_counts.index, \n        value_counts.values, \n        color=[\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"])\n\n# Adding labels and title\nplt.xlabel('Categories')\nplt.ylabel('Values')\nplt.title('Distribution of Target (Species)')\n\nplt.show()\n\n\n\n\n\n\n\n\nNow, df_imbalanced contains the imbalanced dataset. Next, we’ll demonstrate a few techniques to handle this imbalance:\n\nResampling Methods:\n\nOversampling: Randomly duplicate samples from the minority class.\nUndersampling: Randomly remove samples from the majority class.\n\nSynthetic Sampling Methods:\n\nSMOTE (Synthetic Minority Over-sampling Technique): Generates synthetic samples for the minority class.\n\nAlgorithmic Techniques:\n\nAlgorithm Tuning: Adjusting class weights in the algorithm.\nEnsemble Methods: Using ensemble techniques like bagging or boosting.",
    "crumbs": [
      "Python-Data-Analysis",
      "WeekX-Extras",
      "Imbalanced Datasets"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/WeekX-Extras/handling_imbalanced_data.html#resampling",
    "href": "Python-Data-Analysis/WeekX-Extras/handling_imbalanced_data.html#resampling",
    "title": "Imbalanced Datasets",
    "section": "Resampling",
    "text": "Resampling\n\nOversampling\nLet’s implement oversampling using the imbalanced-learn library:\n\nfrom imblearn.over_sampling import RandomOverSampler\n\n# Separate features and target\nX_imbalanced = df_imbalanced.drop('target', axis=1)\ny_imbalanced = df_imbalanced['target']\n\n# Apply Random Over-Sampling\noversample = RandomOverSampler(sampling_strategy='auto', \n                               random_state=42)\nX_resampled, y_resampled = oversample.fit_resample(X_imbalanced, y_imbalanced)\n\n# Check the class distribution after oversampling\noversampled_data_value_counts = pd.Series(y_resampled).value_counts()\nprint(oversampled_data_value_counts)\n\ntarget\n0.0    50\n1.0    50\n2.0    50\nName: count, dtype: int64\n\n\n\nplt.bar(oversampled_data_value_counts.index, \n        oversampled_data_value_counts.values, \n        color=[\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"])\n\n# Adding labels and title\nplt.xlabel('Categories')\nplt.ylabel('Values')\nplt.title('Distribution of Target (Species)')\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nUndersampling:\nUndersampling involves reducing the number of samples in the majority class to balance the dataset. Here’s how you can apply random undersampling using the imbalanced-learn library:\n\nfrom imblearn.under_sampling import RandomUnderSampler\n\n# Apply Random Under-Sampling\nundersample = RandomUnderSampler(sampling_strategy='auto', random_state=42)\nX_resampled, y_resampled = undersample.fit_resample(X_imbalanced, y_imbalanced)\n\n# Check the class distribution after undersampling\nundersampled_data_value_counts = pd.Series(y_resampled).value_counts()\nprint(undersampled_data_value_counts)\n\ntarget\n0.0    25\n1.0    25\n2.0    25\nName: count, dtype: int64\n\n\n\nplt.bar(undersampled_data_value_counts.index, \n        undersampled_data_value_counts.values, \n        color=[\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"])\n\n# Adding labels and title\nplt.xlabel('Categories')\nplt.ylabel('Values')\nplt.title('Distribution of Target (Species)')\n\nplt.show()",
    "crumbs": [
      "Python-Data-Analysis",
      "WeekX-Extras",
      "Imbalanced Datasets"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/WeekX-Extras/handling_imbalanced_data.html#smote-synthetic-minority-over-sampling-technique",
    "href": "Python-Data-Analysis/WeekX-Extras/handling_imbalanced_data.html#smote-synthetic-minority-over-sampling-technique",
    "title": "Imbalanced Datasets",
    "section": "SMOTE (Synthetic Minority Over-sampling Technique)",
    "text": "SMOTE (Synthetic Minority Over-sampling Technique)\nSMOTE generates synthetic samples for the minority class by interpolating between existing minority class samples.\nHere’s how you can apply SMOTE using the imbalanced-learn library:\n\nfrom imblearn.over_sampling import SMOTE\n\n# Apply SMOTE\nsmote = SMOTE(sampling_strategy='auto', random_state=42)\nX_resampled, y_resampled = smote.fit_resample(X_imbalanced, y_imbalanced)\n\n# Check the class distribution after SMOTE\nsmoted_data_value_counts = pd.Series(y_resampled).value_counts()\nprint(smoted_data_value_counts)\n\ntarget\n0.0    50\n1.0    50\n2.0    50\nName: count, dtype: int64\n\n\n\nplt.bar(smoted_data_value_counts.index, \n        smoted_data_value_counts.values, \n        color=[\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"])\n\n# Adding labels and title\nplt.xlabel('Categories')\nplt.ylabel('Values')\nplt.title('Distribution of Target (Species)')\n\nplt.show()\n\n\n\n\n\n\n\n\nBy using SMOTE, you can generate synthetic samples for the minority class, effectively increasing its representation in the dataset. This can help to mitigate the class imbalance issue and improve the performance of your machine learning model.",
    "crumbs": [
      "Python-Data-Analysis",
      "WeekX-Extras",
      "Imbalanced Datasets"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/WeekX-Extras/handling_imbalanced_data.html#algorithmic-techniques",
    "href": "Python-Data-Analysis/WeekX-Extras/handling_imbalanced_data.html#algorithmic-techniques",
    "title": "Imbalanced Datasets",
    "section": "Algorithmic Techniques",
    "text": "Algorithmic Techniques\n\nAlgorithm Tuning:\nMany algorithms allow you to specify class weights to penalize misclassifications of the minority class more heavily. Here’s an example using the class_weight parameter in a logistic regression classifier:\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\n\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_imbalanced, y_imbalanced, \n                                                    test_size=0.2, random_state=42)\n\n# Define the logistic regression classifier with class weights\nclass_weights = {0: 1, 1: 1, 2: 20}  # Penalize the minority class more heavily\nlog_reg = LogisticRegression(class_weight=class_weights)\n\n# Train the model\nlog_reg.fit(X_train, y_train)\n\n# Evaluate the model\ny_pred = log_reg.predict(X_test)\n\n# display classification report\npd.DataFrame(classification_report(y_test, y_pred, output_dict=True)).transpose()\n\n\n\n\n\n\n\n\nprecision\nrecall\nf1-score\nsupport\n\n\n\n\n0.0\n1.000000\n1.000000\n1.000000\n13.00\n\n\n1.0\n1.000000\n0.750000\n0.857143\n8.00\n\n\n2.0\n0.666667\n1.000000\n0.800000\n4.00\n\n\naccuracy\n0.920000\n0.920000\n0.920000\n0.92\n\n\nmacro avg\n0.888889\n0.916667\n0.885714\n25.00\n\n\nweighted avg\n0.946667\n0.920000\n0.922286\n25.00\n\n\n\n\n\n\n\nIn this example, the class weight for the minority class is increased to penalize misclassifications more heavily.\n\n\nEnsemble Methods\nEnsemble methods can also be effective for handling imbalanced datasets. Techniques such as bagging and boosting can improve the performance of classifiers, especially when dealing with imbalanced classes.\nHere’s an example of using ensemble methods like Random Forest, a popular bagging algorithm, with an imbalanced dataset:\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\n\n# Define and train Random Forest classifier\nrf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_classifier.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred_rf = rf_classifier.predict(X_test)\n\n# Evaluate the performance\nprint(\"Random Forest Classifier:\")\npd.DataFrame(data=classification_report(y_test, y_pred_rf, output_dict=True)).transpose()\n\nRandom Forest Classifier:\n\n\n\n\n\n\n\n\n\nprecision\nrecall\nf1-score\nsupport\n\n\n\n\n0.0\n1.000000\n1.000000\n1.000000\n13.00\n\n\n1.0\n1.000000\n0.875000\n0.933333\n8.00\n\n\n2.0\n0.800000\n1.000000\n0.888889\n4.00\n\n\naccuracy\n0.960000\n0.960000\n0.960000\n0.96\n\n\nmacro avg\n0.933333\n0.958333\n0.940741\n25.00\n\n\nweighted avg\n0.968000\n0.960000\n0.960889\n25.00\n\n\n\n\n\n\n\nEnsemble methods like Random Forest build multiple decision trees and combine their predictions to make a final prediction. This can often lead to better generalization and performance, even in the presence of imbalanced classes.\n\n\nAdaBoost Classifier\nAnother ensemble method that specifically addresses class imbalance is AdaBoost (Adaptive Boosting). AdaBoost focuses more on those training instances that were previously misclassified, thus giving higher weight to the minority class instances.\n\nfrom sklearn.ensemble import AdaBoostClassifier\n\n# Define and train AdaBoost classifier\nada_classifier = AdaBoostClassifier(n_estimators=100, random_state=42)\nada_classifier.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred_ada = ada_classifier.predict(X_test)\n\n# Evaluate the performance\nprint(\"AdaBoost Classifier:\")\npd.DataFrame(data=classification_report(y_test, y_pred_ada, output_dict=True)).transpose()\n\nAdaBoost Classifier:\n\n\n/home/jumashafara/venvs/dataidea/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\nprecision\nrecall\nf1-score\nsupport\n\n\n\n\n0.0\n1.000000\n1.000000\n1.000000\n13.00\n\n\n1.0\n1.000000\n0.875000\n0.933333\n8.00\n\n\n2.0\n0.800000\n1.000000\n0.888889\n4.00\n\n\naccuracy\n0.960000\n0.960000\n0.960000\n0.96\n\n\nmacro avg\n0.933333\n0.958333\n0.940741\n25.00\n\n\nweighted avg\n0.968000\n0.960000\n0.960889\n25.00\n\n\n\n\n\n\n\nBy utilizing ensemble methods like Random Forest and AdaBoost, you can often achieve better performance on imbalanced datasets compared to individual classifiers, as these methods inherently mitigate the effects of class imbalance through their construction.\nThese are just a few techniques for handling imbalanced datasets. It’s crucial to experiment with different methods and evaluate their performance using appropriate evaluation metrics to find the best approach for your specific problem.\nA few ads maybe displayed for income as resources are now offered freely. 🤝🤝🤝",
    "crumbs": [
      "Python-Data-Analysis",
      "WeekX-Extras",
      "Imbalanced Datasets"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week7-Preprocessing/72_why_scaling.html",
    "href": "Python-Data-Analysis/Week7-Preprocessing/72_why_scaling.html",
    "title": "DATAIDEA",
    "section": "",
    "text": "Photo by DATAIDEA",
    "crumbs": [
      "Python-Data-Analysis",
      "Week7-Preprocessing",
      "Why re-scaling (Iris)?"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week7-Preprocessing/72_why_scaling.html#why-re-scaling-iris",
    "href": "Python-Data-Analysis/Week7-Preprocessing/72_why_scaling.html#why-re-scaling-iris",
    "title": "DATAIDEA",
    "section": "Why re-scaling (Iris)?",
    "text": "Why re-scaling (Iris)?\nLet’s use the Iris dataset, a popular dataset in machine learning. The Iris dataset consists of 150 samples of iris flowers, with each sample containing four features: sepal length, sepal width, petal length, and petal width. We’ll demonstrate the effect of not scaling the features on K-means clustering.\nFirst, let’s import the necessary libraries and load the Iris dataset:\n\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\n\n# Load the Iris dataset\niris = load_iris()\nX = iris.data\ny = iris.target\n\n\n# picking the first row\nX[0]\n\narray([5.1, 3.5, 1.4, 0.2])\n\n\nNext, let’s perform K-means clustering on the original dataset without scaling the features:\n\n# Apply K-means clustering without scaling\nkmeans_unscaled = KMeans(n_clusters=2, random_state=42)\nkmeans_unscaled.fit(X)\n\n# Get the cluster centers and labels\ncentroids_unscaled = kmeans_unscaled.cluster_centers_\nlabels_unscaled = kmeans_unscaled.labels_\n\n\nfrom sklearn.metrics import silhouette_score\n\n\nsilhouette_avg = silhouette_score(X, kmeans_unscaled.labels_)\nprint('Silhouette Average (Unscaled): ', silhouette_avg)\n\nSilhouette Average (Unscaled):  0.6810461692117462\n\n\nThe interpretation of the silhouette score is relatively straightforward:\n\nClose to +1: A silhouette score near +1 indicates that the sample is far away from the neighboring clusters. This is a good indication of a well-clustered data point.\nClose to 0: A silhouette score near 0 indicates that the sample is on or very close to the decision boundary between two neighboring clusters. It could imply that the sample could belong to either one of the clusters.\nClose to -1: A silhouette score near -1 indicates that the samples might have been assigned to the wrong clusters. This could happen if the clusters overlap significantly or if the wrong number of clusters was chosen.\n\nRead more about the Silhouette Score from here(sklearn)\nNow, let’s visualize the clusters without scaling:\n\n# Visualize clusters without scaling\nplt.figure(figsize=(10, 6))\n\nplt.scatter(X[:, 0], X[:, 1], c=labels_unscaled, cmap='viridis', s=50)\nplt.scatter(centroids_unscaled[:, 0], centroids_unscaled[:, 1], marker='x', s=200, c='black')\n\nplt.xlabel('Sepal Length (cm)')\nplt.ylabel('Sepal Width (cm)')\nplt.title('K-means Clustering Without Scaling')\n\nplt.show()\n\n\n\n\n\n\n\n\nYou’ll notice that the clusters may not seem well-separated or meaningful. This is because the features of the Iris dataset have different scales, with sepal length ranging from approximately 4 to 8 cm, while sepal width ranges from approximately 2 to 4.5 cm.\nNow, let’s repeat the process after scaling the features using StandardScaler:\n\nfrom sklearn.preprocessing import StandardScaler\n\n# Scale the features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Apply K-means clustering on scaled features\nkmeans_scaled = KMeans(n_clusters=2, random_state=42)\nkmeans_scaled.fit(X_scaled)\n\n# Get the cluster centers and labels\ncentroids_scaled = kmeans_scaled.cluster_centers_\nlabels_scaled = kmeans_scaled.labels_\n\n\nsilhouette_avg = silhouette_score(X, kmeans_scaled.labels_)\nprint('Silhouette Average (Scaled): ', silhouette_avg)\n\nSilhouette Average (Scaled):  0.6867350732769777\n\n\nVisualize the clusters after scaling:\n\n# Visualize clusters with scaling\nplt.figure(figsize=(10, 6))\n\nplt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=labels_scaled, cmap='viridis', s=50)\nplt.scatter(centroids_scaled[:, 0], centroids_scaled[:, 1], marker='x', s=200, c='black')\n\nplt.xlabel('Sepal Length (scaled)')\nplt.ylabel('Sepal Width (scaled)')\nplt.title('K-means Clustering With Scaling')\n\nplt.show()\n\n\n\n\n\n\n\n\nYou should see clearer and more meaningful clusters after scaling the features, demonstrating the importance of feature scaling for K-means clustering, especially when dealing with datasets with features of different scales.\n\n\n\n\n\n\nTo be among the first to hear about future updates, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel.",
    "crumbs": [
      "Python-Data-Analysis",
      "Week7-Preprocessing",
      "Why re-scaling (Iris)?"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week7-Preprocessing/71_feature_selection.html",
    "href": "Python-Data-Analysis/Week7-Preprocessing/71_feature_selection.html",
    "title": "Feature Selection",
    "section": "",
    "text": "Photo by DATAIDEA",
    "crumbs": [
      "Python-Data-Analysis",
      "Week7-Preprocessing",
      "Feature Selection"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week7-Preprocessing/71_feature_selection.html#what-is-feature-selection",
    "href": "Python-Data-Analysis/Week7-Preprocessing/71_feature_selection.html#what-is-feature-selection",
    "title": "Feature Selection",
    "section": "What is Feature Selection",
    "text": "What is Feature Selection\nFeature selection is a process where you automatically select those features in your data that contribute most to the prediction variable or output in which you are interested.\nHaving irrelevant features in your data can decrease the accuracy of many models, especially linear algorithms like linear and logistic regression.\nThree benefits of performing feature selection before modeling your data are:\n\nReduces Overfitting: Less redundant data means less opportunity to make decisions based on noise.\nImproves Accuracy: Less misleading data means modeling accuracy improves.\nReduces Training Time: Less data means that algorithms train faster.\n\nYou can learn more about feature selection with scikit-learn in the article Feature selection.\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom dataidea.datasets import loadDataset\n\n\ndata = loadDataset('../assets/demo_cleaned.csv', \n                    inbuilt=False, file_type='csv')\ndata.head()\n\n\n\n\n\n\n\n\nage\ngender\nmarital_status\naddress\nincome\nincome_category\njob_category\n\n\n\n\n0\n55\nf\n1\n12\n72.0\n3.0\n3\n\n\n1\n56\nm\n0\n29\n153.0\n4.0\n3\n\n\n2\n24\nm\n1\n4\n26.0\n2.0\n1\n\n\n3\n45\nm\n0\n9\n76.0\n4.0\n2\n\n\n4\n44\nm\n1\n17\n144.0\n4.0\n3\n\n\n\n\n\n\n\n\ndata = pd.get_dummies(data, columns=['gender'], \n                      dtype='int', drop_first=True)\ndata.head(n=5)\n\n\n\n\n\n\n\n\nage\nmarital_status\naddress\nincome\nincome_category\njob_category\ngender_m\n\n\n\n\n0\n55\n1\n12\n72.0\n3.0\n3\n0\n\n\n1\n56\n0\n29\n153.0\n4.0\n3\n1\n\n\n2\n24\n1\n4\n26.0\n2.0\n1\n1\n\n\n3\n45\n0\n9\n76.0\n4.0\n2\n1\n\n\n4\n44\n1\n17\n144.0\n4.0\n3\n1",
    "crumbs": [
      "Python-Data-Analysis",
      "Week7-Preprocessing",
      "Feature Selection"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week7-Preprocessing/71_feature_selection.html#univariate-feature-selection-techniques",
    "href": "Python-Data-Analysis/Week7-Preprocessing/71_feature_selection.html#univariate-feature-selection-techniques",
    "title": "Feature Selection",
    "section": "Univariate Feature Selection Techniques",
    "text": "Univariate Feature Selection Techniques\nStatistical tests can be used to select those features that have the strongest relationship with the output variable.\nThe scikit-learn library provides the SelectKBest class that can be used with a suite of different statistical tests to select a specific number of features.\nMany different statistical tests can be used with this selection method. For example the ANOVA F-value method is appropriate for numerical inputs and categorical data. This can be used via the f_classif() function. We will select the 4 best features using this method in the example below.\n\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\nfrom sklearn.feature_selection import f_regression\n\nLet’s first separate our data into features ie X and outcome ie y as below.\n\nX = data.drop('marital_status', axis=1)\ny = data.marital_status\n\n\nNumeric or Continuous Features with Categorical Outcome\nBeginning with the numeric columns, let’s find which of them best contributes to the outcome variable\n\nX_numeric = X[['age', 'income', 'address']].copy()\n\n\n# create a test object from SelectKBest\ntest = SelectKBest(score_func=f_classif, k=2)\n\n# fit the test object to the data\nfit = test.fit(X_numeric, y)\n\n# get the scores and features\nscores = fit.scores_\n\n# get the selected indices\nfeatures = fit.transform(X_numeric)\nselected_indices = test.get_support(indices=True)\n\n# print the scores and features\nprint('Feature Scores: ', scores)\nprint('Selected Features Indices: ', selected_indices)\n\nFeature Scores:  [1.34973748 1.73808724 0.02878244]\nSelected Features Indices:  [0 1]\n\n\nThis shows us that the best 2 features to use to differentiate between the groups in our outcome are [0, 1] ie age and income\n\n\nNumeric Features with Numeric Outcome\nLet’s selecting the input features X, and the output (outcome), y\n\n# pick numeric input and output\nX = data[['age', 'address']].copy()\ny = data.income\n\nWe will still use the SelectKBest class but with our score_func as f_regression instead.\n\ntest = SelectKBest(score_func=f_regression, k=1)\n\n# Fit the test to the data\nfit = test.fit(X, y)\n\n# get scores\ntest_scores = fit.scores_\n\n# summarize selected features\nfeatures = fit.transform(X)\n\n# Get the selected feature indices\nselected_indices = fit.get_support(indices=True)\n\nprint('Feature Scores: ', test_scores)\nprint('Selected Features Indices: ', selected_indices)\n\nFeature Scores:  [25.18294605 23.43115992]\nSelected Features Indices:  [0]\n\n\nHere, we can see that age is selected because it returns the higher f_statistic between the two features\n\n\nBoth input and outcome Categorical\nLet’s begin by selecting out only the categorical features to make our X set and set y as categorical\n\n# selecting categorical features\nX = data[['gender_m', 'income_category', 'job_category']].copy()\n\n# selecting categorical outcome\ny = data.marital_status\n\nNow we shall again use SelectKBest but with the score_func as chi2.\n\nfrom sklearn.feature_selection import chi2\n\n\ntest = SelectKBest(score_func=chi2, k=2)\nfit = test.fit(X, y)\nscores = fit.scores_\nfeatures = fit.transform(X)\nselected_indices = fit.get_support(indices=True)\n\nprint('Feature Scores: ', scores)\nprint('Selected Features Indices: ', selected_indices)\n\nFeature Scores:  [0.20921223 0.61979264 0.00555967]\nSelected Features Indices:  [0 1]\n\n\nNote: When using the Chi-Square (chi2) as the the score function for feature selection, you use the Chi-Square statistic.\nAgain, we can see that the features with higher f_statistic scores have been selected\n\nf_classif is most applicable where the input features are continuous and the outcome is categorical.\nf_regression is most applicable where the input features are continuous and the outcome is continuous.\nchi2 is best for when the both the input and outcome are categorical.",
    "crumbs": [
      "Python-Data-Analysis",
      "Week7-Preprocessing",
      "Feature Selection"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week7-Preprocessing/71_feature_selection.html#recursive-feature-elimination",
    "href": "Python-Data-Analysis/Week7-Preprocessing/71_feature_selection.html#recursive-feature-elimination",
    "title": "Feature Selection",
    "section": "Recursive Feature Elimination",
    "text": "Recursive Feature Elimination\nThe Recursive Feature Elimination (or RFE) works by recursively removing attributes and building a model on those attributes that remain.\nIt uses the model accuracy to identify which attributes (and combination of attributes) contribute the most to predicting the target attribute.\nYou can learn more about the RFE class in the scikit-learn documentation.\n\nLogistic Regression\n\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\n\n\nX = data.drop('marital_status', axis=1)\ny = data.marital_status\n\n\n# feature extraction\nmodel = LogisticRegression()\nrfe = RFE(model)\nfit = rfe.fit(X, y)\n\nprint(\"Num Features: %d\" % fit.n_features_)\nprint(\"Selected Features: %s\" % fit.support_)\nprint(\"Feature Ranking: %s\" % fit.ranking_)\n\nNum Features: 3\nSelected Features: [False False False  True  True  True]\nFeature Ranking: [2 3 4 1 1 1]\n\n\n\nX.head()\n\n\n\n\n\n\n\n\nage\naddress\nincome\nincome_category\njob_category\ngender_m\n\n\n\n\n0\n55\n12\n72.0\n3.0\n3\n0\n\n\n1\n56\n29\n153.0\n4.0\n3\n1\n\n\n2\n24\n4\n26.0\n2.0\n1\n1\n\n\n3\n45\n9\n76.0\n4.0\n2\n1\n\n\n4\n44\n17\n144.0\n4.0\n3\n1\n\n\n\n\n\n\n\nFrom the operation above, we can observe features that bring out the best from the LogisticRegression model ranked from 1 as most best and bigger numbers as less.",
    "crumbs": [
      "Python-Data-Analysis",
      "Week7-Preprocessing",
      "Feature Selection"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week7-Preprocessing/71_feature_selection.html#feature-importance",
    "href": "Python-Data-Analysis/Week7-Preprocessing/71_feature_selection.html#feature-importance",
    "title": "Feature Selection",
    "section": "Feature Importance",
    "text": "Feature Importance\nBagged decision trees like Random Forest and Extra Trees can be used to estimate the importance of features.\nIn the example below we construct a ExtraTreesClassifier classifier for the Pima Indians onset of diabetes dataset. You can learn more about the ExtraTreesClassifier class in the scikit-learn API.\n\nExtra Trees Classifier\n\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n\n# feature extraction\nmodel = ExtraTreesClassifier()\nmodel.fit(X, y)\n\n# see the best features\nprint(model.feature_importances_)\n\n[0.29058207 0.24978811 0.26342117 0.06763375 0.08501043 0.04356447]\n\n\n\n\nRandom Forest Classifier\n\n# feature extraction\nmodel = RandomForestClassifier()\nmodel.fit(X, y)\n\n# see the best features\nprint(model.feature_importances_)\n\n[0.28927782 0.2515934  0.28839236 0.06166801 0.06610313 0.04296528]\n\n\nmore about random forest here\n\n\n\n\n\n\nTo be among the first to hear about future updates, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel.",
    "crumbs": [
      "Python-Data-Analysis",
      "Week7-Preprocessing",
      "Feature Selection"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week7-Preprocessing/handling_missing_data_quiz.html",
    "href": "Python-Data-Analysis/Week7-Preprocessing/handling_missing_data_quiz.html",
    "title": "Handling Missing Data Quiz",
    "section": "",
    "text": "Which of the following is a common method for handling missing data?\n\n\nDeleting rows with missing values\n\n\nUsing the mean to fill missing values\n\n\nUsing a machine learning algorithm to predict missing values\n\n\nAll of the above\n\n\nWhat is the term used for removing rows or columns that contain missing data?\n\n\nImputation\n\n\nDeletion\n\n\nInterpolation\n\n\nNormalization\n\n\nWhich imputation method replaces missing values with the mean, median, or mode?\n\n\nRandom sampling imputation\n\n\nRegression imputation\n\n\nCentral tendency imputation\n\n\nK-nearest neighbors imputation\n\n\nWhat is the potential drawback of deleting rows with missing data?\n\n\nIt is computationally expensive.\n\n\nIt can lead to biased results.\n\n\nIt always improves model accuracy.\n\n\nIt requires complex algorithms.\n\n\nWhich technique involves predicting missing values based on other available data?\n\n\nListwise deletion\n\n\nPairwise deletion\n\n\nMultiple imputation\n\n\nHot deck imputation\n\n\nWhich of the following is NOT a method for handling missing data in time series analysis?\n\n\nForward fill\n\n\nBackward fill\n\n\nInterpolation\n\n\nCross-validation\n\n\nIn the context of handling missing data, what does ‘MCAR’ stand for?\n\n\nMissing Completely at Random\n\n\nMissing Conditional on Available Rows\n\n\nMissing Characteristic Attribute Reduction\n\n\nMissing Completely Available Records\n\n\nWhich method is most suitable for handling missing data when the data is ‘MAR’ (Missing At Random)?\n\n\nListwise deletion\n\n\nMultiple imputation\n\n\nMean imputation\n\n\nMode imputation\n\n\nWhich Python library is widely used for data manipulation and handling missing data?\n\n\nNumPy\n\n\nPandas\n\n\nSciPy\n\n\nMatplotlib\n\n\nWhich of the following is a disadvantage of using mean imputation?\n\n\nIt is computationally intensive.\n\n\nIt can distort the variance of the data.\n\n\nIt requires labeled data.\n\n\nIt is only applicable to categorical data.",
    "crumbs": [
      "Python-Data-Analysis",
      "Week7-Preprocessing",
      "Handling Missing Data Quiz"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week7-Preprocessing/handling_missing_data_quiz.html#questions",
    "href": "Python-Data-Analysis/Week7-Preprocessing/handling_missing_data_quiz.html#questions",
    "title": "Handling Missing Data Quiz",
    "section": "",
    "text": "Which of the following is a common method for handling missing data?\n\n\nDeleting rows with missing values\n\n\nUsing the mean to fill missing values\n\n\nUsing a machine learning algorithm to predict missing values\n\n\nAll of the above\n\n\nWhat is the term used for removing rows or columns that contain missing data?\n\n\nImputation\n\n\nDeletion\n\n\nInterpolation\n\n\nNormalization\n\n\nWhich imputation method replaces missing values with the mean, median, or mode?\n\n\nRandom sampling imputation\n\n\nRegression imputation\n\n\nCentral tendency imputation\n\n\nK-nearest neighbors imputation\n\n\nWhat is the potential drawback of deleting rows with missing data?\n\n\nIt is computationally expensive.\n\n\nIt can lead to biased results.\n\n\nIt always improves model accuracy.\n\n\nIt requires complex algorithms.\n\n\nWhich technique involves predicting missing values based on other available data?\n\n\nListwise deletion\n\n\nPairwise deletion\n\n\nMultiple imputation\n\n\nHot deck imputation\n\n\nWhich of the following is NOT a method for handling missing data in time series analysis?\n\n\nForward fill\n\n\nBackward fill\n\n\nInterpolation\n\n\nCross-validation\n\n\nIn the context of handling missing data, what does ‘MCAR’ stand for?\n\n\nMissing Completely at Random\n\n\nMissing Conditional on Available Rows\n\n\nMissing Characteristic Attribute Reduction\n\n\nMissing Completely Available Records\n\n\nWhich method is most suitable for handling missing data when the data is ‘MAR’ (Missing At Random)?\n\n\nListwise deletion\n\n\nMultiple imputation\n\n\nMean imputation\n\n\nMode imputation\n\n\nWhich Python library is widely used for data manipulation and handling missing data?\n\n\nNumPy\n\n\nPandas\n\n\nSciPy\n\n\nMatplotlib\n\n\nWhich of the following is a disadvantage of using mean imputation?\n\n\nIt is computationally intensive.\n\n\nIt can distort the variance of the data.\n\n\nIt requires labeled data.\n\n\nIt is only applicable to categorical data.",
    "crumbs": [
      "Python-Data-Analysis",
      "Week7-Preprocessing",
      "Handling Missing Data Quiz"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week7-Preprocessing/handling_missing_data_quiz.html#answers",
    "href": "Python-Data-Analysis/Week7-Preprocessing/handling_missing_data_quiz.html#answers",
    "title": "Handling Missing Data Quiz",
    "section": "Answers:",
    "text": "Answers:\n\n\nAll of the above\n\n\nDeletion\n\n\nCentral tendency imputation\n\n\nIt can lead to biased results.\n\n\nMultiple imputation\n\n\nCross-validation\n\n\nMissing Completely at Random\n\n\nMultiple imputation\n\n\nPandas\n\n\nIt can distort the variance of the data.",
    "crumbs": [
      "Python-Data-Analysis",
      "Week7-Preprocessing",
      "Handling Missing Data Quiz"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week7-Preprocessing/75_bonus.html",
    "href": "Python-Data-Analysis/Week7-Preprocessing/75_bonus.html",
    "title": "ANOVA for Feature Selection",
    "section": "",
    "text": "Photo by DATAIDEA",
    "crumbs": [
      "Python-Data-Analysis",
      "Week7-Preprocessing",
      "ANOVA for Feature Selection"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week7-Preprocessing/75_bonus.html#why-anova-for-feature-selection",
    "href": "Python-Data-Analysis/Week7-Preprocessing/75_bonus.html#why-anova-for-feature-selection",
    "title": "ANOVA for Feature Selection",
    "section": "Why ANOVA for Feature Selection",
    "text": "Why ANOVA for Feature Selection\nThis bonus notebook is to demonstrate in another way how ANOVA is actually used to help us find better features.\n\n# !pip install -U dataidea\n\nLet’s import some packages, scipy has f_oneway for performing Analysis of Variation, DATAIDEA’s loadDataset for loading the fantasy premier league dataset and Sci-Kit Learn’s SelectKBest for Univariate Feature selection basing on some statistical tests\n\nimport scipy as sp\nfrom sklearn.feature_selection import SelectKBest\nimport dataidea as di\n\n\n# load fpl inbuilt\nfpl = di.loadDataset('fpl') \n\n# select top 5\nfpl.head(n=5)\n\n\n\n\n\n\n\n\nFirst_Name\nSecond_Name\nClub\nGoals_Scored\nAssists\nTotal_Points\nMinutes\nSaves\nGoals_Conceded\nCreativity\nInfluence\nThreat\nBonus\nBPS\nICT_Index\nClean_Sheets\nRed_Cards\nYellow_Cards\nPosition\n\n\n\n\n0\nBruno\nFernandes\nMUN\n18\n14\n244\n3101\n0\n36\n1414.9\n1292.6\n1253\n36\n870\n396.2\n13\n0\n6\nMID\n\n\n1\nHarry\nKane\nTOT\n23\n14\n242\n3083\n0\n39\n659.1\n1318.2\n1585\n40\n880\n355.9\n12\n0\n1\nFWD\n\n\n2\nMohamed\nSalah\nLIV\n22\n6\n231\n3077\n0\n41\n825.7\n1056.0\n1980\n21\n657\n385.8\n11\n0\n0\nMID\n\n\n3\nHeung-Min\nSon\nTOT\n17\n11\n228\n3119\n0\n36\n1049.9\n1052.2\n1046\n26\n777\n315.2\n13\n0\n0\nMID\n\n\n4\nPatrick\nBamford\nLEE\n17\n11\n194\n3052\n0\n50\n371.0\n867.2\n1512\n26\n631\n274.6\n10\n0\n3\nFWD\n\n\n\n\n\n\n\nANOVA helps us determine if there’s a significant difference between the means of many groups.\nThis concept can be used to obtain the best features for a categorical outcome by picking features that can best show the difference between categories. After ANOVA test, features with higher F-statistics would fit this idea.\nBelow, I’ve created groups of goals scored by each position of the players ie Goals scored by forwards make one group, midfielders too and so on.\n\n# Create groups of goals scored for each player position\n\nforwards_goals = fpl[fpl.Position == 'FWD']['Goals_Scored']\nmidfielders_goals = fpl[fpl.Position == 'MID']['Goals_Scored']\ndefenders_goals = fpl[fpl.Position == 'DEF']['Goals_Scored']\ngoalkeepers_goals = fpl[fpl.Position == 'GK']['Goals_Scored']\n\nLet’s run an ANOVA test to see if there’s a significant difference between the means of the goals scored by each of the groups.\n\n# Perform the ANOVA test for the groups\n\nf_statistic, p_value = sp.stats.f_oneway(forwards_goals, midfielders_goals,\n                                         defenders_goals, goalkeepers_goals\n                                        )\nprint(\"F-statistic:\", f_statistic)\nprint(\"p-value:\", p_value)\n\nF-statistic: 33.281034594400445\np-value: 3.9257634156019246e-20\n\n\nWe observe an F-statistic of 33.281 (seems big) and a p-value of 3.926e-20 which is infinitely small and shows significance at 95%, 97% and even 99% confidence levels.\nBelow, I’ve created groups of assist obtained by each position of the players ie Assists obtained by forwards make one group, midfielders too and so on.\n\n# Create groups of assists for each player position\n\nforwards_assists = fpl[fpl.Position == 'FWD']['Assists']\nmidfielders_assists = fpl[fpl.Position == 'MID']['Assists']\ndefenders_assists = fpl[fpl.Position == 'DEF']['Assists']\ngoalkeepers_assists = fpl[fpl.Position == 'GK']['Assists']\n\nLet’s run an ANOVA test to see if there’s a significant difference between the means of the assists by each of the groups.\n\n# Perform the ANOVA test for the groups\n\nf_statistic, p_value = sp.stats.f_oneway(forwards_assists, midfielders_assists,\n                                         defenders_assists, goalkeepers_assists\n                                        )\nprint(\"F-statistic:\", f_statistic)\nprint(\"p-value:\", p_value)\n\nF-statistic: 19.263717036430815\np-value: 5.124889288362087e-12\n\n\nWe observe an F-statistic of 19.264 (seems big too) and a p-value of 5.125e-20 which is infinitely small and shows significance at 95%, 97% and even 99% confidence levels.\nAs we can observe, both features have big and significant F-statistics but it’s clear that the goals is ahead of assists. Basing on the idea that ANOVA checks for the difference between means of groups, it easy to say goals scored can best differentiate between the positions of player as the differences in the means of goals is bigger.\nThat’s good, but it’s alot of work when when we can wrap it up in a SelectKBest class from sklearn as demostrated below\n\n# Use scikit-learn's SelectKBest (with f_classif)\ntest = SelectKBest(k=1)\n\n# select numeric features\nfit = test.fit(fpl[['Goals_Scored', 'Assists']], fpl.Position)\n\n# get the f-statistics\nscores = fit.scores_\n\n# select the best feature\nfeatures = fit.transform(fpl[['Goals_Scored', 'Assists']])\n\n# get the indices (optional)\nselected_indices = test.get_support(indices=True)\n\n# print indices and scores\nprint('Feature Scores: ', scores)\nprint('Selected Features Indices: ', selected_indices)\n\nFeature Scores:  [33.28103459 19.26371704]\nSelected Features Indices:  [0]\n\n\nAs we can observe the 0th feature which is Goals Scored is selected as best 1 of the 2 features as expected basing on the F-statistics\n\n\n\n\n\n\nTo be among the first to hear about future updates, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel.",
    "crumbs": [
      "Python-Data-Analysis",
      "Week7-Preprocessing",
      "ANOVA for Feature Selection"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week1-Intro/12_numpy.html",
    "href": "Python-Data-Analysis/Week1-Intro/12_numpy.html",
    "title": "DATAIDEA",
    "section": "",
    "text": "Photo by DATAIDEA",
    "crumbs": [
      "Python-Data-Analysis",
      "Week1-Intro",
      "Numpy"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week1-Intro/12_numpy.html#numpy",
    "href": "Python-Data-Analysis/Week1-Intro/12_numpy.html#numpy",
    "title": "DATAIDEA",
    "section": "Numpy",
    "text": "Numpy\n\nNumpy is a python package used for scientific computing\nNumpy provides arrays which are greater and faster alternatives to traditional python lists. An array is a group of elements of the same data type\nA standard numpy array is required to have elements of the same data type.\n\n\n# Uncomment and run this cell to install numpy\n# !pip install numpy\n\n\nInspecting our arrays\n\n# import numpy module\nimport numpy as np\n\n\n# checking the numpy version\nnp.__version__\n\n'1.26.4'\n\n\n\n# creating a numpy array\nnum_arr = np.array([1, 2, 3, 4])\n\nThe object that’s created by array() is called ndarray. This can be shown by checking the type of the object using type()\n\n# Checking type of object\ntype(num_arr)\n\nnumpy.ndarray\n\n\n\nData Types\nThe table below describes some of the most common data types we use in numpy\n\n\n\nData Type\nDescription\n\n\n\n\nnp.int64\nSigned 64-bit integer types\n\n\nnp.float32\nStandard double-precision floating point\n\n\nnp.complex\nComplex numbers represented by 128 floats\n\n\nnp.bool\nBoolean type storing True and False values\n\n\nnp.object\nPython object type\n\n\nnp.string_\nFixed-length string type\n\n\nnp.unicode_\nFixed-length unicode type\n\n\n\n\n# shape of array\nnum_arr.shape\n\n(4,)\n\n\n\n# finding the number of dimensions\nnum_arr.ndim\n\n\n# number of elements in array\nlen(num_arr)\n\n\n# another way to get the number of elements\nnum_arr.size\n\n\n# finding data type of array elements\nprint(num_arr.dtype.name)\n\n\n# converting an array\nfloat_arr = np.array([1.2, 3.5, 7.0])\n\n# use astype() to convert to a specific\nint_arr = float_arr.astype(int)\n\nprint(f'Array: {float_arr}, Data Type: {float_arr.dtype}')\nprint(f'Array: {int_arr}, Data Type: {int_arr.dtype}')\n\n\n\n\nAsk for help\n\nnp.info(np.ndarray.shape)\n\n\n?np.ndarray.shape",
    "crumbs": [
      "Python-Data-Analysis",
      "Week1-Intro",
      "Numpy"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week1-Intro/12_numpy.html#array-mathematics",
    "href": "Python-Data-Analysis/Week1-Intro/12_numpy.html#array-mathematics",
    "title": "DATAIDEA",
    "section": "Array mathematics",
    "text": "Array mathematics\nNumpy has out of the box tools to help us perform some import mathematical operations\n\nArithmetic Operations\n\n# creating arrays\narray1 = np.array([1, 4, 6, 7])\narray2 = np.array([3, 5, 3, 1])\n\n\n# subtract\ndifference1 = array2 - array1\nprint('difference1 =', difference1)\n\n# another way\ndifference2 = np.subtract(array2, array1)\nprint('difference2 =', difference2)\n\nAs we may notice, numpy does element-wise operations for ordinary arithmetic operations\n\n# sum\nsummation1 = array1 + array2\nprint('summation1 =', summation1)\n\n# another way\nsummation2 = np.add(array1, array2)\nprint('summation2 =', summation2)\n\n\n\nTrigonometric operations\n\n# sin\nprint('sin(array1) =', np.sin(array1))\n# cos\nprint('cos(array1) =', np.cos(array1))\n# log\nprint('log(array1) =', np.log(array1))\n\n\n# dot product\narray1.dot(array2)\n\nGiven matrices A and B, the dot operation mulitiplies A with the transpose of B\nResearch:\n\nanother way to dot matrices (arrays)\n\n\n\nComparison\n\narray1 == array2\n\n\narray1 &gt; 3\n\n\n\nAggregate functions\n\n# average\nmean = array1.mean()\nprint('Mean: ', mean)\n\n# min\nminimum = array1.min()\nprint('Minimum: ', minimum)\n\n# max\nmaximum = array1.max()\nprint('Maximum: ', maximum)\n\n# corrcoef\ncorrelation_coefficient = np.corrcoef(array1, array2)\nprint('Correlation Coefficient: ', correlation_coefficient)\n\nstandard_deviation = np.std(array1)\nprint('Standard Deviation: ', standard_deviation)\n\nResearch:\n\ncopying arrays (you might meet view(), copy())",
    "crumbs": [
      "Python-Data-Analysis",
      "Week1-Intro",
      "Numpy"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week1-Intro/12_numpy.html#subsetting-slicing-and-indexing",
    "href": "Python-Data-Analysis/Week1-Intro/12_numpy.html#subsetting-slicing-and-indexing",
    "title": "DATAIDEA",
    "section": "Subsetting, Slicing and Indexing",
    "text": "Subsetting, Slicing and Indexing\n\nIndexing is the technique we use to access individual elements in an array. 0 represents the first element, 1 the represents second element and so on.\nSlicing is used to access elements of an array using a range of two indexes. The first index is the start of the range while the second index is the end of the range. The indexes are separated by a colon ie [start:end]\n\n\n# Creating numpy arrays of different dimension\n# 1D array\narr1 = np.array([1, 4, 6, 7])\nprint('Array1 (1D): \\n', arr1)\n\n# 2D array\narr2 = np.array([[1.5, 2, 3], [4, 5, 6]]) \nprint('Array2 (2D): \\n', arr2)\n\n#3D array\narr3 = np.array([[[1, 2, 3], [4, 5, 6], [7, 8, 9]], \n                 [[10, 11, 12], [13, 14, 15], [16, 17, 18]]]) \nprint('Array3 (3D): \\n', arr3)\n\n\n# find the dimensions of an array\nprint('Array1 (1D):', arr1.shape)\nprint('Array2 (2D):', arr2.shape)\nprint('Array3 (3D):', arr3.shape)\n\n\nIndexing\n\n# accessing items in a 1D array\narr1[2]\n\n\narr2\n\n\n# accessing items in 2D array\narr2[1, 2]\n\n\narr3\n\n\n# accessing in a 3D array\narr3[0, 1, 2]\n\n\n\nslicing\n\n# slicing 1D array\narr1[0:3]\n\n\narr2\n\n\n# slicing a 2D array\narr2[1, 1:]\n\n\n# slicing a 3D array\nfirst = arr3[0, 2]\nsecond = arr3[1, 0]\n\nnp.concatenate((first, second))\n\n\n\nBoolean Indexing\nBoolean indexing is technique that we use to pick out values from an array that satisfy a specific condition\n\n# boolean indexing\narr1[arr1 &lt; 5]\n\nResearch:\n\nFancy Indexing",
    "crumbs": [
      "Python-Data-Analysis",
      "Week1-Intro",
      "Numpy"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week1-Intro/12_numpy.html#array-manipulation",
    "href": "Python-Data-Analysis/Week1-Intro/12_numpy.html#array-manipulation",
    "title": "DATAIDEA",
    "section": "Array manipulation",
    "text": "Array manipulation\n\nprint(arr2)\n\n\n# transpose\narr2_transpose1 = np.transpose(arr2) \nprint('Transpose1: \\n', arr2_transpose1)\n\n# another way\narr2_transpose2 = arr2.T\nprint('Transpose2: \\n', arr2_transpose2)\n\n\n# combining arrays\nfirst = arr3[0, 2]\nsecond = arr3[1, 0]\n\nnp.concatenate((first, second))\n\n\ntest_arr1 = np.array([[7, 8, 9], [10, 11, 12]])\ntest_arr2 = np.array([[1, 2, 3], [4, 5, 6]])\n\nnp.concatenate((test_arr1, test_arr2), axis=1)\n\n\nSome homework\nResearch:\nAdding/Removing Elements - resize() - append() - insert() - delete()\nChanging array shape - ravel() - reshape()\n\n# stacking \n# np.vstack((a,b))\n# np.hstack((a,b))\n# np.column_stack((a,b))\n# np.c_[a, b]\n\n\n# splitting arrays\n# np.hsplit()\n# np.vsplit()\n\nA few ads maybe displayed for income as resources are now offered freely. 🤝🤝🤝",
    "crumbs": [
      "Python-Data-Analysis",
      "Week1-Intro",
      "Numpy"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week5-Statistics/hypothesis_testing.html",
    "href": "Python-Data-Analysis/Week5-Statistics/hypothesis_testing.html",
    "title": "Hypothesis Testing Quiz",
    "section": "",
    "text": "What is the primary purpose of inferential statistics?\n\nA. To describe the main features of a dataset\nB. To draw conclusions about a population based on a sample\nC. To organize and summarize data\nD. To determine the reliability of data\n\nWhich of the following is an example of a parameter?\n\nA. Sample mean\nB. Sample standard deviation\nC. Population mean\nD. Sample variance\n\nWhich statistical test is used to compare the means of two independent groups?\n\nA. Paired t-test\nB. Independent t-test\nC. ANOVA\nD. Chi-square test\n\nA Type I error occurs when:\n\nA. A true null hypothesis is incorrectly rejected\nB. A false null hypothesis is incorrectly rejected\nC. A true null hypothesis is incorrectly accepted\nD. A false null hypothesis is incorrectly accepted\n\nThe p-value represents:\n\nA. The probability that the null hypothesis is true\nB. The probability of observing the data, or something more extreme, assuming the null hypothesis is true\nC. The probability of making a Type I error\nD. The power of the test\n\nWhich of the following is true about the confidence interval?\n\nA. It provides the exact value of the population parameter\nB. It indicates the range within which the sample mean will fall 95% of the time\nC. It provides a range of values within which the population parameter is likely to fall\nD. It determines the sample size needed for a study\n\nWhat is the central limit theorem?\n\nA. The distribution of sample means will be approximately normally distributed, regardless of the distribution of the population, given a sufficiently large sample size\nB. The sum of a large number of independent and identically distributed random variables will be approximately normally distributed\nC. The probability distribution of a continuous random variable will be normal if the sample size is large enough\nD. The mean of a large sample will equal the mean of the population\n\nWhen should you use a chi-square test?\n\nA. To compare the means of more than two groups\nB. To test the association between two categorical variables\nC. To compare the variances of two samples\nD. To analyze the relationship between a continuous and a categorical variable\n\nIf the 95% confidence interval for a mean difference includes zero, what does this imply?\n\nA. The difference is statistically significant\nB. The null hypothesis cannot be rejected\nC. The sample size is too small\nD. The p-value is less than 0.05\n\nThe power of a statistical test is:\n\nA. The probability of rejecting the null hypothesis when it is true\nB. The probability of accepting the null hypothesis when it is false\nC. The probability of rejecting the null hypothesis when it is false\nD. The probability of making a Type I error\n\nIn hypothesis testing, the null hypothesis is:\n\nA. A statement of no effect or no difference\nB. A statement that there is an effect or a difference\nC. Always the hypothesis that the researcher wants to prove\nD. Less important than the alternative hypothesis\n\nThe sample statistic that is most commonly used to estimate a population parameter is called:\n\nA. Estimator\nB. Point estimate\nC. Confidence interval\nD. Hypothesis\n\nWhich method is used to determine the sample size for a study?\n\nA. Power analysis\nB. T-test\nC. Chi-square test\nD. Correlation coefficient\n\nThe F-test in ANOVA is used to:\n\nA. Test for the equality of two variances\nB. Test for the equality of three or more means\nC. Test for the equality of proportions\nD. Test for independence between two categorical variables\n\nWhich of the following is true regarding the normal distribution?\n\nA. It is skewed to the right\nB. It is defined by its mean and standard deviation\nC. It can only take on positive values\nD. It is bimodal\n\n\n\nCorrect Answers\n\nB\nC\nB\nA\nB\nC\nA\nB\nB\nC\nA\nB\nA\nB\nB\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Python-Data-Analysis",
      "Week5-Statistics",
      "Hypothesis Testing Quiz"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week5-Statistics/52_inferencial_statistics.html",
    "href": "Python-Data-Analysis/Week5-Statistics/52_inferencial_statistics.html",
    "title": "Inferencial Statistics",
    "section": "",
    "text": "Photo by DATAIDEA",
    "crumbs": [
      "Python-Data-Analysis",
      "Week5-Statistics",
      "Inferencial Statistics"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week5-Statistics/52_inferencial_statistics.html#what-is-statistical-inference",
    "href": "Python-Data-Analysis/Week5-Statistics/52_inferencial_statistics.html#what-is-statistical-inference",
    "title": "Inferencial Statistics",
    "section": "What is Statistical Inference",
    "text": "What is Statistical Inference\nStatistical inference is a process in which conclusions about populations or processes are drawn from a sample of data using statistical methods. It involves making inferences, predictions, or decisions about a population based on information obtained from a sample.\n\n## Uncomment and run this cell to install the packages\n# !pip install pandas numpy statsmodels scipy\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nimport pandas as pd\nimport scipy as sp\n\n\n# load the dataset (modify the path to point to your copy of the dataset)\ndata = pd.read_csv('../assets/nobel_prize_year.csv')\ndata = data[data.Gender != 'org']\n\n\n# get the age column data (optional: convert to numpy array)\nages = np.array(data['age'])",
    "crumbs": [
      "Python-Data-Analysis",
      "Week5-Statistics",
      "Inferencial Statistics"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week5-Statistics/52_inferencial_statistics.html#normal-distribution",
    "href": "Python-Data-Analysis/Week5-Statistics/52_inferencial_statistics.html#normal-distribution",
    "title": "Inferencial Statistics",
    "section": "Normal Distribution",
    "text": "Normal Distribution\nThe normal distribution is an important probability distribution used in statistics.\nMany real world examples of data are normally distributed.\nNormal Distribution The normal distribution is described by the mean and the standard deviation\nThe normal distribution is often referred to as a ‘bell curve’ because of it’s shape:\n\nMost of the values are around the center\nThe median and mean are equal\nIt has only one mode\nIt is symmetric, meaning it decreases the same amount on the left and the right of the center\n\nThe area under the curve of the normal distribution represents probabilities for the data.\nThe area under the whole curve is equal to 1, or 100%\nHere is a graph of a normal distribution with probabilities between standard deviations over the nobel prize laureates\n\n# Calculate mean and standard deviation of the standardized ages\nmu = np.mean(ages)\nsigma = np.std(ages)\n\n# Plot histogram\nplt.hist(ages, bins=10, density=True, color='skyblue', edgecolor='black', alpha=0.7)\n\n# Create an array of values within three standard deviations from the mean\nx = np.linspace(min(ages), max(ages), 100)\n\n# Calculate the probability density function (PDF) for the normal distribution\npdf = stats.norm.pdf(x, mu, sigma)\n\n# Plot the normal distribution\nplt.plot(x, pdf, label='Normal Distribution', color='blue')\n\n# Fill the area between one standard deviation from the mean\nplt.fill_between(x, pdf, where=(x &gt;= mu - 3*sigma) & (x &lt;= mu + 3*sigma), color='r', alpha=0.5, label='Between ±3σ')\n\n# Fill the area between two standard deviations from the mean\nplt.fill_between(x, pdf, where=(x &gt;= mu - 2*sigma) & (x &lt;= mu + 2*sigma), color='g', alpha=0.5, label='Between ±2σ')\n\n# Fill the area between one standard deviation from the mean\nplt.fill_between(x, pdf, where=(x &gt;= mu - sigma) & (x &lt;= mu + sigma), color='b', alpha=0.5, label='Between ±σ')\n\n# Add labels and legend\nplt.xlabel('Age')\nplt.ylabel('Probability Density')\nplt.title('Nobel Prize Winners\\' Ages with Probabilities Between Standard Deviations')\n\nplt.legend()\n\n# Display the plot\nplt.grid(True)\nplt.show()",
    "crumbs": [
      "Python-Data-Analysis",
      "Week5-Statistics",
      "Inferencial Statistics"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week5-Statistics/52_inferencial_statistics.html#standard-normal-distribution",
    "href": "Python-Data-Analysis/Week5-Statistics/52_inferencial_statistics.html#standard-normal-distribution",
    "title": "Inferencial Statistics",
    "section": "Standard Normal Distribution",
    "text": "Standard Normal Distribution\nNormally distributed data can be transformed into a standard normal distribution.\nThe standard normal distribution is used for:\n\nCalculating confidence intervals\nHypothesis tests\n\n\nZ-Values\nZ-values express how many standard deviations from the mean a value is.\nThe formula for calculating a Z-value is:\n\\(( z = \\frac{x - \\mu}{\\sigma} )\\)\nWhere:\n\n\\(( \\mu )\\) is the mean.\n\\(( \\sigma )\\) is the standard deviation\n\nHere is a graph of the standard normal distribution with probability values between the standard deviations:\n\n# Standardize ages\nstandardized_ages = (ages - np.mean(ages)) / np.std(ages)\n\n# Calculate mean and standard deviation of the standardized ages\nmu = np.mean(standardized_ages)\nsigma = np.std(standardized_ages)\n\n# Plot histogram\nplt.hist(standardized_ages, bins=10, density=True, color='skyblue', edgecolor='black', alpha=0.7)\n\n# Create an array of values within three standard deviations from the mean\nx = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n\n# Calculate the probability density function (PDF) for the normal distribution\npdf = stats.norm.pdf(x, mu, sigma)\n\n# Plot the normal distribution\nplt.plot(x, pdf, label='Normal Distribution', color='blue')\n\n# Fill the area between three standard deviation from the mean\nplt.fill_between(x, pdf, where=(x &gt;= mu - 3*sigma) & (x &lt;= mu + 3*sigma), color='r', alpha=0.5, label='Between ±3σ')\n\n# Fill the area between two standard deviations from the mean\nplt.fill_between(x, pdf, where=(x &gt;= mu - 2*sigma) & (x &lt;= mu + 2*sigma), color='g', alpha=0.5, label='Between ±2σ')\n\n# Fill the area between one standard deviation from the mean\nplt.fill_between(x, pdf, where=(x &gt;= mu - sigma) & (x &lt;= mu + sigma), color='b', alpha=0.5, label='Between ±σ')\n\n# Add labels and legend\nplt.title('Normal Distribution of Ages of Nobel Prize Winners')\nplt.xlabel('Standardized Ages')\nplt.ylabel('Probability Density')\nplt.legend()\n\n# Display the plot\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nThe Probability Density Function (PDF)\nThe Probability Density Function (PDF) of a normal distribution represents the relative likelihood of observing different values of a continuous random variable. For a normal distribution with mean \\(( \\mu )\\) and standard deviation \\(( \\sigma )\\), the PDF is denoted as \\(( f(x) )\\).\nThe formula for the standard normal PDF \\(( \\phi(z) )\\) is:\n\\(\\phi(z) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{1}{2}z^2}\\)\nWhere: - \\(( z )\\) is the standardized value of the random variable \\(( X )\\), calculated as \\(( z = \\frac{x - \\mu}{\\sigma} )\\). - \\(( e )\\) is the base of the natural logarithm. - \\(( \\pi )\\) is the mathematical constant pi (approximately 3.14159).\nIf your data is not standardized (i.e., does not have a mean of 0 and standard deviation of 1), you would first standardize the values using the formula \\(( z = \\frac{x - \\mu}{\\sigma} )\\) before evaluating the PDF.\n\n\nThe Cummulative Distribution Function\nThe CDF represents the probability that a random variable \\(( X )\\) is less than or equal to a given value \\(( x )\\). For a normal distribution with mean \\(( \\mu )\\) and standard deviation \\(( \\sigma )\\), the CDF is denoted as \\(( \\Phi(x) )\\).\nThe formula for the standard normal cumulative distribution function \\(( \\Phi(z) )\\) is:\n$(z) = _{-}^{z} e{-t2} dt $\nWhere: - \\(( z )\\) is the standardized value of the random variable \\(( X )\\), calculated as \\(( z = \\frac{x - \\mu}{\\sigma} )\\). - \\(( e )\\) is the base of the natural logarithm. - \\(( \\pi )\\) is the mathematical constant pi (approximately 3.14159).\nTo calculate the probability of a specific range of values, you can subtract the CDF at the lower bound of the range from the CDF at the upper bound of the range:\n$P(a X b) = () - () $\nWhere \\(( a )\\) and \\(( b )\\) are the lower and upper bounds of the range, respectively.\nFor a normal distribution with a mean \\(( \\mu )\\) and standard deviation \\(( \\sigma )\\), you can use the above formula to calculate probabilities associated with specific values or ranges of values. If your data is not standardized (i.e., does not have a mean of 0 and standard deviation of 1), you would first standardize the values using the formula \\(( z = \\frac{x - \\mu}{\\sigma} )\\) before using the standard normal CDF.\n\n\nFinding the P-value of a Z-Value\n\nmean = ages.mean()\nstandard_deviation = ages.std()\n\npdf2 = stats.norm(loc=mean, scale=standard_deviation)\np_value = pdf2.cdf(40)\n\nprint('p-value:', p_value)\n\np-value: 0.05756104393909259\n\n\n\n\nFinding the Z-value of a P-Value\n\nmean = ages.mean()\nstandard_deviation = ages.std()\n\npdf2 = stats.norm(loc=mean, scale=standard_deviation)\nz_val = pdf2.ppf(0.05756104393909259)\n\nprint('z-value:', z_val)\n\nz-value: 40.0",
    "crumbs": [
      "Python-Data-Analysis",
      "Week5-Statistics",
      "Inferencial Statistics"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week5-Statistics/52_inferencial_statistics.html#hypothesis-testing-and-p-values",
    "href": "Python-Data-Analysis/Week5-Statistics/52_inferencial_statistics.html#hypothesis-testing-and-p-values",
    "title": "Inferencial Statistics",
    "section": "Hypothesis Testing and p-values",
    "text": "Hypothesis Testing and p-values\nHypothesis testing is a statistical method used to make inferences about a population based on sample data. It involves testing a hypothesis or claim about a population parameter, such as a mean or proportion.\n\nSteps in Hypothesis Testing\n\nNull and Alternative Hypothesis\nTypes of errors\nSignificance level\n\n\nNull and Alternative Hypotheses:\n\nNull Hypothesis (H0): It is a statement of no effect or no difference, typically denoted as the status quo.\nAlternative Hypothesis (H1 or Ha): It is the hypotheses that we are trying to support, indicating there is an effect or difference.\n\n\nExample: Null hypothesis - mean of age is equal to 40\nAlternative hypothesis - mean of age is not equal to 40\nH0: μ = 40, Ha: μ ≠ 40\n\nTypes of Errors: Type I and Type II:\n\nType I Error (False Positive): It occurs when we reject the null hypothesis when it is actually true.\nType II Error (False Negative): It occurs when we fail to reject the null hypothesis when it is actually false.\n\n\nExample: Type I error - Rejecting the null hypothesis when the population mean is actually equal to 40\nType II error - Failing to reject the null hypothesis when the population mean is actually not equal to 40\n\nSignificance Level (α):\n\nSignificance level is denoted by α and represents the probability of making a Type I error.\nCommonly used significance levels include 0.05, 0.01, etc.\n\np-values and their interpretation:\n\np-value is the probability of observing the data or more extreme data under the null hypothesis.\nIf p-value is less than α, we reject the null hypothesis; otherwise, we fail to reject it.\n\n\n\nt_statistic, p_value = stats.ttest_1samp(ages, 40)\n\nprint(\"t-statistic:\", t_statistic)\nprint(\"p-value:\", p_value)\n\nt-statistic: 49.443255923939155\np-value: 2.6038097524296416e-265\n\n\n\nOne-sample and Two-sample t-tests:\n\nOne-sample t-test is used to compare the mean of a single sample to a known value or population mean.\nTwo-sample t-test is used to compare the means of two independent samples.\n\n\n\ngroup1, group2 = np.array_split(ages, 2)\n\nt_statistic, p_value = stats.ttest_ind(group1, group2)\n\nprint(\"t-statistic:\", t_statistic)\nprint(\"p-value:\", p_value)\n\nt-statistic: 1.2207551590600962\np-value: 0.2224810017825374\n\n\n\nZ-tests and Confidence Intervals:\n\nZ-tests are similar to t-tests but are used when sample size is large and population standard deviation is known.\nConfidence Intervals provide a range of values that is likely to contain the population parameter with a certain level of confidence.\n\n\n\nA z-test a statistical test used to determine whether two means are different when the population standard deviation is known\n\n\ndef ztest(sample_data=None, population_data=None, sample_mean:float=None, population_mean:float=None):\n    if sample_data.all():\n        sample_mean = np.mean(sample_ages)\n        sample_size = len(sample_ages)\n    if population_data.all():\n        population_mean = np.mean(ages)\n        population_std = np.std(ages)\n        \n    z_score = (sample_mean - population_mean)/(population_std/(np.sqrt(sample_size)))\n    p_value = stats.norm.cdf(z_score)\n    return (z_score, p_value)\n\n\nsample_ages = np.array(data.sample(n=56)['age'])\n\n\nz_score, p_value = ztest(sample_ages, ages)\n\n\np_value = stats.norm.cdf(z_score)\n\n\np_value\n\n0.6737098436973401\n\n\n\nsample_mean = np.mean(ages)\nsample_std = np.std(ages)\n\nn = len(ages)\nalpha = 0.05\nz_critical = stats.norm.ppf(1 - alpha/2)\n\nmargin_of_error = z_critical * (sample_std / np.sqrt(n))\nconfidence_interval = (sample_mean - margin_of_error, sample_mean + margin_of_error)\n\nprint('z critical:', z_critical)\nprint(\"Confidence Interval:\", confidence_interval)\n\nz critical: 1.959963984540054\nConfidence Interval: (59.314817066181526, 60.934136908713874)\n\n\n\n(1.959963984540054 * sample_std) + sample_mean\n\n85.15855423882927\n\n\n\nChi-square Tests for Independence:\n\nChi-square test is used to determine whether there is a significant association between two categorical variables.\n\n\n\n# Pick out on the Gender and Category from the dataset\n# We drop all the missing values just for demonstration purposes\ngender_category_data = data[['Gender', 'Category']].dropna()\n\n\n# Obtain the cross tabulation of Gender and Category\n# The cross tabulation is also known as the contingency table\ngender_category_tab = pd.crosstab(gender_category_data.Gender, gender_category_data.Category)\n\n\n# Example: Performing a chi-square test for independence\nchi2_stat, p_value, dof, expected = stats.chi2_contingency(gender_category_tab)\n\nprint(\"Chi-square Statistic:\", chi2_stat)\nprint(\"p-value:\", p_value)\n\nChi-square Statistic: 40.7686907732235\np-value: 1.044840181761602e-07\n\n\n\nANOVA (Analysis of Variance) and its applications:\n\nANOVA is used to compare means of three or more groups to determine if there is a statistically significant difference between them.\n\n\n\n# Example: Performing ANOVA\ngroup1, group2, group3 = np.array_split(ages, 3) \nf_statistic, p_value = stats.f_oneway(group1, group2, group3)\n\nprint(\"F-statistic:\", f_statistic)\nprint(\"p-value:\", p_value)\n\nF-statistic: 0.26064539756513505\np-value: 0.7706090235692291\n\n\nThe p-value of 0.770… is way higher than the significance level (0.05), and therefore we fail to reject the null hypothesis (ie. The means are statistically the same)\nF-statistic tells us whether there are significant differences between the means of the groups\nf_statistic = between_group_variance/within_group_variance\nExercise:\nFind the applications of ANOVA\nThese code snippets demonstrate various hypothesis testing techniques and their implementation in Python using libraries like SciPy.",
    "crumbs": [
      "Python-Data-Analysis",
      "Week5-Statistics",
      "Inferencial Statistics"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week5-Statistics/52_inferencial_statistics.html#key-terms-associated-with-statistical-inference",
    "href": "Python-Data-Analysis/Week5-Statistics/52_inferencial_statistics.html#key-terms-associated-with-statistical-inference",
    "title": "Inferencial Statistics",
    "section": "Key terms associated with statistical inference:",
    "text": "Key terms associated with statistical inference:\n\nPopulation: The entire group of individuals or elements that the researcher is interested in studying. It’s often impractical or impossible to collect data from the entire population, so we work with samples instead.\nSample: A subset of the population from which data is collected. The sample should ideally be representative of the population to make valid inferences.\nParameter: A numerical characteristic of a population. Examples include the population mean, population proportion, or population standard deviation.\nStatistic: A numerical characteristic of a sample. Examples include the sample mean, sample proportion, or sample standard deviation.\nEstimation: The process of using sample data to estimate the value of a population parameter. Point estimation involves providing a single value as an estimate, while interval estimation provides a range of values (confidence interval) within which the parameter is believed to lie.\nHypothesis Testing: A method used to make decisions or draw conclusions about a population parameter based on sample data. It involves stating a null hypothesis (H0) and an alternative hypothesis (H1), collecting data, and then using statistical tests to determine whether there is enough evidence to reject the null hypothesis.\nConfidence Intervals: A range of values calculated from sample data that is likely to contain the true population parameter with a certain level of confidence (e.g., 95% confidence interval).\nSignificance Level (α): The probability of rejecting the null hypothesis when it is actually true. It is typically set at 0.05 or 0.01, indicating a 5% or 1% chance of a Type I error, respectively.\nType I Error: Rejecting the null hypothesis when it is actually true (false positive).\nType II Error: Failing to reject the null hypothesis when it is actually false (false negative).\n\n\nTo be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel.",
    "crumbs": [
      "Python-Data-Analysis",
      "Week5-Statistics",
      "Inferencial Statistics"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week8-More-ML/83_GridSearchCV.html",
    "href": "Python-Data-Analysis/Week8-More-ML/83_GridSearchCV.html",
    "title": "DATAIDEA",
    "section": "",
    "text": "Photo by DATAIDEA",
    "crumbs": [
      "Python-Data-Analysis",
      "Week8-More-ML",
      "GridSearchCV"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week8-More-ML/83_GridSearchCV.html#gridsearchcv",
    "href": "Python-Data-Analysis/Week8-More-ML/83_GridSearchCV.html#gridsearchcv",
    "title": "DATAIDEA",
    "section": "GridSearchCV",
    "text": "GridSearchCV\nGridSearchCV is a method in the scikit-learn library, which is a popular machine learning library in Python. It’s used for hyperparameter optimization, which involves searching for the best set of hyperparameters for a machine learning model.\n\nLet’s import some packages\nWe begin by importing necessary packages and modules. The KNeighborsRegressor model is imported from the sklearn.neighbors module.\n\n# Let's import some packages\nfrom dataidea.packages import * # imports np, pd, plt, etc\nfrom dataidea.datasets import loadDataset\nfrom sklearn.neighbors import KNeighborsRegressor\n\n\n\nLet’s import necessary components from sklearn\nWe import essential components from sklearn, including Pipeline, which we’ll use to create a pipe as from the previous section, ColumnTransformer, StandardScaler, and OneHotEncoder which we’ll use to transform the numeric and categorical columns respectively to be good for modelling.\n\n# lets import the Pipeline from sklearn\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\n\n\n\nLoading the dataset\nWe load the dataset named boston using the loadDataset function, which is inbuilt in the dataidea package. The loaded dataset is stored in the variable data. More about the boston dataset is in the Pipeline notebook\n\n# loading the data set\ndata = loadDataset('boston')\n\n\n# looking at the top part\ndata.head()\n\n\n\n\n\n\n\n\nCRIM\nZN\nINDUS\nCHAS\nNOX\nRM\nAGE\nDIS\nRAD\nTAX\nPTRATIO\nB\nLSTAT\nMEDV\n\n\n\n\n0\n0.00632\n18.0\n2.31\n0\n0.538\n6.575\n65.2\n4.0900\n1\n296.0\n15.3\n396.90\n4.98\n24.0\n\n\n1\n0.02731\n0.0\n7.07\n0\n0.469\n6.421\n78.9\n4.9671\n2\n242.0\n17.8\n396.90\n9.14\n21.6\n\n\n2\n0.02729\n0.0\n7.07\n0\n0.469\n7.185\n61.1\n4.9671\n2\n242.0\n17.8\n392.83\n4.03\n34.7\n\n\n3\n0.03237\n0.0\n2.18\n0\n0.458\n6.998\n45.8\n6.0622\n3\n222.0\n18.7\n394.63\n2.94\n33.4\n\n\n4\n0.06905\n0.0\n2.18\n0\n0.458\n7.147\n54.2\n6.0622\n3\n222.0\n18.7\n396.90\n5.33\n36.2\n\n\n\n\n\n\n\n\n\nSelecting features (X) and target variable (y)\nWe separate the features (X) from the target variable (y). Features are stored in X, excluding the target variable ‘MEDV’, which is stored in y.\n\n# Selecting our X set and y\nX = data.drop('MEDV', axis=1)\ny = data.MEDV\n\n\n\nDefining numeric and categorical columns\nWe define lists of column names representing numeric and categorical features in the dataset. We identified these columns as the best features from the previous section of this week\n\n# numeric columns\nnumeric_cols = [\n    'INDUS', 'NOX', 'RM', \n    'TAX', 'PTRATIO', 'LSTAT'\n    ]\n    \n# categorical columns\ncategorical_cols = ['CHAS', 'RAD']\n\n\n\nPreprocessing steps\nWe define transformers for preprocessing numeric and categorical features. StandardScaler is used for standardizing numeric features, while OneHotEncoder is used for one-hot encoding categorical features. These transformers are applied to respective feature types using ColumnTransformer as we learned in the previous section.\n\n# Preprocessing steps\nnumeric_transformer = StandardScaler()\ncategorical_transformer = OneHotEncoder(handle_unknown='ignore')\n\n# Combine preprocessing steps\ncolumn_transformer = ColumnTransformer(\n    transformers=[\n        ('numeric', numeric_transformer, numeric_cols),\n        ('categorical', categorical_transformer, categorical_cols)\n    ])\n\n\n\nDefining the pipeline\nWe construct a machine learning pipeline using Pipeline. The pipeline consists of preprocessing steps (defined in column_transformer) and a KNeighborsRegressor model with 10 neighbors.\n\n# Pipeline\npipe = Pipeline([\n    ('column_transformer', column_transformer),\n    ('model', KNeighborsRegressor(n_neighbors=10))\n])\n\npipe\n\nPipeline(steps=[('column_transformer',\n                 ColumnTransformer(transformers=[('numeric', StandardScaler(),\n                                                  ['INDUS', 'NOX', 'RM', 'TAX',\n                                                   'PTRATIO', 'LSTAT']),\n                                                 ('categorical',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['CHAS', 'RAD'])])),\n                ('model', KNeighborsRegressor(n_neighbors=10))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  Pipeline?Documentation for PipelineiNot fittedPipeline(steps=[('column_transformer',\n                 ColumnTransformer(transformers=[('numeric', StandardScaler(),\n                                                  ['INDUS', 'NOX', 'RM', 'TAX',\n                                                   'PTRATIO', 'LSTAT']),\n                                                 ('categorical',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['CHAS', 'RAD'])])),\n                ('model', KNeighborsRegressor(n_neighbors=10))])  column_transformer: ColumnTransformer?Documentation for column_transformer: ColumnTransformerColumnTransformer(transformers=[('numeric', StandardScaler(),\n                                 ['INDUS', 'NOX', 'RM', 'TAX', 'PTRATIO',\n                                  'LSTAT']),\n                                ('categorical',\n                                 OneHotEncoder(handle_unknown='ignore'),\n                                 ['CHAS', 'RAD'])]) numeric['INDUS', 'NOX', 'RM', 'TAX', 'PTRATIO', 'LSTAT']  StandardScaler?Documentation for StandardScalerStandardScaler() categorical['CHAS', 'RAD']  OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(handle_unknown='ignore')  KNeighborsRegressor?Documentation for KNeighborsRegressorKNeighborsRegressor(n_neighbors=10) \n\n\n\n\nFitting the pipeline\nAs we learned, the Pipeline has the fit, score and predict methods which we use to fit on the dataset (X, y) and evaluate the model’s performance using the score() method, finally making predictions.\n\n# Fit the pipeline\npipe.fit(X, y)\n\n# Score the pipeline\npipe_score = pipe.score(X, y)\n\n# Predict using the pipeline\npipe_predicted_y = pipe.predict(X)\n\nprint('Pipe Score:', pipe_score)\n\nPipe Score: 0.818140222027107\n\n\n\n\nHyperparameter tuning using GridSearchCV\nWe perform hyperparameter tuning using GridSearchCV. The pipeline (pipe) serves as the base estimator, and we define a grid of hyperparameters to search through, focusing on the number of neighbors for the KNN model.\n\nfrom sklearn.model_selection import GridSearchCV\n\n\nmodel = GridSearchCV(\n    estimator=pipe,\n    param_grid={\n        'model__n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n    },\n    cv=3\n    )\n\n\n\nFitting the model for hyperparameter tuning\nWe fit the GridSearchCV model on the dataset to find the optimal hyperparameters. This involves preprocessing the data and training the model multiple times using cross-validation.\n\nmodel.fit(X, y)\n\nGridSearchCV(cv=3,\n             estimator=Pipeline(steps=[('column_transformer',\n                                        ColumnTransformer(transformers=[('numeric',\n                                                                         StandardScaler(),\n                                                                         ['INDUS',\n                                                                          'NOX',\n                                                                          'RM',\n                                                                          'TAX',\n                                                                          'PTRATIO',\n                                                                          'LSTAT']),\n                                                                        ('categorical',\n                                                                         OneHotEncoder(handle_unknown='ignore'),\n                                                                         ['CHAS',\n                                                                          'RAD'])])),\n                                       ('model',\n                                        KNeighborsRegressor(n_neighbors=10))]),\n             param_grid={'model__n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=3,\n             estimator=Pipeline(steps=[('column_transformer',\n                                        ColumnTransformer(transformers=[('numeric',\n                                                                         StandardScaler(),\n                                                                         ['INDUS',\n                                                                          'NOX',\n                                                                          'RM',\n                                                                          'TAX',\n                                                                          'PTRATIO',\n                                                                          'LSTAT']),\n                                                                        ('categorical',\n                                                                         OneHotEncoder(handle_unknown='ignore'),\n                                                                         ['CHAS',\n                                                                          'RAD'])])),\n                                       ('model',\n                                        KNeighborsRegressor(n_neighbors=10))]),\n             param_grid={'model__n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}) estimator: PipelinePipeline(steps=[('column_transformer',\n                 ColumnTransformer(transformers=[('numeric', StandardScaler(),\n                                                  ['INDUS', 'NOX', 'RM', 'TAX',\n                                                   'PTRATIO', 'LSTAT']),\n                                                 ('categorical',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['CHAS', 'RAD'])])),\n                ('model', KNeighborsRegressor(n_neighbors=10))])  column_transformer: ColumnTransformer?Documentation for column_transformer: ColumnTransformerColumnTransformer(transformers=[('numeric', StandardScaler(),\n                                 ['INDUS', 'NOX', 'RM', 'TAX', 'PTRATIO',\n                                  'LSTAT']),\n                                ('categorical',\n                                 OneHotEncoder(handle_unknown='ignore'),\n                                 ['CHAS', 'RAD'])]) numeric['INDUS', 'NOX', 'RM', 'TAX', 'PTRATIO', 'LSTAT']  StandardScaler?Documentation for StandardScalerStandardScaler() categorical['CHAS', 'RAD']  OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(handle_unknown='ignore')  KNeighborsRegressor?Documentation for KNeighborsRegressorKNeighborsRegressor(n_neighbors=10) \n\n\n\n\nExtracting and displaying cross-validation results\nWe extract the results of cross-validation performed during hyperparameter tuning and present them in a tabular format using a DataFrame.\n\ncv_results = pd.DataFrame(model.cv_results_)\ncv_results\n\n\n\n\n\n\n\n\nmean_fit_time\nstd_fit_time\nmean_score_time\nstd_score_time\nparam_model__n_neighbors\nparams\nsplit0_test_score\nsplit1_test_score\nsplit2_test_score\nmean_test_score\nstd_test_score\nrank_test_score\n\n\n\n\n0\n0.004732\n0.000748\n0.003540\n0.000235\n1\n{'model__n_neighbors': 1}\n0.347172\n0.561780\n0.295295\n0.401415\n0.115356\n10\n\n\n1\n0.003898\n0.000124\n0.002893\n0.000093\n2\n{'model__n_neighbors': 2}\n0.404829\n0.612498\n0.276690\n0.431339\n0.138369\n9\n\n\n2\n0.004030\n0.000161\n0.003201\n0.000090\n3\n{'model__n_neighbors': 3}\n0.466325\n0.590333\n0.243375\n0.433345\n0.143552\n8\n\n\n3\n0.003658\n0.000036\n0.002809\n0.000061\n4\n{'model__n_neighbors': 4}\n0.569672\n0.619854\n0.246539\n0.478688\n0.165428\n4\n\n\n4\n0.003693\n0.000060\n0.002730\n0.000054\n5\n{'model__n_neighbors': 5}\n0.613900\n0.600994\n0.230320\n0.481738\n0.177857\n2\n\n\n5\n0.003681\n0.000137\n0.002855\n0.000016\n6\n{'model__n_neighbors': 6}\n0.620587\n0.607083\n0.225238\n0.484302\n0.183269\n1\n\n\n6\n0.004109\n0.000393\n0.002984\n0.000106\n7\n{'model__n_neighbors': 7}\n0.639693\n0.583685\n0.218612\n0.480663\n0.186704\n3\n\n\n7\n0.003641\n0.000018\n0.002798\n0.000052\n8\n{'model__n_neighbors': 8}\n0.636143\n0.567841\n0.209472\n0.471152\n0.187125\n5\n\n\n8\n0.003670\n0.000036\n0.002757\n0.000006\n9\n{'model__n_neighbors': 9}\n0.649335\n0.542624\n0.197917\n0.463292\n0.192639\n6\n\n\n9\n0.003590\n0.000023\n0.002807\n0.000031\n10\n{'model__n_neighbors': 10}\n0.653370\n0.535112\n0.191986\n0.460156\n0.195674\n7\n\n\n\n\n\n\n\nThese are the results of a grid search cross-validation performed on our pipeline (pipe). Let’s break down each column:\n\nmean_fit_time: The average time taken to fit the estimator on the training data across all folds.\nstd_fit_time: The standard deviation of the fitting time across all folds.\nmean_score_time: The average time taken to score the estimator on the test data across all folds.\nstd_score_time: The standard deviation of the scoring time across all folds.\nparam_model__n_neighbors: The value of the n_neighbors parameter of the KNeighborsRegressor model in our pipeline for this particular grid search iteration.\nparams: A dictionary containing the parameters used in this grid search iteration.\nsplit0_test_score, split1_test_score, split2_test_score: The test scores obtained for each fold of the cross-validation. Each fold corresponds to one entry here.\nmean_test_score: The average test score across all folds.\nstd_test_score: The standard deviation of the test scores across all folds.\nrank_test_score: The rank of this model configuration based on the mean test score. Lower values indicate better performance.\n\nThese results allow you to compare different parameter configurations and select the one that performs best based on the mean test score and other relevant metrics.\nA few ads maybe displayed for income as resources are now offered freely. 🤝🤝🤝",
    "crumbs": [
      "Python-Data-Analysis",
      "Week8-More-ML",
      "GridSearchCV"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week8-More-ML/80_classification_metrics.html",
    "href": "Python-Data-Analysis/Week8-More-ML/80_classification_metrics.html",
    "title": "DATAIDEA",
    "section": "",
    "text": "Photo by DATAIDEA",
    "crumbs": [
      "Python-Data-Analysis",
      "Week8-More-ML",
      "SciKit-Learn Classification Metrics"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week8-More-ML/80_classification_metrics.html#scikit-learn-classification-metrics",
    "href": "Python-Data-Analysis/Week8-More-ML/80_classification_metrics.html#scikit-learn-classification-metrics",
    "title": "DATAIDEA",
    "section": "SciKit-Learn Classification Metrics",
    "text": "SciKit-Learn Classification Metrics\nIn scikit-learn, classification metrics are essential tools to evaluate the performance of a classification model. They provide insights into how well the model is performing and where it may need improvements. Here are some commonly used classification metrics along with examples of how to implement them using scikit-learn:\n\nimport pandas as pd\n\n\n# True labels\ny_true = [0, 1, 1, 0, 1]\n# Predicted labels\ny_pred = [1, 1, 0, 0, 1]\n\n\nAccuracy:\n\nAccuracy measures the ratio of correctly predicted instances to the total instances.\nFormula:\n\nAccuracy = Number of Correct Predictions / Total Number of Predictions\n\n\n\nphoto by evidentlyai\n\n\n\nfrom sklearn.metrics import accuracy_score\n\n# Calculate accuracy\naccuracy = accuracy_score(y_true=y_true, y_pred=y_pred)\nprint(\"Accuracy:\", accuracy)\n\nAccuracy: 0.6\n\n\n\n\nPrecision:\n\nPrecision measures the ability of the classifier not to label as positive a sample that is negative.\nPrecision is simply the models ability to not make a mistake\nFormula:\n\nPrecision = True Positives / (True Positives + False Positives)\n\n\n\nphoto by evidentlyai\n\n\n\nfrom sklearn.metrics import precision_score\n\n# Calculate precision\nprecision = precision_score(y_true, y_pred)\nprint(\"Precision:\", precision * 100)\n\nPrecision: 66.66666666666666\n\n\n\n\nRecall (also known as Sensitivity or True Positive Rate):\n\nRecall measures the ability of the classifier to find all the positive samples.\nFormula:\n\nRecall = True Positives / (True Positives + False Negatives)\n\n\n\nphoto by evidentlyai\n\n\n\nfrom sklearn.metrics import recall_score\n\n# Calculate recall\nrecall = recall_score(y_true, y_pred)\nprint(\"Recall:\", recall)\n\nRecall: 0.6666666666666666\n\n\n\n\nF1 Score:\n\nF1 Score is the harmonic mean of precision and recall.\nIt provides a balance between precision and recall.\nFormula:\n\nF1 Score = (2 x Precision x Recall) / (Precision + Recall)\n\nfrom sklearn.metrics import f1_score\n\n# Calculate F1 Score\nf1 = f1_score(y_true, y_pred)\nprint(\"F1 Score:\", f1)\n\nF1 Score: 0.6666666666666666\n\n\nIn classification tasks, metrics like precision, recall, and F1-score are commonly used to evaluate the performance of a model. When dealing with multi-class classification, you often need a way to aggregate these metrics across all classes. Three common methods for doing this are micro-average, macro-average, and weighted average.\n\n\nMicro-average:\n\nCalculate metrics globally by counting the total true positives, false negatives, and false positives.\nThis method gives equal weight to each individual prediction, regardless of class imbalance.\n\n\n\n\nphoto by evidentlyai\n\n\n\n# Calculate micro-average\nmicro_precision = precision_score(y_true, y_pred, average='micro')\nmicro_recall = recall_score(y_true, y_pred, average='micro')\nmicro_f1 = f1_score(y_true, y_pred, average='micro')\n\nprint('Micro Precision:', micro_precision)\nprint('Micro Recall:', micro_recall)\nprint('Micro F1:', micro_f1)\n\nMicro Precision: 0.6\nMicro Recall: 0.6\nMicro F1: 0.6\n\n\n\n\nMacro-average:\n\nCalculate metrics for each class individually and then average them.\nThis method treats all classes equally, giving each class the same weight.\nTo obtain macro-averaged precision, recall, and F1-score:\n\nCalculate precision, recall, and F1-score for each class.\nAverage the precision, recall, and F1-score across all classes.\n\n\n\n\n\nphoto by evidentlyai\n\n\n\n# Calculate macro-average\nmacro_precision = precision_score(y_true, y_pred, average='macro')\nmacro_recall = recall_score(y_true, y_pred, average='macro')\nmacro_f1 = f1_score(y_true, y_pred, average='macro')\n\nprint('Macro Precision:', macro_precision)\nprint('Macro Recall:', macro_recall)\nprint('Macro F1:', macro_f1)\n\nMacro Precision: 0.5833333333333333\nMacro Recall: 0.5833333333333333\nMacro F1: 0.5833333333333333\n\n\n\n\nWeighted average:\n\nCalculate metrics for each class individually and then average them, weighted by the number of true instances for each class.\nThis method considers class imbalance by giving more weight to classes with more instances.\nTo obtain weighted-averaged precision, recall, and F1-score:\n\nCalculate precision, recall, and F1-score for each class.\nWeighted average is calculated as the sum of (metric * class_weight) / total_number_of_samples, where class_weight is the ratio of the number of true instances in the given class to the total number of true instances.\n\n\n\n# Calculate weighted-average\nweighted_precision = precision_score(y_true, y_pred, average='weighted')\nweighted_recall = recall_score(y_true, y_pred, average='weighted')\nweighted_f1 = f1_score(y_true, y_pred, average='weighted')\n\nprint('Weighted Precision:', weighted_precision)\nprint('Weighted Recall:', weighted_recall)\nprint('Weighted F1:', weighted_f1)\n\nWeighted Precision: 0.6\nWeighted Recall: 0.6\nWeighted F1: 0.6\n\n\nWeighted_precision:\n(Precision_A * N_A + Precision_B * N_B, ... , Precision_n * N_n) / (N_A + N_B + ... + N_n)\n\nMicro-average is useful when overall performance across all classes is important\nMacro-average is helpful when you want to evaluate the model’s performance on smaller classes equally.\nWeighted average is suitable when you want to account for class imbalance.\n\n\n\nThe Classification Report\nThe classification report in scikit-learn provides a comprehensive summary of different classification metrics for each class in the dataset. It includes precision, recall, F1-score, and support (the number of true instances for each label). Here’s how you can generate a classification report:\n\nfrom sklearn.metrics import classification_report\n\n# Generate classification report\nclass_report_dict = classification_report(y_true, y_pred, output_dict=True)\nclass_report_df = pd.DataFrame(class_report_dict).transpose()\n\nprint(\"Classification Report:\\n\")\nclass_report_df\n\nClassification Report:\n\n\n\n\n\n\n\n\n\n\nprecision\nrecall\nf1-score\nsupport\n\n\n\n\n0\n0.500000\n0.500000\n0.500000\n2.0\n\n\n1\n0.666667\n0.666667\n0.666667\n3.0\n\n\naccuracy\n0.600000\n0.600000\n0.600000\n0.6\n\n\nmacro avg\n0.583333\n0.583333\n0.583333\n5.0\n\n\nweighted avg\n0.600000\n0.600000\n0.600000\n5.0\n\n\n\n\n\n\n\n\n(0.666667 + 0.5)/2\n\n0.5833335\n\n\n\n\nConfusion Matrix:\n\nA confusion matrix is a table that is often used to describe the performance of a classification model.\nIt presents a summary of the model’s predictions on the classification problem, showing correct predictions as well as types of errors made.\n\n\nfrom sklearn.metrics import confusion_matrix\nimport pandas as pd\n\n# Calculate confusion matrix\nconf_matrix = confusion_matrix(y_true, y_pred)\n\nconf_matrix = pd.DataFrame(conf_matrix, index=[0, 1], columns=[0, 1])\n# print(\"Confusion Matrix:\\n\", conf_matrix)\nconf_matrix\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n0\n1\n1\n\n\n1\n1\n2\n\n\n\n\n\n\n\nThese are just a few of the many classification metrics available in scikit-learn. Depending on your specific problem and requirements, you may want to explore other metrics as well.\nUnderstanding these metrics and how they are computed can provide valuable insights into the performance of a classification model and help in making informed decisions about its improvement.\nA few ads maybe displayed for income as resources are now offered freely. 🤝🤝🤝",
    "crumbs": [
      "Python-Data-Analysis",
      "Week8-More-ML",
      "SciKit-Learn Classification Metrics"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week2-Cleaning-Intro/pandas-puzzles.html",
    "href": "Python-Data-Analysis/Week2-Cleaning-Intro/pandas-puzzles.html",
    "title": "60 pandas puzzles",
    "section": "",
    "text": "Photo by DATAIDEA\nSince pandas is a large library with many different specialist features and functions, these excercises focus mainly on the fundamentals of manipulating data (indexing, grouping, aggregating, cleaning), making use of the core DataFrame and Series objects.\nMany of the excerises here are stright-forward in that the solutions require no more than a few lines of code (in pandas or NumPy… don’t go using pure Python or Cython!). Choosing the right methods and following best practices is the underlying goal.\nThe exercises are loosely divided in sections. Each section has a difficulty rating; these ratings are subjective, of course, but should be a seen as a rough guide as to how inventive the required solution is.\nIf you’re just starting out with pandas and you are looking for some other resources, the official documentation is very extensive. In particular, some good places get a broader overview of pandas are…\nEnjoy the puzzles!",
    "crumbs": [
      "Python-Data-Analysis",
      "Week2-Cleaning-Intro",
      "60 pandas puzzles"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week2-Cleaning-Intro/pandas-puzzles.html#importing-pandas",
    "href": "Python-Data-Analysis/Week2-Cleaning-Intro/pandas-puzzles.html#importing-pandas",
    "title": "60 pandas puzzles",
    "section": "Importing pandas",
    "text": "Importing pandas\n\nGetting started and checking your pandas setup\nDifficulty: easy\n1. Import pandas under the alias pd.\n\nimport pandas as pd\n\n\n# solution\n\n2. Print the version of pandas that has been imported.\n\n# solution\n\n3. Print out all the version information of the libraries that are required by the pandas library.\n\n# solution",
    "crumbs": [
      "Python-Data-Analysis",
      "Week2-Cleaning-Intro",
      "60 pandas puzzles"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week2-Cleaning-Intro/pandas-puzzles.html#dataframe-basics",
    "href": "Python-Data-Analysis/Week2-Cleaning-Intro/pandas-puzzles.html#dataframe-basics",
    "title": "60 pandas puzzles",
    "section": "DataFrame basics",
    "text": "DataFrame basics\n\nA few of the fundamental routines for selecting, sorting, adding and aggregating data in DataFrames\nDifficulty: easy\nNote: remember to import numpy using:\nimport numpy as np\nConsider the following Python dictionary data and Python list labels:\ndata = {'animal': ['cat', 'cat', 'snake', 'dog', 'dog', 'cat', 'snake', 'cat', 'dog', 'dog'],\n        'age': [2.5, 3, 0.5, np.nan, 5, 2, 4.5, np.nan, 7, 3],\n        'visits': [1, 3, 2, 3, 2, 3, 1, 1, 2, 1],\n        'priority': ['yes', 'yes', 'no', 'yes', 'no', 'no', 'no', 'yes', 'no', 'no']}\n\nlabels = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']\n(This is just some meaningless data I made up with the theme of animals and trips to a vet.)\n4. Create a DataFrame df from this dictionary data which has the index labels.\n\nimport numpy as np\n\ndata = {'animal': ['cat', 'cat', 'snake', 'dog', 'dog', 'cat', 'snake', 'cat', 'dog', 'dog'],\n        'age': [2.5, 3, 0.5, np.nan, 5, 2, 4.5, np.nan, 7, 3],\n        'visits': [1, 3, 2, 3, 2, 3, 1, 1, 2, 1],\n        'priority': ['yes', 'yes', 'no', 'yes', 'no', 'no', 'no', 'yes', 'no', 'no']}\n\nlabels = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']\n\n# df =  (complete this line of code)\n\n5. Display a summary of the basic information about this DataFrame and its data (hint: there is a single method that can be called on the DataFrame).\n\n# solution\n\n6. Return the first 3 rows of the DataFrame df.\n\n# solution\n\n7. Select just the ‘animal’ and ‘age’ columns from the DataFrame df.\n\n# solution\n\n8. Select the data in rows [3, 4, 8] and in columns ['animal', 'age'].\n\n# solution\n\n9. Select only the rows where the number of visits is greater than 3.\n\n# solution\n\n\n\n\n\n10. Select the rows where the age is missing, i.e. it is NaN.\n\n# solution\n\n11. Select the rows where the animal is a cat and the age is less than 3.\n\n# solution\n\n12. Select the rows the age is between 2 and 4 (inclusive).\n\n# solution\n\n13. Change the age in row ‘f’ to 1.5.\n\n# solution\n\n14. Calculate the sum of all visits in df (i.e. find the total number of visits).\n\n# solution\n\n\n\n\n\n15. Calculate the mean age for each different animal in df.\n\n# solution\n\n16. Append a new row ‘k’ to df with your choice of values for each column. Then delete that row to return the original DataFrame.\n\n# solution\n\n17. Count the number of each type of animal in df.\n\n# solution\n\n18. Sort df first by the values in the ‘age’ in decending order, then by the value in the ‘visits’ column in ascending order (so row i should be first, and row d should be last).\n\n# solution\n\n19. The ‘priority’ column contains the values ‘yes’ and ‘no’. Replace this column with a column of boolean values: ‘yes’ should be True and ‘no’ should be False.\n\n# solution\n\n\n\n\n\n20. In the ‘animal’ column, change the ‘snake’ entries to ‘python’.\n\n# solution\n\n21. For each animal type and each number of visits, find the mean age. In other words, each row is an animal, each column is a number of visits and the values are the mean ages (hint: use a pivot table).\n\n# solution",
    "crumbs": [
      "Python-Data-Analysis",
      "Week2-Cleaning-Intro",
      "60 pandas puzzles"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week2-Cleaning-Intro/pandas-puzzles.html#dataframes-beyond-the-basics",
    "href": "Python-Data-Analysis/Week2-Cleaning-Intro/pandas-puzzles.html#dataframes-beyond-the-basics",
    "title": "60 pandas puzzles",
    "section": "DataFrames: beyond the basics",
    "text": "DataFrames: beyond the basics\n\nSlightly trickier: you may need to combine two or more methods to get the right answer\nDifficulty: medium\nThe previous section was tour through some basic but essential DataFrame operations. Below are some ways that you might need to cut your data, but for which there is no single “out of the box” method.\n22. You have a DataFrame df with a column ‘A’ of integers. For example:\ndf = pd.DataFrame({'A': [1, 2, 2, 3, 4, 5, 5, 5, 6, 7, 7]})\nHow do you filter out rows which contain the same integer as the row immediately above?\nYou should be left with a column containing the following values:\n1, 2, 3, 4, 5, 6, 7\n\n# solution\n\n23. Given a DataFrame of numeric values, say\ndf = pd.DataFrame(np.random.random(size=(5, 3))) # a 5x3 frame of float values\nhow do you subtract the row mean from each element in the row?\n\n# solution\n\n24. Suppose you have DataFrame with 10 columns of real numbers, for example:\ndf = pd.DataFrame(np.random.random(size=(5, 10)), columns=list('abcdefghij'))\nWhich column of numbers has the smallest sum? Return that column’s label.\n\n# solution\n\n\n\n\n\n25. How do you count how many unique rows a DataFrame has (i.e. ignore all rows that are duplicates)? As input, use a DataFrame of zeros and ones with 10 rows and 3 columns.\ndf = pd.DataFrame(np.random.randint(0, 2, size=(10, 3)))\n\n# solution\n\nThe next three puzzles are slightly harder.\n26. In the cell below, you have a DataFrame df that consists of 10 columns of floating-point numbers. Exactly 5 entries in each row are NaN values.\nFor each row of the DataFrame, find the column which contains the third NaN value.\nYou should return a Series of column labels: e, c, d, h, d\n\nnan = np.nan\n\ndata = [[0.04,  nan,  nan, 0.25,  nan, 0.43, 0.71, 0.51,  nan,  nan],\n        [ nan,  nan,  nan, 0.04, 0.76,  nan,  nan, 0.67, 0.76, 0.16],\n        [ nan,  nan, 0.5 ,  nan, 0.31, 0.4 ,  nan,  nan, 0.24, 0.01],\n        [0.49,  nan,  nan, 0.62, 0.73, 0.26, 0.85,  nan,  nan,  nan],\n        [ nan,  nan, 0.41,  nan, 0.05,  nan, 0.61,  nan, 0.48, 0.68]]\n\ncolumns = list('abcdefghij')\n\ndf = pd.DataFrame(data, columns=columns)\n\n# write a solution to the question here\n\n27. A DataFrame has a column of groups ‘grps’ and and column of integer values ‘vals’:\ndf = pd.DataFrame({'grps': list('aaabbcaabcccbbc'), \n                   'vals': [12,345,3,1,45,14,4,52,54,23,235,21,57,3,87]})\nFor each group, find the sum of the three greatest values. You should end up with the answer as follows:\ngrps\na    409\nb    156\nc    345\n\ndf = pd.DataFrame({'grps': list('aaabbcaabcccbbc'), \n                   'vals': [12,345,3,1,45,14,4,52,54,23,235,21,57,3,87]})\n\n# write a solution to the question here\n\n28. The DataFrame df constructed below has two integer columns ‘A’ and ‘B’. The values in ‘A’ are between 1 and 100 (inclusive).\nFor each group of 10 consecutive integers in ‘A’ (i.e. (0, 10], (10, 20], …), calculate the sum of the corresponding values in column ‘B’.\nThe answer should be a Series as follows:\nA\n(0, 10]      635\n(10, 20]     360\n(20, 30]     315\n(30, 40]     306\n(40, 50]     750\n(50, 60]     284\n(60, 70]     424\n(70, 80]     526\n(80, 90]     835\n(90, 100]    852\n\ndf = pd.DataFrame(np.random.RandomState(8765).randint(1, 101, size=(100, 2)), columns = [\"A\", \"B\"])\n\n# write a solution to the question here",
    "crumbs": [
      "Python-Data-Analysis",
      "Week2-Cleaning-Intro",
      "60 pandas puzzles"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week2-Cleaning-Intro/pandas-puzzles.html#dataframes-harder-problems",
    "href": "Python-Data-Analysis/Week2-Cleaning-Intro/pandas-puzzles.html#dataframes-harder-problems",
    "title": "60 pandas puzzles",
    "section": "DataFrames: harder problems",
    "text": "DataFrames: harder problems\n\nThese might require a bit of thinking outside the box…\n…but all are solvable using just the usual pandas/NumPy methods (and so avoid using explicit for loops).\nDifficulty: hard\n29. Consider a DataFrame df where there is an integer column ‘X’:\ndf = pd.DataFrame({'X': [7, 2, 0, 3, 4, 2, 5, 0, 3, 4]})\nFor each value, count the difference back to the previous zero (or the start of the Series, whichever is closer). These values should therefore be\n[1, 2, 0, 1, 2, 3, 4, 0, 1, 2]\nMake this a new column ‘Y’.\n\n# solution\n\n\n\n\n\n30. Consider the DataFrame constructed below which contains rows and columns of numerical data.\nCreate a list of the column-row index locations of the 3 largest values in this DataFrame. In this case, the answer should be:\n[(5, 7), (6, 4), (2, 5)]\n\ndf = pd.DataFrame(np.random.RandomState(30).randint(1, 101, size=(8, 8)))\n\n31. You are given the DataFrame below with a column of group IDs, ‘grps’, and a column of corresponding integer values, ‘vals’.\ndf = pd.DataFrame({\"vals\": np.random.RandomState(31).randint(-30, 30, size=15), \n                   \"grps\": np.random.RandomState(31).choice([\"A\", \"B\"], 15)})\nCreate a new column ‘patched_values’ which contains the same values as the ‘vals’ any negative values in ‘vals’ with the group mean:\n    vals grps  patched_vals\n0    -12    A          13.6\n1     -7    B          28.0\n2    -14    A          13.6\n3      4    A           4.0\n4     -7    A          13.6\n5     28    B          28.0\n6     -2    A          13.6\n7     -1    A          13.6\n8      8    A           8.0\n9     -2    B          28.0\n10    28    A          28.0\n11    12    A          12.0\n12    16    A          16.0\n13   -24    A          13.6\n14   -12    A          13.6\n\n# solution\n\n32. Implement a rolling mean over groups with window size 3, which ignores NaN value. For example consider the following DataFrame:\n&gt;&gt;&gt; df = pd.DataFrame({'group': list('aabbabbbabab'),\n                       'value': [1, 2, 3, np.nan, 2, 3, np.nan, 1, 7, 3, np.nan, 8]})\n&gt;&gt;&gt; df\n   group  value\n0      a    1.0\n1      a    2.0\n2      b    3.0\n3      b    NaN\n4      a    2.0\n5      b    3.0\n6      b    NaN\n7      b    1.0\n8      a    7.0\n9      b    3.0\n10     a    NaN\n11     b    8.0\nThe goal is to compute the Series:\n0     1.000000\n1     1.500000\n2     3.000000\n3     3.000000\n4     1.666667\n5     3.000000\n6     3.000000\n7     2.000000\n8     3.666667\n9     2.000000\n10    4.500000\n11    4.000000\nE.g. the first window of size three for group ‘b’ has values 3.0, NaN and 3.0 and occurs at row index 5. Instead of being NaN the value in the new column at this row index should be 3.0 (just the two non-NaN values are used to compute the mean (3+3)/2)\n\n# solution",
    "crumbs": [
      "Python-Data-Analysis",
      "Week2-Cleaning-Intro",
      "60 pandas puzzles"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week2-Cleaning-Intro/pandas-puzzles.html#series-and-datetimeindex",
    "href": "Python-Data-Analysis/Week2-Cleaning-Intro/pandas-puzzles.html#series-and-datetimeindex",
    "title": "60 pandas puzzles",
    "section": "Series and DatetimeIndex",
    "text": "Series and DatetimeIndex\n\nExercises for creating and manipulating Series with datetime data\nDifficulty: easy/medium\npandas is fantastic for working with dates and times. These puzzles explore some of this functionality.\n33. Create a DatetimeIndex that contains each business day of 2015 and use it to index a Series of random numbers. Let’s call this Series s.\n\n# solution\n\n34. Find the sum of the values in s for every Wednesday.\n\n# solution\n\n\n\n\n\n35. For each calendar month in s, find the mean of values.\n\n# solution\n\n36. For each group of four consecutive calendar months in s, find the date on which the highest value occurred.\n\n# solution\n\n37. Create a DateTimeIndex consisting of the third Thursday in each month for the years 2015 and 2016.\n\n# solution",
    "crumbs": [
      "Python-Data-Analysis",
      "Week2-Cleaning-Intro",
      "60 pandas puzzles"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week2-Cleaning-Intro/pandas-puzzles.html#cleaning-data",
    "href": "Python-Data-Analysis/Week2-Cleaning-Intro/pandas-puzzles.html#cleaning-data",
    "title": "60 pandas puzzles",
    "section": "Cleaning Data",
    "text": "Cleaning Data\n\nMaking a DataFrame easier to work with\nDifficulty: easy/medium\nIt happens all the time: someone gives you data containing malformed strings, Python, lists and missing data. How do you tidy it up so you can get on with the analysis?\nTake this monstrosity as the DataFrame to use in the following puzzles:\ndf = pd.DataFrame({'From_To': ['LoNDon_paris', 'MAdrid_miLAN', 'londON_StockhOlm', \n                               'Budapest_PaRis', 'Brussels_londOn'],\n              'FlightNumber': [10045, np.nan, 10065, np.nan, 10085],\n              'RecentDelays': [[23, 47], [], [24, 43, 87], [13], [67, 32]],\n                   'Airline': ['KLM(!)', '&lt;Air France&gt; (12)', '(British Airways. )', \n                               '12. Air France', '\"Swiss Air\"']})\nFormatted, it looks like this:\n            From_To  FlightNumber  RecentDelays              Airline\n0      LoNDon_paris       10045.0      [23, 47]               KLM(!)\n1      MAdrid_miLAN           NaN            []    &lt;Air France&gt; (12)\n2  londON_StockhOlm       10065.0  [24, 43, 87]  (British Airways. )\n3    Budapest_PaRis           NaN          [13]       12. Air France\n4   Brussels_londOn       10085.0      [67, 32]          \"Swiss Air\"\n(It’s some flight data I made up; it’s not meant to be accurate in any way.)\n38. Some values in the the FlightNumber column are missing (they are NaN). These numbers are meant to increase by 10 with each row so 10055 and 10075 need to be put in place. Modify df to fill in these missing numbers and make the column an integer column (instead of a float column).\n\n# solution\n\n39. The From_To column would be better as two separate columns! Split each string on the underscore delimiter _ to give a new temporary DataFrame called ‘temp’ with the correct values. Assign the correct column names ‘From’ and ‘To’ to this temporary DataFrame.\n\n# solution\n\n\n\n\n\n40. Notice how the capitalisation of the city names is all mixed up in this temporary DataFrame ‘temp’. Standardise the strings so that only the first letter is uppercase (e.g. “londON” should become “London”.)\n\n# solution\n\n41. Delete the From_To column from df and attach the temporary DataFrame ‘temp’ from the previous questions.\n\n# solution\n\n42. In the Airline column, you can see some extra puctuation and symbols have appeared around the airline names. Pull out just the airline name. E.g. '(British Airways. )' should become 'British Airways'.\n\n# solution\n\n43. In the RecentDelays column, the values have been entered into the DataFrame as a list. We would like each first value in its own column, each second value in its own column, and so on. If there isn’t an Nth value, the value should be NaN.\nExpand the Series of lists into a DataFrame named delays, rename the columns delay_1, delay_2, etc. and replace the unwanted RecentDelays column in df with delays.\n\n# solution\n\nThe DataFrame should look much better now.\n   FlightNumber          Airline      From         To  delay_1  delay_2  delay_3\n0         10045              KLM    London      Paris     23.0     47.0      NaN\n1         10055       Air France    Madrid      Milan      NaN      NaN      NaN\n2         10065  British Airways    London  Stockholm     24.0     43.0     87.0\n3         10075       Air France  Budapest      Paris     13.0      NaN      NaN\n4         10085        Swiss Air  Brussels     London     67.0     32.0      NaN",
    "crumbs": [
      "Python-Data-Analysis",
      "Week2-Cleaning-Intro",
      "60 pandas puzzles"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week2-Cleaning-Intro/pandas-puzzles.html#using-multiindexes",
    "href": "Python-Data-Analysis/Week2-Cleaning-Intro/pandas-puzzles.html#using-multiindexes",
    "title": "60 pandas puzzles",
    "section": "Using MultiIndexes",
    "text": "Using MultiIndexes\n\nGo beyond flat DataFrames with additional index levels\nDifficulty: medium\nPrevious exercises have seen us analysing data from DataFrames equipped with a single index level. However, pandas also gives you the possibilty of indexing your data using multiple levels. This is very much like adding new dimensions to a Series or a DataFrame. For example, a Series is 1D, but by using a MultiIndex with 2 levels we gain of much the same functionality as a 2D DataFrame.\nThe set of puzzles below explores how you might use multiple index levels to enhance data analysis.\nTo warm up, we’ll look make a Series with two index levels.\n44. Given the lists letters = ['A', 'B', 'C'] and numbers = list(range(10)), construct a MultiIndex object from the product of the two lists. Use it to index a Series of random numbers. Call this Series s.\n\n# solution\n\n\n\n\n\n45. Check the index of s is lexicographically sorted (this is a necessary proprty for indexing to work correctly with a MultiIndex).\n\n# solution\n\n46. Select the labels 1, 3 and 6 from the second level of the MultiIndexed Series.\n\n# solution\n\n47. Slice the Series s; slice up to label ‘B’ for the first level and from label 5 onwards for the second level.\n\n# solution\n\n48. Sum the values in s for each label in the first level (you should have Series giving you a total for labels A, B and C).\n\n# solution\n\n49. Suppose that sum() (and other methods) did not accept a level keyword argument. How else could you perform the equivalent of s.sum(level=1)?\n\n# solution\n\n\n\n\n\n50. Exchange the levels of the MultiIndex so we have an index of the form (letters, numbers). Is this new Series properly lexsorted? If not, sort it.\n\n# solution",
    "crumbs": [
      "Python-Data-Analysis",
      "Week2-Cleaning-Intro",
      "60 pandas puzzles"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week2-Cleaning-Intro/pandas-puzzles.html#minesweeper",
    "href": "Python-Data-Analysis/Week2-Cleaning-Intro/pandas-puzzles.html#minesweeper",
    "title": "60 pandas puzzles",
    "section": "Minesweeper",
    "text": "Minesweeper\n\nGenerate the numbers for safe squares in a Minesweeper grid\nDifficulty: medium to hard\nIf you’ve ever used an older version of Windows, there’s a good chance you’ve played with Minesweeper: - https://en.wikipedia.org/wiki/Minesweeper_(video_game)\nIf you’re not familiar with the game, imagine a grid of squares: some of these squares conceal a mine. If you click on a mine, you lose instantly. If you click on a safe square, you reveal a number telling you how many mines are found in the squares that are immediately adjacent. The aim of the game is to uncover all squares in the grid that do not contain a mine.\nIn this section, we’ll make a DataFrame that contains the necessary data for a game of Minesweeper: coordinates of the squares, whether the square contains a mine and the number of mines found on adjacent squares.\n51. Let’s suppose we’re playing Minesweeper on a 5 by 4 grid, i.e.\nX = 5\nY = 4\nTo begin, generate a DataFrame df with two columns, 'x' and 'y' containing every coordinate for this grid. That is, the DataFrame should start:\n   x  y\n0  0  0\n1  0  1\n2  0  2\n\n# solution\n\n52. For this DataFrame df, create a new column of zeros (safe) and ones (mine). The probability of a mine occuring at each location should be 0.4.\n\n# solution\n\n53. Now create a new column for this DataFrame called 'adjacent'. This column should contain the number of mines found on adjacent squares in the grid.\n(E.g. for the first row, which is the entry for the coordinate (0, 0), count how many mines are found on the coordinates (0, 1), (1, 0) and (1, 1).)\n\n# solution\n\n54. For rows of the DataFrame that contain a mine, set the value in the 'adjacent' column to NaN.\n\n# solution\n\n\n\n\n\n55. Finally, convert the DataFrame to grid of the adjacent mine counts: columns are the x coordinate, rows are the y coordinate.\n\n# solution",
    "crumbs": [
      "Python-Data-Analysis",
      "Week2-Cleaning-Intro",
      "60 pandas puzzles"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week2-Cleaning-Intro/pandas-puzzles.html#plotting",
    "href": "Python-Data-Analysis/Week2-Cleaning-Intro/pandas-puzzles.html#plotting",
    "title": "60 pandas puzzles",
    "section": "Plotting",
    "text": "Plotting\n\nVisualize trends and patterns in data\nDifficulty: medium\nTo really get a good understanding of the data contained in your DataFrame, it is often essential to create plots: if you’re lucky, trends and anomalies will jump right out at you. This functionality is baked into pandas and the puzzles below explore some of what’s possible with the library.\n56. Pandas is highly integrated with the plotting library matplotlib, and makes plotting DataFrames very user-friendly! Plotting in a notebook environment usually makes use of the following boilerplate:\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use('ggplot')\nmatplotlib is the plotting library which pandas’ plotting functionality is built upon, and it is usually aliased to plt.\n%matplotlib inline tells the notebook to show plots inline, instead of creating them in a separate window.\nplt.style.use('ggplot') is a style theme that most people find agreeable, based upon the styling of R’s ggplot package.\nFor starters, make a scatter plot of this random data, but use black X’s instead of the default markers.\ndf = pd.DataFrame({\"xs\":[1,5,2,8,1], \"ys\":[4,2,1,9,6]})\nConsult the documentation if you get stuck!\n\n# solution\n\n57. Columns in your DataFrame can also be used to modify colors and sizes. Bill has been keeping track of his performance at work over time, as well as how good he was feeling that day, and whether he had a cup of coffee in the morning. Make a plot which incorporates all four features of this DataFrame.\n(Hint: If you’re having trouble seeing the plot, try multiplying the Series which you choose to represent size by 10 or more)\nThe chart doesn’t have to be pretty: this isn’t a course in data viz!\ndf = pd.DataFrame({\"productivity\":[5,2,3,1,4,5,6,7,8,3,4,8,9],\n                   \"hours_in\"    :[1,9,6,5,3,9,2,9,1,7,4,2,2],\n                   \"happiness\"   :[2,1,3,2,3,1,2,3,1,2,2,1,3],\n                   \"caffienated\" :[0,0,1,1,0,0,0,0,1,1,0,1,0]})\n\n# solution\n\n58. What if we want to plot multiple things? Pandas allows you to pass in a matplotlib Axis object for plots, and plots will also return an Axis object.\nMake a bar plot of monthly revenue with a line plot of monthly advertising spending (numbers in millions)\ndf = pd.DataFrame({\"revenue\":[57,68,63,71,72,90,80,62,59,51,47,52],\n                   \"advertising\":[2.1,1.9,2.7,3.0,3.6,3.2,2.7,2.4,1.8,1.6,1.3,1.9],\n                   \"month\":range(12)\n                  })\n\n# solution\n\nNow we’re finally ready to create a candlestick chart, which is a very common tool used to analyze stock price data. A candlestick chart shows the opening, closing, highest, and lowest price for a stock during a time window. The color of the “candle” (the thick part of the bar) is green if the stock closed above its opening price, or red if below.\n\n\n\nCandlestick Example\n\n\nThis was initially designed to be a pandas plotting challenge, but it just so happens that this type of plot is just not feasible using pandas’ methods. If you are unfamiliar with matplotlib, we have provided a function that will plot the chart for you so long as you can use pandas to get the data into the correct format.\nYour first step should be to get the data in the correct format using pandas’ time-series grouping function. We would like each candle to represent an hour’s worth of data. You can write your own aggregation function which returns the open/high/low/close, but pandas has a built-in which also does this.\nThe below cell contains helper functions. Call day_stock_data() to generate a DataFrame containing the prices a hypothetical stock sold for, and the time the sale occurred. Call plot_candlestick(df) on your properly aggregated and formatted stock data to print the candlestick chart.\n\nimport numpy as np\ndef float_to_time(x):\n    return str(int(x)) + \":\" + str(int(x%1 * 60)).zfill(2) + \":\" + str(int(x*60 % 1 * 60)).zfill(2)\n\ndef day_stock_data():\n    #NYSE is open from 9:30 to 4:00\n    time = 9.5\n    price = 100\n    results = [(float_to_time(time), price)]\n    while time &lt; 16:\n        elapsed = np.random.exponential(.001)\n        time += elapsed\n        if time &gt; 16:\n            break\n        price_diff = np.random.uniform(.999, 1.001)\n        price *= price_diff\n        results.append((float_to_time(time), price))\n    \n    \n    df = pd.DataFrame(results, columns = ['time','price'])\n    df.time = pd.to_datetime(df.time)\n    return df\n\n#Don't read me unless you get stuck!\ndef plot_candlestick(agg):\n    \"\"\"\n    agg is a DataFrame which has a DatetimeIndex and five columns: [\"open\",\"high\",\"low\",\"close\",\"color\"]\n    \"\"\"\n    fig, ax = plt.subplots()\n    for time in agg.index:\n        ax.plot([time.hour] * 2, agg.loc[time, [\"high\",\"low\"]].values, color = \"black\")\n        ax.plot([time.hour] * 2, agg.loc[time, [\"open\",\"close\"]].values, color = agg.loc[time, \"color\"], linewidth = 10)\n\n    ax.set_xlim((8,16))\n    ax.set_ylabel(\"Price\")\n    ax.set_xlabel(\"Hour\")\n    ax.set_title(\"OHLC of Stock Value During Trading Day\")\n    plt.show()\n\n59. Generate a day’s worth of random stock data, and aggregate / reformat it so that it has hourly summaries of the opening, highest, lowest, and closing prices\n\n# solution\n\n\n\n\n\n60. Now that you have your properly-formatted data, try to plot it yourself as a candlestick chart. Use the plot_candlestick(df) function above, or matplotlib’s plot documentation if you get stuck.\n\n# solution\n\nMore exercises to follow soon…",
    "crumbs": [
      "Python-Data-Analysis",
      "Week2-Cleaning-Intro",
      "60 pandas puzzles"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to DATAIDEA",
    "section": "",
    "text": "NOTE:",
    "crumbs": [
      "Welcome to DATAIDEA"
    ]
  },
  {
    "objectID": "index.html#learn-programming-for-data-science",
    "href": "index.html#learn-programming-for-data-science",
    "title": "Welcome to DATAIDEA",
    "section": "Learn Programming For Data Science",
    "text": "Learn Programming For Data Science\n\nProgramming for Data Science is a subject we’ve designed to explore the various programming components of data science.\nYou’ll learn programming techniques to do Data Science stuff like data manipulation, analysis, visualization, machine learning.\n\nThis subject has been split into specific courses as below. The duration is the average of what our older students have taken, but can be longer or shorter for different types of students:\n\nFundamentals of Programming (Python)(1 month) \nData Analysis with Python  (2 months)\nMachine Learning (1 month) \nDeep Learning with Pytorch (3-5 months) \nDatabase Management with SQL and Python (optional) (1 month)\nWeb Development with Django (optional) (2-6 months) \n\nThis subject can be taken as a whole or you can take specific parts of strong interest or application to you and your field. But if you’re a complete beginner, we advise you take the whole subject.\nHere is how our subject is offered:\n\nDuration: (Depends on the course units you have chosen)\nSessions: Four times a week (one on one)\nTiming: Evenings or/and weekends\nLocation: Online or/and at UMF House, Sir Apollo Kagwa Road\n\nIf you’re serious about learning Programming and getting prepared for Data Science and Web Development roles, I highly encourage you to enroll in our programming courses.\nDon’t waste your time following disconnected, outdated tutorials. We can teach you all that you need to kickstart your career.\nContact us at:\n\ndataideaorg@gmail.com\n+256701520768 / +256771754118\nwww.dataidea.org",
    "crumbs": [
      "Welcome to DATAIDEA"
    ]
  },
  {
    "objectID": "Python/11_python_objective_quiz.html",
    "href": "Python/11_python_objective_quiz.html",
    "title": "Python Objective Quiz",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\n\nQuestion 1:\nWhat is the output of the following code?\nprint(type([]) is list)\n\nTrue\n\nFalse\n\nError\n\nNone\n\n\n\nAnswer:\n\nTrue\n\n\n\n\nQuestion 2:\nWhich of the following is not a valid Python data type?\n\nlist\n\ntuple\n\ndictionary\n\narray\n\n\n\nAnswer:\n\narray\n\n\n\n\nQuestion 3:\nWhat does the len() function do?\n\nReturns the number of characters in a string\n\nReturns the number of items in a list or tuple\n\nReturns the number of key-value pairs in a dictionary\n\nAll of the above\n\n\n\nAnswer:\n\nAll of the above\n\n\n\n\n\n\n\n\n\nQuestion 4:\nHow do you create a set in Python?\n\nmy_set = {}\n\nmy_set = []\n\nmy_set = ()\n\nmy_set = set()\n\n\n\nAnswer:\n\nmy_set = set()\n\n\n\n\nQuestion 5:\nWhat is the output of the following code?\nx = \"Python\"\nprint(x[0])\n\nP\n\ny\n\nh\n\nn\n\n\n\nAnswer:\n\nP\n\n\n\n\nQuestion 6:\nWhich of the following methods can be used to add an element to the end of a list in Python?\n\nappend()\n\nadd()\n\ninsert()\n\nextend()\n\n\n\nAnswer:\n\nappend()\n\n\n\n\n\n\n\n\n\nQuestion 7:\nWhat is the output of the following code?\nfor i in range(3):\n    print(i)\n\n1 2 3\n\n0 1 2\n\n0 1 2 3\n\n1 2 3 4\n\n\n\nAnswer:\n\n0 1 2\n\n\n\n\nQuestion 8:\nHow do you create a function in Python?\n\nfunction my_func():\n\ndef my_func:\n\ndef my_func():\n\nfunc my_func():\n\n\n\nAnswer:\n\ndef my_func():\n\n\n\n\nQuestion 9:\nWhat will be the output of the following code?\nprint(2**3)\n\n6\n\n8\n\n9\n\n11\n\n\n\nAnswer:\n\n8\n\n\n\n\nQuestion 10:\nWhich keyword is used to handle exceptions in Python?\n\ntry\n\ncatch\n\nexcept\n\nhandle\n\n\n\nAnswer:\n\nexcept\n\n\nTo be among the first to hear about future updates, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel.\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Python",
      "Python Objective Quiz"
    ]
  },
  {
    "objectID": "Python/07_advanced.html",
    "href": "Python/07_advanced.html",
    "title": "DATAIDEA",
    "section": "",
    "text": "Photo by DATAIDEA",
    "crumbs": [
      "Python",
      "Advanced"
    ]
  },
  {
    "objectID": "Python/07_advanced.html#advanced",
    "href": "Python/07_advanced.html#advanced",
    "title": "DATAIDEA",
    "section": "Advanced",
    "text": "Advanced\nIn this notebook, we’ll introduce and discuss the following concepts in Python\n\nClasses and Objects\nFormatted Strings\nHandling Errors\nVariable Scopes",
    "crumbs": [
      "Python",
      "Advanced"
    ]
  },
  {
    "objectID": "Python/07_advanced.html#classes-and-objects",
    "href": "Python/07_advanced.html#classes-and-objects",
    "title": "DATAIDEA",
    "section": "Classes and Objects",
    "text": "Classes and Objects\n\n \n\nIn Python, everything is an object. A class helps us create objects.\n\nCreating a Class\nUse the class keyword to create a class\n\nclass Person:\n    first_name = \"Betty\"\n    last_name = \"Kawala\"\n    age = 30\n\n\n\nInstantiating a class\nNow we can ceate an object from the class by instantiating it.\nTo instantiate a class, add round brackets to the class name.\n\nperson_obj1 = Person()\n\ntype(person_obj1)\n\n__main__.Person\n\n\nAfter instantiating a class, we can now access the object’s properties.\n\n# print attributes\nprint(person_obj1.first_name)\nprint(person_obj1.last_name)\nprint(person_obj1.age)\n\nBetty\nKawala\n30",
    "crumbs": [
      "Python",
      "Advanced"
    ]
  },
  {
    "objectID": "Python/07_advanced.html#class-attributes",
    "href": "Python/07_advanced.html#class-attributes",
    "title": "DATAIDEA",
    "section": "Class Attributes",
    "text": "Class Attributes\nA class can have attributes. Forexample the Person Class can have attributes like the name, height and feet\n\nclass Person:\n    def __init__(self, name, height, feet):\n        self.name = name\n        self.height = height\n        self.feet = feet\n\n\n\nNote!\n\n\nFor now, focus on the syntax. Later we will explain the init() function and the self parameter.\n\n\nNow that our class is ready, we can now instantiate it and provide values to it’s attributes.\nThis process can also be called “creating an instance of a class”.\nAn instance is simply the object created from a class\nIn this example, person_obj1 is a unique instance of the person class.\n\n# create a class instance\nperson_obj1 = Person(\n    name='Betty Kawala', \n    height=1.57, \n    feet=4\n    )\n\n# accessing the properties\nprint('Name:', person_obj1.name)\nprint('Height:', person_obj1.height)\nprint('Feet:', person_obj1.feet)\n\nName: Betty Kawala\nHeight: 1.57\nFeet: 4\n\n\nThe self parameter allows us to access the attributes and methods of a class\nThe __init__() functino allows us to provide values for the attributes of a class\n\nInstances are unique\nLet’s say you have 500 people and you need to manage their data.\nIt is inefficient to create a variable for each of them, instead, you can create unique instances of a class.\nIn this example, the student1 and student2 instances are different from each other\n\nclass Student:\n  def __init__(self, id_number, name, age):\n    self.id_number = id_number\n    self.name = name\n    self.age = age\n\nstudent1 = Student(5243, \"Mary Doe\", 18)\nstudent2 = Student(3221, \"John Doe\", 18)\n\nprint(\"Student 1 ID:\", student1.id_number)\nprint(\"Student 1 Name:\", student1.name)\nprint(\"Student 1 Age:\", student1.age)\n\nprint(\"---------------------\")\n\nprint(\"Student 2 ID:\", student2.id_number)\nprint(\"Student 2 Name:\", student2.name)\nprint(\"Student 2 Age:\", student2.age)\n\nStudent 1 ID: 5243\nStudent 1 Name: Mary Doe\nStudent 1 Age: 18\n---------------------\nStudent 2 ID: 3221\nStudent 2 Name: John Doe\nStudent 2 Age: 18",
    "crumbs": [
      "Python",
      "Advanced"
    ]
  },
  {
    "objectID": "Python/07_advanced.html#methods",
    "href": "Python/07_advanced.html#methods",
    "title": "DATAIDEA",
    "section": "Methods",
    "text": "Methods\nMethods are functions that can access the class attributes. These methods should be defined (created) inside the class\n\nclass Person:\n    def __init__(self, name, height, feet):\n        self.name = name\n        self.height = height\n        self.feet = feet\n        \n    def jump(self):\n        return \"I'm jumping \" + str(self.feet) + \" Feet\"\n\n\nperson_obj1 = Person(name='Juma', height=1.59, feet=5)\n\nprint(person_obj1.jump())\n\nI'm jumping 5 Feet\n\n\nAs you may notice, we used the self parameter to access the feet attribute",
    "crumbs": [
      "Python",
      "Advanced"
    ]
  },
  {
    "objectID": "Python/07_advanced.html#python-inheritance",
    "href": "Python/07_advanced.html#python-inheritance",
    "title": "DATAIDEA",
    "section": "Python Inheritance",
    "text": "Python Inheritance\nInheritance is a feature that allows us to create a class that inherits the attributes or properties and methods of another class\n\nExample\nThe Animal class below can be used to tell that an animal can eat\n\nclass Animal:\n    def __init__(self, name, age):\n        self.name = name\n        self.age = age\n\n    def eat(self):\n        print(f\"{self.name} is eating.\")\n\nLet’s say we need to create another class called Dog.\nSince a dog is also an animal, it’s more efficient to have access to all the properties and methods of the Animal class than to create another\nThis example creates a class named Dog and inherits from the Animal class\n\nclass Dog(Animal):\n    def __init__(self, name, age, color):\n        super().__init__(name, age)\n        self.color = color\n\n    def sound(self):\n        print(self.name, \"barks\")\n\n\n\nNote!\n\n\nAs you may notice, to inherit from a parent, we simply pass the name of that class as a parameter of the child class.\n\n\nNow we can use the properties and methods of both the Animal and the Dog classes using just one instance\n\ndog1 = Dog(name='Brian', age=8, color='White')\ndog1.eat()\ndog1.sound()\n\nBrian is eating.\nBrian barks\n\n\nThe super() and __init__ functions found in the Dog class allow us to inherit the properties and methods of the Animal class.\n\n\nParent and Child Class\nThe parent class is the class from whick the other class inherits from.\nThe child class is the the class that inherits from another class\nIn our example above, the Animal is the parent class while the Dog class is the child class",
    "crumbs": [
      "Python",
      "Advanced"
    ]
  },
  {
    "objectID": "Python/07_advanced.html#formatted-strings",
    "href": "Python/07_advanced.html#formatted-strings",
    "title": "DATAIDEA",
    "section": "Formatted Strings",
    "text": "Formatted Strings\n\n \n\nIn Python, we can format a string by adding substring/s within it.\nThe format() function allows us to format strings.\n\nPlaceholders {}\nPlaceholders help us control which part of the string should be formated.\nThey are defined using curly braces {}.\n\n\nMultiple placeholders\nIf you want to format multiple parts of a string, use multiple placeholders.\nIn this example, we will concatenate (add) a substring to where the curly braces are placed\n\nstatement = '{} loves to code in {}'\n\nformatted = statement.format('Juma', 'JavaScript')\n\nprint(formatted)\n\nJuma loves to code in JavaScript\n\n\n\n\nLiteral String Interpolation\nLiteral string interpolation allows you to use expression inside your strings.\nSimply add f before you opening quote, then surround your expressions with curly braces {}.\n\nname = 'Juma'; \nlanguage = 'JavaScript'\n\nstatement = f'{name} loves to code in {language}'\n\nprint(statement)\n\nJuma loves to code in JavaScript\n\n\nHere’s another example\n\nanswer = f'The summation of 5 and 7 is {5 + 7}'\n\nprint(answer)\n\nThe summation of 5 and 7 is 12\n\n\n\n\nUsing indexes\nWe can use index numbers to specify exactly where the values should be placed.\nThe index numbers should be inside the curly braces: {index_number}\n\nstatement = '{0} loves to code in {1}'\n\nmodified = statement.format('Juma', 'JavaScript')\n\nprint(modified)\n\nJuma loves to code in JavaScript\n\n\n\n\nNote!\n\n\n0 represents the first value, 1 represents the second value and so on.\n\n\n\n\nUsing named indexes\nWe can also use named indexes to specify exactly where the values should be placed\nThe arguments of the format() functino should be in key/value pairs ie key=value\nThe key/value pairs should be separated by commas.\n\nstatement = '{name} loves to code in {language}'\n\nmodified = statement.format(language='JavaScript', name='Juma')\n\nprint(modified)\n\nJuma loves to code in JavaScript",
    "crumbs": [
      "Python",
      "Advanced"
    ]
  },
  {
    "objectID": "Python/07_advanced.html#errors-in-python",
    "href": "Python/07_advanced.html#errors-in-python",
    "title": "DATAIDEA",
    "section": "Errors in Python",
    "text": "Errors in Python\nWhen coding in Python, you will encounter errors.\nWhen errors occur, the program crashes or stops executing.\nFortunately, errors can be handled in Python\n\nThe try...except statment\nThe try...except statement is used to handle exceptions(errors)\nThe try statement takes a block of code to test for errors\nThe except statement handles the exceptions.\n\ntry:\n    # age = input('Enter your age: ')\n    age = '32'\n\n    if age &gt;= 18:\n        print('Your vote has been cast')\n    else:\n        print('You are not eligible to vote')\nexcept:\n    print('A problem occured while picking your age \\n'\n          'You did not enter a number')\n\nA problem occured while picking your age \nYou did not enter a number\n\n\n\n\nThrow Exceptions\nWe can intentionally throw and exception to stop the execution of a program.\nThe raise keyword throws an excrption.\n\n# Creating your own errors\ntry: \n    # age = int(input('Enter your age: '))\n    age = ''\n\n    if age &lt; 18:\n        raise Exception('Not an adult')\nexcept Exception as error:\n    print('A problem occurred \\n'\n          f'Error: {error}')\n\nA problem occurred \nError: '&lt;' not supported between instances of 'str' and 'int'\n\n\n\n\nKinds of Exceptions\nIn Python, there are different kinds of exceptions and we can handle them individually with the try...except statement.\ntry:\n    # statements\nexcept ExceptionKind:\n    #statments\nOne of the most common kind of exceptions is the NameError. This is thrown when you use a variable that is not defined\n\ntry:\n    print(rand_var)\nexcept NameError:\n    print('You used a variable that is not defined!')\n\nYou used a variable that is not defined!",
    "crumbs": [
      "Python",
      "Advanced"
    ]
  },
  {
    "objectID": "Python/07_advanced.html#variable-scope",
    "href": "Python/07_advanced.html#variable-scope",
    "title": "DATAIDEA",
    "section": "Variable Scope",
    "text": "Variable Scope",
    "crumbs": [
      "Python",
      "Advanced"
    ]
  },
  {
    "objectID": "Python/07_advanced.html#python-variable-scopes",
    "href": "Python/07_advanced.html#python-variable-scopes",
    "title": "DATAIDEA",
    "section": "Python Variable Scopes",
    "text": "Python Variable Scopes\nThe accessibility of variable depends on its scope. In Python, there are two variable scopes:\n\nGlobal Scope\nLocal Scope\n\n\nGlobal Scope\nA variable that is defined (created) outside a function has a global scope\nA global variable can be accessed anywhere in a program\n\nname = 'Viola'\n# name can be accessed here\nprint(name)\n\ndef greet():\n    # name can be accessed here\n    print('Hello ' + name)\n\ngreet()\n\nViola\nHello Viola",
    "crumbs": [
      "Python",
      "Advanced"
    ]
  },
  {
    "objectID": "Python/07_advanced.html#local-scope",
    "href": "Python/07_advanced.html#local-scope",
    "title": "DATAIDEA",
    "section": "Local Scope",
    "text": "Local Scope\nA variable that is defined (created) inside a function has a local scope. A local scope variable can only be accessed and used inside the function.\n\ndef greet():\n    local_name = 'Viola'\n    print('Hello ' + local_name)\n\ngreet()\n\ntry:\n    # local_name cannot be accessed here\n    print(local_name)\nexcept Exception as e:\n    print(e)\n\nHello Viola\nname 'local_name' is not defined\n\n\n\nThe global Keyword\nWe can force a local variable to be a global variable by using the global keyword.\n\n# Global Keyword\n\ndef add():\n    global summ\n    number1 = 5\n    number2 = 7\n    summ = number1 + number2\n    return summ\n\nadd()\n\n# summ is accessible even outside the function\nprint(summ)\n\n12\n\n\n\n\nDon’t miss out on any updates and developments! Subscribe to the DATAIDEA Newsletter it’s easy and safe.",
    "crumbs": [
      "Python",
      "Advanced"
    ]
  },
  {
    "objectID": "Python/01_basics.html",
    "href": "Python/01_basics.html",
    "title": "DATAIDEA",
    "section": "",
    "text": "Photo by DATAIDEA",
    "crumbs": [
      "Python",
      "Basics"
    ]
  },
  {
    "objectID": "Python/01_basics.html#basics",
    "href": "Python/01_basics.html#basics",
    "title": "DATAIDEA",
    "section": "Basics",
    "text": "Basics\n\nOverview\nThis course will teach you the basics and advanced concepts of Python Programming\n\n\nPrerequisites\nWhat do you need before learning Python?\n\nComputer Literacy\nKnowledge of installing a software\nA compiler\n\n\n\nPython is Easy\n\nTo learn Python, you don’t need any prior knowledge of experience on programming.\nPython is human readable, making it easier to understand.\nTake alook at this example\n\n\nx = 4\ny = 3\nsumm = x + y\nprint(summ)\n\n7\n\n\nAlthough we have not taught you how to code in Python yet, you can still easily pick up what the code is doing",
    "crumbs": [
      "Python",
      "Basics"
    ]
  },
  {
    "objectID": "Python/01_basics.html#what-is-python",
    "href": "Python/01_basics.html#what-is-python",
    "title": "DATAIDEA",
    "section": "What is Python",
    "text": "What is Python\n\nPython is a programming language.\nPython is one of the most popular programming languages\n\n\nWho created Python?\nPython was created by Guido van Rossum and it was first implemented in 1989\n\n\nWhat is Python used for?\nPython is used for: 1. Web Development 2. Machine Learning 3. Data Science 4. Scripting 5. And many more\n\n\nWhat is the latest version of Python?\n\nPython 3 is the latest version of Python\nThis tutorial is based on the standards of Python 3",
    "crumbs": [
      "Python",
      "Basics"
    ]
  },
  {
    "objectID": "Python/01_basics.html#installing-python",
    "href": "Python/01_basics.html#installing-python",
    "title": "DATAIDEA",
    "section": "Installing Python",
    "text": "Installing Python\nBefore you can run Python on your PC, you need to install it first.\nTo install Python in a PC, go to https://www.python.org/downloads/ then download the latest version.\nAfter that, install it just like how you install other apps.\nMake sure that you check “Add Python to PATH” for easier installation.",
    "crumbs": [
      "Python",
      "Basics"
    ]
  },
  {
    "objectID": "Python/01_basics.html#writing-python-code",
    "href": "Python/01_basics.html#writing-python-code",
    "title": "DATAIDEA",
    "section": "Writing Python Code",
    "text": "Writing Python Code\nIn order to learn Python, you need to be able to write and execute code.\n\nPython Console (Shell)\nPython console also known as shell allows you to execute Python code line by line\nAssuming that you have already installed Python on your PC, you can access the Python console by opening the command prompt and typing python\nLet’s start using the console\nType the following and hit enter\nname = 'Juma Shafara'\nAgain, type the following and hit enter\nprint(name)\nAfter that, you should see this\nJuma Shafara\n\n\nPython Files\nPython files are saved with .py file extension\nYou can use any text editor (even notepad) to create Python files\nJust make sure that you save them with the .py extension, forexample hello.py.\nCopy this code and save it as hello.py:\nprint('Hello World!')\nTo run this Python file on a PC, navigate to the folder where is is located using the command prompt.\nThen type the following and hit enter\npython hello.py\nThe console should then output:\nHello World!\n\n\n\nPython Console",
    "crumbs": [
      "Python",
      "Basics"
    ]
  },
  {
    "objectID": "Python/01_basics.html#python-displaying-output",
    "href": "Python/01_basics.html#python-displaying-output",
    "title": "DATAIDEA",
    "section": "Python Displaying output",
    "text": "Python Displaying output\n\n \n\nTo display an output in Python, use the print() function.\n\nprint('Hello world!')\n\nHello world!\n\n\n\nprint(27)\n\n27\n\n\n\nprint(3 + 27)\n\n30",
    "crumbs": [
      "Python",
      "Basics"
    ]
  },
  {
    "objectID": "Python/01_basics.html#printing-two-objects",
    "href": "Python/01_basics.html#printing-two-objects",
    "title": "DATAIDEA",
    "section": "Printing two objects",
    "text": "Printing two objects\nThe print() function can be used to print two objects. Eg.\n\nprint('Hello', 'Juma')\n\nHello Juma\n\n\n\nx = 3\ny = 7\nsumm = x + y\nprint('the sum is ', summ)\n\nthe sum is  10\n\n\n\nx = 4; y = 3; print(x + y)\n\n7",
    "crumbs": [
      "Python",
      "Basics"
    ]
  },
  {
    "objectID": "Python/01_basics.html#python-statements",
    "href": "Python/01_basics.html#python-statements",
    "title": "DATAIDEA",
    "section": "Python Statements",
    "text": "Python Statements\nA python statement is used to write a value, compute a value, assign a value to a variable, call a functino and many more. Eg.\n\nx = 5\ny = 3\nsumm = x + y\nprint(summ)\n\n8\n\n\nIn the example above, we have 4 lines of code. In python, each line typically contains one statement\n\nMultiple statements in one line\nYou can also write multiple statements in a single of code. Simply separate the statements with semicolons ;\n\na = 4; b = 3; sum = a + b; print(sum)\n\n7",
    "crumbs": [
      "Python",
      "Basics"
    ]
  },
  {
    "objectID": "Python/01_basics.html#python-syntax",
    "href": "Python/01_basics.html#python-syntax",
    "title": "DATAIDEA",
    "section": "Python Syntax",
    "text": "Python Syntax\nWhen coding in Python, we follow a syntax. Syntax is the set of rules followed when writing programs\n\nPython indentation\n\nIn python, indentation indicates a block(group) of statements\nTabs or leading whitespaces are used to compute the indentation level of a line\nIt depends on you whether you want to use tabs or whitespaces, in the example below, we use 2 whitespaces\n\n\nnumber1 = 4\nnumber2 = 3\n\nif number1 &gt; number2:\n  x = 'Hello, world'\n  print(x)\n\nHello, world",
    "crumbs": [
      "Python",
      "Basics"
    ]
  },
  {
    "objectID": "Python/01_basics.html#python-comments",
    "href": "Python/01_basics.html#python-comments",
    "title": "DATAIDEA",
    "section": "Python Comments",
    "text": "Python Comments\n\nComments in Python are used to clarify or explain codes\nComments are not interpreted by Python, meaning comments will not be executed\n\n\n# this is a comment\nx = 4 \ny = 3\n\n# some comment\n# second comment\n# third comment\n\nprint(x + y) # prints out the sum\n\n7",
    "crumbs": [
      "Python",
      "Basics"
    ]
  },
  {
    "objectID": "Python/01_basics.html#end-of-first-module",
    "href": "Python/01_basics.html#end-of-first-module",
    "title": "DATAIDEA",
    "section": "End of first module",
    "text": "End of first module\nThe nice introduction ends here, in the next section, we will look at variables in Python\n\n\nDon’t miss out on any updates and developments! Subscribe to the DATAIDEA Newsletter it’s easy and safe.",
    "crumbs": [
      "Python",
      "Basics"
    ]
  },
  {
    "objectID": "Python/06_flow_control.html",
    "href": "Python/06_flow_control.html",
    "title": "DATAIDEA",
    "section": "",
    "text": "Photo by DATAIDEA",
    "crumbs": [
      "Python",
      "Flow Control"
    ]
  },
  {
    "objectID": "Python/06_flow_control.html#flow-control",
    "href": "Python/06_flow_control.html#flow-control",
    "title": "DATAIDEA",
    "section": "Flow Control",
    "text": "Flow Control",
    "crumbs": [
      "Python",
      "Flow Control"
    ]
  },
  {
    "objectID": "Python/06_flow_control.html#functions",
    "href": "Python/06_flow_control.html#functions",
    "title": "DATAIDEA",
    "section": "Functions",
    "text": "Functions\nA function in python is a group statements that perform a particular task\n\n \n\n\n# This function calculates Body Mass Index\ndef calculateBodyMassIndex(weight_kg, height_m):\n\n    body_mass_index = weight_kg / pow(height_m, 2)\n    rounded_bmi = round(body_mass_index, 2)\n\n    return rounded_bmi\n\n\n# lets try\ncalculateBodyMassIndex(67, 1.6)\n\n26.17\n\n\n\nCreating a function\nTo create a function, we need the following: - The def keyword - A function name - Round brackets () and a colon : - A function body- a group of statements\n\ndef greeter():\n    message = 'Hello'\n    print(message)\n\n\nTo execute a function, it needs to be called\nTo call a function, use its function name with parentheses ()\n\n\ngreeter()\n\nHello\n\n\n\n\nFunction Parameters/Arguments\n\nWhen calling a function, we can pass data using parameters/arguments\nA parameter is a variable declared in the function. In the example below, number1 and number2 are parameter\nThe argument is the value passed to the function when its called. In the example below 3 and 27 are the arguments\n\n\n# define the function\ndef addNumbers(number1, number2):\n    sum = number1 + number2\n    print(sum)\n\n# Call the function\naddNumbers(3, 27)\n\n30\n\n\n\n# setting a default argument\ndef greet(name='you'):\n    message = 'Hello ' + name\n    print(message)\n\ngreet('Tinye')\ngreet()\n\nHello Tinye\nHello you\n\n\n\n\nReturn Statement\nThe return statement is used to return a value to a function caller\n\ndef addNumbers(number1, number2):\n    sum = number1 + number2\n    return sum\n\nsummation = addNumbers(56, 4)\nprint(summation)\n\n60\n\n\n\n\n\n\n\n### lambda functions - Lambda functions are functions that donot have names - The body of a lambda function can only have one expression, but can have multiple arguments - The result of the expression is automatically returned\nSyntax: python   lambda parameters: expression\n\n# Example of lambda function\ncalculateBMI = lambda weight_kg, height_m: round((weight_kg/(height_m ** 2)), 2)\n# Calling a labda function\ncalculateBMI(67, 1.7)\n\n23.18\n\n\n\n\nPractice functions\n\nCalculate CGPA\n\n# Assume 4 course units\n# 1. Math - A\n# 2. Science - B\n# 3. SST - B\n# 4. English - C\n\n\ndef calculate_CGPA(GPs_list, CUs_list):\n    length = len(GPs_list)\n    product_sum = 0\n\n    for item in range(length):\n        product_sum += GPs_list[item] * CUs_list[item]\n\n    CUs_sum = sum(CUs_list)\n\n    CGPA = product_sum / CUs_sum\n\n    return CGPA\n\n# calculate_CGPA(4, 5)\n\n\n\nGet someones age given birth month and year\n\ndef getAge(month, year):\n    month_diff = 12 - month\n    year_diff = 2023 - year\n\n    return str(year_diff) + ' years ' + str(month_diff) + ' months'  \n    \nage = getAge(year=2000, month=10) # keyword argument\nage2 = getAge(10, 2000) # positional argument\n\nprint(age)\n\n23 years 2 months\n\n\n\n\n\n\n\n\n\n\nLoops\n\n \n\n\nLoops are used to repetitively execute a group of statements\nwe have 2 types, for and while loop\n\n\nFor Loop\nA for loop is used to loop through or iterate over a sequence or iterable objects\nSyntax:\nfor variable in sequence:\n    statements\n\npets = ['cat', 'dog', 'rabbit']\n# iterate through pets\nfor pet in pets:\n    print(pet)\n\ncat\ndog\nrabbit\n\n\n\n# convert all weights in list from kg to pounds\nweights_kg = [145, 100, 76, 80]\nweights_pds = []\n\nfor weight in weights_kg:\n    pounds = weight * 2.2\n    rounded_pds = round(pounds, 2)\n    weights_pds.append(rounded_pds)\n\nprint(weights_pds)\n\n[319.0, 220.0, 167.2, 176.0]\n\n\n\n# Display all letters in a name\nname = 'Shafara'\n\nfor letter in name:\n    print(letter)\n\nS\nh\na\nf\na\nr\na\n\n\n\n# print 'Hello you' 5 times\nfor step in range(0, 5):\n    print('Hello you')\n\nHello you\nHello you\nHello you\nHello you\nHello you\n\n\n\n\n\n\n\n\n\n\nWhile loop\n\nThe while loop executes a given group of statements as long as the given expression is True\n\nSyntax:\nwhile expression:\n    statements\n\ncounter = 0\n\nwhile counter &lt; 5:\n    print('Hello you')\n    counter += 1\n\nHello you\nHello you\nHello you\nHello you\nHello you\n\n\n\n# Convert the weights in the list from kgs to pounds\nweights_kg = [145, 100, 76, 80]\nweights_pds = []\n\ncounter = 0\nend = len(weights_kg)\n\nwhile counter &lt; end:\n\n    pound = weights_kg[counter] * 2.2\n    rounded_pds = round(pound, 3)\n    weights_pds.append(rounded_pds)\n\n    counter += 1\n\nprint(weights_pds)\n\n[319.0, 220.0, 167.2, 176.0]",
    "crumbs": [
      "Python",
      "Flow Control"
    ]
  },
  {
    "objectID": "Python/06_flow_control.html#conditional-statements",
    "href": "Python/06_flow_control.html#conditional-statements",
    "title": "DATAIDEA",
    "section": "Conditional Statements",
    "text": "Conditional Statements\n\n \n\nConditional statements in Python are fundamental building blocks for controlling the flow of a program based on certain conditions. They enable the execution of specific blocks of code when certain conditions are met. The primary conditional statements in Python include if, elif, and else.\n\nBasic Syntax\n\nIf Statement\nThe if statement is used to test a condition. If the condition evaluates to True, the block of code inside the if statement is executed.\nif condition:\n    # block of code\nExample:\nx = 10\nif x &gt; 5:\n    print(\"x is greater than 5\")\n\n\nElse Statement\nThe else statement is used to execute a block of code if the condition in the if statement evaluates to False.\nif condition:\n    # block of code if condition is True\nelse:\n    # block of code if condition is False\nExample:\nx = 3\nif x &gt; 5:\n    print(\"x is greater than 5\")\nelse:\n    print(\"x is not greater than 5\")\n\n\nElif Statement\nThe elif (short for else if) statement allows you to check multiple conditions. If the first condition is False, it checks the next elif condition, and so on. If all conditions are False, the else block is executed.\nif condition1:\n    # block of code if condition1 is True\nelif condition2:\n    # block of code if condition2 is True\nelse:\n    # block of code if none of the above conditions are True\nExample:\nx = 7\nif x &gt; 10:\n    print(\"x is greater than 10\")\nelif x &gt; 5:\n    print(\"x is greater than 5 but less than or equal to 10\")\nelse:\n    print(\"x is 5 or less\")\n\n\n\n\n\n\n\n\nNested Conditional Statements\nConditional statements can be nested within each other to handle more complex decision-making processes.\nExample:\nx = 15\nif x &gt; 10:\n    if x &gt; 20:\n        print(\"x is greater than 20\")\n    else:\n        print(\"x is greater than 10 but not greater than 20\")\nelse:\n    print(\"x is 10 or less\")\n\n\nConditional Expressions (Ternary Operator)\nPython also supports conditional expressions, which allow for a more concise way to write simple if-else statements.\nvariable = value_if_true if condition else value_if_false\nExample:\nx = 10\nresult = \"greater than 5\" if x &gt; 5 else \"5 or less\"\nprint(result)  # Output: greater than 5\n\n\nCombining Conditions\nMultiple conditions can be combined using logical operators (and, or, not).\nExample:\nx = 8\nif x &gt; 5 and x &lt; 10:\n    print(\"x is between 5 and 10\")\n\n\nPractical Usage\nConditional statements are used in a wide variety of scenarios, such as:\n\nValidating user input.\nControlling the flow of loops.\nImplementing different behaviors in functions or methods.\nHandling exceptions or special cases in data processing.\n\nUnderstanding and effectively using conditional statements are crucial for writing efficient and readable code in Python. They enable developers to build programs that can make decisions and respond dynamically to different inputs and situations.\n\n\n\n\n\n\n\nDon’t miss out on any updates and developments! Subscribe to the DATAIDEA Newsletter it’s easy and safe.",
    "crumbs": [
      "Python",
      "Flow Control"
    ]
  },
  {
    "objectID": "Python/12_true_or_false_quiz.html",
    "href": "Python/12_true_or_false_quiz.html",
    "title": "True or False",
    "section": "",
    "text": "For this Python Quiz Respond with Either True or False only. Answers are provided at the bottom.\n# your answer\n# your answer\n# your answer\n# your answer\n# your answer\n# your answer\n# your answer\n# your answer\n# your answer\n# your answer\n# your answer\n# your answer\n# your answer\n# your answer\n# your answer\n# your answer\n# your answer\n# your answer\n# your answer\n# your answer",
    "crumbs": [
      "Python",
      "True or False"
    ]
  },
  {
    "objectID": "Python/12_true_or_false_quiz.html#answers",
    "href": "Python/12_true_or_false_quiz.html#answers",
    "title": "True or False",
    "section": "Answers",
    "text": "Answers\n\nTrue\nTrue\nTrue\nTrue\nFalse (The map function returns a map object, which is an iterator, not a list)\nTrue\nTrue\nTrue\nFalse (Python does not support method overloading like other languages; you can define methods with default parameters instead)\nFalse (The is operator checks for object identity, not equality)\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n\nHow many did you get? If you need more details or explanations for any specific questions, let me know!\n\nTo be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel.",
    "crumbs": [
      "Python",
      "True or False"
    ]
  },
  {
    "objectID": "Python/miscellaneous_exerise.html",
    "href": "Python/miscellaneous_exerise.html",
    "title": "Miscellaneous Exercise",
    "section": "",
    "text": "Reverse a String: Write a function that takes a string as input and returns the reverse of that string.\n\n\n#Qn.1 reverse a string\ndef reverseString (text):\n    return text[::-1]\nreverseString ('hello world')\n\n'dlrow olleh'\n\n\n\nPalindrome Checker: Write a function that takes a string as input and returns True if it’s a palindrome (reads the same forwards and backwards), False otherwise.\n\n\n#Qn.2 Palindrome Checker\ndef isPalindrome (text):\n    return text == text[::-1]\nisPalindrome('madam')\n\nTrue\n\n\n\nFactorial Calculator: Write a function that calculates the factorial of a given number. The factorial of a non-negative integer is the product of all positive integers less than or equal to the integer.\n\n\n#Qn.3 Factorial Calculator\ndef numberFactorial(number):\n    if number==0 or number==1:\n        print(1)\n    else:\n        factorial = 1\n        for value in range (1, number+1):\n            factorial *= value\n        print(factorial)\nnumberFactorial(6)\n\n720\n\n\n\n\n\n\n\n\nFibonacci Sequence: Write a function to generate the Fibonacci sequence up to a certain number of terms. The Fibonacci sequence is a series of numbers where each number is the sum of the two preceding ones, usually starting with 0 and 1.\n\n\n#Qn.4 Fibonacci Sequence\ndef fibonacciSequence (number):\n    sequence = [0,1]\n    while len (sequence)&lt; number:\n        sequence.append(sequence[-1] + sequence[-2])\n    return sequence\nfibonacciSequence (8)\n\n[0, 1, 1, 2, 3, 5, 8, 13]\n\n\n\nPrime Number Checker: Write a function that checks if a given number is prime.\n\n\n#Qn.5 Prime Number Checker\ndef isPrimeNumber (number):\n    if number &lt;= 1:\n        return False\n    for value in range(2,int(number**0.5)+1):\n        if number % value == 0:\n            return False\n    return True\nisPrimeNumber(4)\n\nFalse\n\n\n\nList Reversal: Write a function that reverses a list.\n\n\n#Qn.6 List Reversal\ndef reverseList(list_):\n    return list_[::-1]\nreverseList([1,3,5,6,8])\n\n[8, 6, 5, 3, 1]\n\n\n\n\n\n\n\n\nList Sorting: Write a function that sorts a list of integers in ascending order without using Python’s built-in sorting functions.\n\n\n#Qn.7 List Sorting\ndef sortList(list_):\n    return sorted(list_)\nsortList([3,7,1,9,2])\n\n[1, 2, 3, 7, 9]\n\n\n\nAnagram Checker: Write a function that takes two strings as input and returns True if they are anagrams (contain the same letters with the same frequency), False otherwise.\n\n\n#Qn.8 Anagram Checker\ndef areAnagram(string1, string2):\n    return sorted(string1) == sorted(string2)\nareAnagram('listen','silent')\n\nTrue\n\n\n\nCount Words in a String: Write a function that takes a string as input and returns the count of words in that string.\n\n\n#Qn.9 Count words in a String\ndef countWords(string):\n    return len(string.split())\ncountWords('hello world')\n\n2\n\n\n\nUnique Elements: Write a function that takes a list and returns a new list with only the unique elements from the original list.\n\n\n#Qn.10 Unique Elements\ndef uniqueElements(list_):\n    return list(set(list_))\nuniqueElements([2,4,3,2,6,4,7,9,5,3,5,1,6])\n\n[1, 2, 3, 4, 5, 6, 7, 9]\n\n\n\nTo be among the first to hear about future updates, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Python",
      "Miscellaneous Exercise"
    ]
  },
  {
    "objectID": "Python/02_variables.html",
    "href": "Python/02_variables.html",
    "title": "Python Variables",
    "section": "",
    "text": "Photo by DATAIDEA",
    "crumbs": [
      "Python",
      "Python Variables"
    ]
  },
  {
    "objectID": "Python/02_variables.html#variables",
    "href": "Python/02_variables.html#variables",
    "title": "Python Variables",
    "section": "Variables",
    "text": "Variables\n\nVaribles are used to store values.\nTo create a variable, use the equal sign (=).\n\nIn the examples below, we create varibales name fruit and name and we assign them values 'mango' and 'viola' respectively\n\nfruit = 'mango'\nname = 'voila'\n\n# printing out\nprint(name, ' likes ', fruit )\n\nvoila  likes  mango\n\n\n\nRules to consider\n\nSpaces are not allowed eg\nCan not start with number eg\nCan not have special characters eg\nAre case sensitive\n\nExamples of good variable names\n\nmy_favorite_character = 'Stewie Griffin' # snake case\nmyFavoriteCharacter = 'Meg Griffin' # camel case\nMyFavoriteCharacter = 'Brian Griffin' # Pascal case",
    "crumbs": [
      "Python",
      "Python Variables"
    ]
  },
  {
    "objectID": "Python/02_variables.html#data-types",
    "href": "Python/02_variables.html#data-types",
    "title": "Python Variables",
    "section": "Data Types",
    "text": "Data Types\nIn this section of the tutorial, you will learn the most basic data types in Python\n\nNumbers\nThese are two basic types of numbers and they are called: - integer(numbers without decimal places) - floating point numbers(numbers with decimal places)\n\n# python numbers\n# Integers\nage = 45\npopulation = 45000000\n\n\n# Floating point numbers\nheight = 1.7\nweight = 147.45\n\nMore on Numbers",
    "crumbs": [
      "Python",
      "Python Variables"
    ]
  },
  {
    "objectID": "Python/02_variables.html#strings",
    "href": "Python/02_variables.html#strings",
    "title": "Python Variables",
    "section": "Strings",
    "text": "Strings\nStrings are simply text. A string must be surrounded by single or double quotes\n\n# Strings\nname = 'Juma'\nother_name = \"Masaba Calvin\"\nstatement = 'I love coding'\n\n\nSingle or double quotes?\nUse single quotes when your string contains double quotes, or the vice versa.\n\n# when to use which quotes\nreport = 'He said, \"I will not go home\"'\n\n\n\nString Methods\nIn this lesson, we will study some methods(functions) that can be used to work with strings.\n\nCapitalize String\nThe capitalize() function returns the string where the first letter is in upper case.\nNote that it doesn’t modify the original string.\n\ntext = 'shafara VEe'\ncapitalized_text = text.capitalize()\nprint(capitalized_text)\n\nShafara vee\n\n\n\n\nConvert to Upper Case\nThe upper() function returns a copy of the given string but all the letters are in upper case.\nNote that this does not modify the original string.\n\ntext = 'shafara VEe'\nupper_text = text.upper()\nprint(upper_text)\n\nSHAFARA VEE\n\n\n\n\nConvert to Lower Case\nThe lower() function returns a copy of the given string but all the letter are in lower case.\nNote that it does not modify the original text/string\n\ntext = 'shafara VEe'\nlower_text = text.lower()\nprint(lower_text)\n\nLower case text: shafara vee\n\n\n\n\nGet the lenght of a String\nThe length of a string is the number of characters it contains\nThe len() function returns the length of a string. It takes on paramter, the string.\n\ntext = 'shafara VEe'\ntext_length = len(text)\nprint(text_length)\n\n11\n\n\n\n\nReplace Parts of a String.\nThe replace() method/function replaces the occurrences of a specified substring with another substring.\nIt doesn’t modify the original string.\n\ntext = 'shafara VEe'\ncorrected_text = text.replace('VEe', 'Viola')\nprint(corrected_text)\n\nshafara Viola\n\n\n\n\nCheck if a Value is Present in a String\nTo check if a substring is present in a string, use the in keyword.\nIt returns True if the substring is found, otherwise False.\n\ntext = 'shafara VEe'\nprint('shafara' not in text)\n\nFalse\n\n\n\n\n\n\n\n\n\n\nBooleans\nBoolean data type can only have on fo these values: True or False\n\n\nNote!\n\n\nThe first letter of a Boolean is in upper case.\n\n\nBooleans are often the result of evaluated expressions.\nForexample, when you compare two numbers, Python evaluates the expression and returns either True or False\n\nprint(5 == 5)\nprint(10 &gt; 5)\nprint(20 &lt; 10)\n\nTrue\nTrue\nFalse\n\n\nThese are often used in if statments.\nIn this example, if the value of the variable age is more than 18, the program will tell the user that they are allowed to enter\n\nage = 19\n\nif age &gt; 18:\n    print('You are allowed to enter.')\n\nYou are allowed to enter.\n\n\n\n\nNote!\n\n\nYou will learn more about if statements later in the course\n\n\n\nChecking the Type of a Boolean\nWe can check the data type of a Boolean variable using the type() method.\n\nmarried = True\nprint(type(married))\n\n&lt;class 'bool'&gt;\n\n\n\n\n\nLists\n\nA list is an ordered collection of data\nIt can contain strings, numbers or even other lists\nLists are written with square brackets ([])\nThe values in lists (also called elements) are separated by commas (,)\n\n\n# Lists\nnames = ['juma', 'john', 'calvin']\nprint(names)\n\n['juma', 'john', 'calvin']\n\n\nA list can contain mixed data types\n\nother_stuff = ['mango', True, 38]\nprint(other_stuff)\n\n['mango', True, 38]\n\n\n\n\nChecking data types\nTo check the data type of an object in python, use type(object), for example, below we get the data type of the object stored in names\n\n# Which data type\ntype(names)\n\nlist",
    "crumbs": [
      "Python",
      "Python Variables"
    ]
  },
  {
    "objectID": "Python/02_variables.html#converting-types",
    "href": "Python/02_variables.html#converting-types",
    "title": "Python Variables",
    "section": "Converting Types",
    "text": "Converting Types\n\nConvert to String\nThe str() method returns the string version of a given object\n\n# convert an integer to a string\nage = 45\nmessage = 'Peter Griffin is '+ str(age) + ' years old'\nprint(message)\n\nPeter Griffin is 45 years old\n\n\n\n\nConvert to Integer\nThe int() method returns the integer version of the given object.\n\n# Convert floating point to integer\npi = 3.14159\nprint(int(pi))\n\n\n\nConvert to Float\nThe float() method returns the floating point version of the given object.\n\nside = 4\nprint(float(side))\n\n4.0\n\n\n\n\nDon’t miss out on any updates and developments! Subscribe to the DATAIDEA Newsletter it’s easy and safe.",
    "crumbs": [
      "Python",
      "Python Variables"
    ]
  },
  {
    "objectID": "Pytorch/Text/07_torchtext_custom_dataset_tutorial.html",
    "href": "Pytorch/Text/07_torchtext_custom_dataset_tutorial.html",
    "title": "Preprocess custom text dataset using Torchtext",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\n\n\n# For tips on running notebooks in Google Colab, see\n# https://pytorch.org/tutorials/beginner/colab\n\n\nPreprocess custom text dataset using Torchtext\nAuthor: Anupam Sharma\nThis tutorial illustrates the usage of torchtext on a dataset that is not built-in. In the tutorial, we will preprocess a dataset that can be further utilized to train a sequence-to-sequence model for machine translation (something like, in this tutorial: Sequence to Sequence Learning with Neural Networks) but without using legacy version of torchtext.\nIn this tutorial, we will learn how to:\n\nRead a dataset\nTokenize sentence\nApply transforms to sentence\nPerform bucket batching\n\nLet us assume that we need to prepare a dataset to train a model that can perform English to German translation. We will use a tab-delimited German - English sentence pairs provided by the Tatoeba Project which can be downloaded from this link.\nSentence pairs for other languages can be found in this link.\n\n\nSetup\nFirst, download the dataset, extract the zip, and note the path to the file deu.txt.\nEnsure that following packages are installed:\n\nTorchdata 0.6.0 (Installation instructions)\nTorchtext 0.15.0 (Installation instructions)\nSpacy\n\nHere, we are using Spacy to tokenize text. In simple words tokenization means to convert a sentence to list of words. Spacy is a python package used for various Natural Language Processing (NLP) tasks.\nDownload the English and German models from Spacy as shown below:\npython -m spacy download en_core_web_sm\npython -m spacy download de_core_news_sm\nLet us start by importing required modules:\n\nimport torchdata.datapipes as dp\nimport torchtext.transforms as T\nimport spacy\nfrom torchtext.vocab import build_vocab_from_iterator\neng = spacy.load(\"en_core_web_sm\") # Load the English model to tokenize English text\nde = spacy.load(\"de_core_news_sm\") # Load the German model to tokenize German text\n\nNow we will load the dataset\n\nFILE_PATH = 'data/deu.txt'\ndata_pipe = dp.iter.IterableWrapper([FILE_PATH])\ndata_pipe = dp.iter.FileOpener(data_pipe, mode='rb')\ndata_pipe = data_pipe.parse_csv(skip_lines=0, delimiter='\\t', as_tuple=True)\n\nIn the above code block, we are doing following things:\n\nAt line 2, we are creating an iterable of filenames\nAt line 3, we pass the iterable to FileOpener which then opens the file in read mode\nAt line 4, we call a function to parse the file, which again returns an iterable of tuples representing each rows of the tab-delimited file\n\nDataPipes can be thought of something like a dataset object, on which we can perform various operations. Check this tutorial for more details on DataPipes.\nWe can verify if the iterable has the pair of sentences as shown below:\n\nfor sample in data_pipe:\n    print(sample)\n    break\n\nNote that we also have attribution details along with pair of sentences. We will write a small function to remove the attribution details:\n\ndef removeAttribution(row):\n    \"\"\"\n    Function to keep the first two elements in a tuple\n    \"\"\"\n    return row[:2]\ndata_pipe = data_pipe.map(removeAttribution)\n\nThe map function at line 6 in above code block can be used to apply some function on each elements of data_pipe. Now, we can verify that the data_pipe only contains pair of sentences.\n\nfor sample in data_pipe:\n    print(sample)\n    break\n\nNow, let us define few functions to perform tokenization:\n\ndef engTokenize(text):\n    \"\"\"\n    Tokenize an English text and return a list of tokens\n    \"\"\"\n    return [token.text for token in eng.tokenizer(text)]\n\ndef deTokenize(text):\n    \"\"\"\n    Tokenize a German text and return a list of tokens\n    \"\"\"\n    return [token.text for token in de.tokenizer(text)]\n\nAbove function accepts a text and returns a list of words as shown below:\n\nprint(engTokenize(\"Have a good day!!!\"))\nprint(deTokenize(\"Haben Sie einen guten Tag!!!\"))\n\n\n\nBuilding the vocabulary\nLet us consider an English sentence as the source and a German sentence as the target.\nVocabulary can be considered as the set of unique words we have in the dataset. We will build vocabulary for both our source and target now.\nLet us define a function to get tokens from elements of tuples in the iterator.\n\ndef getTokens(data_iter, place):\n    \"\"\"\n    Function to yield tokens from an iterator. Since, our iterator contains\n    tuple of sentences (source and target), `place` parameters defines for which\n    index to return the tokens for. `place=0` for source and `place=1` for target\n    \"\"\"\n    for english, german in data_iter:\n        if place == 0:\n            yield engTokenize(english)\n        else:\n            yield deTokenize(german)\n\nNow, we will build vocabulary for source:\n\nsource_vocab = build_vocab_from_iterator(\n    getTokens(data_pipe,0),\n    min_freq=2,\n    specials= ['&lt;pad&gt;', '&lt;sos&gt;', '&lt;eos&gt;', '&lt;unk&gt;'],\n    special_first=True\n)\nsource_vocab.set_default_index(source_vocab['&lt;unk&gt;'])\n\nThe code above, builds the vocabulary from the iterator. In the above code block:\n\nAt line 2, we call the getTokens() function with place=0 as we need vocabulary for source sentences.\nAt line 3, we set min_freq=2. This means, the function will skip those words that occurs less than 2 times.\nAt line 4, we specify some special tokens:\n\n&lt;sos&gt; for start of sentence\n&lt;eos&gt; for end of sentence\n&lt;unk&gt; for unknown words. An example of unknown word is the one skipped because of min_freq=2.\n&lt;pad&gt; is the padding token. While training, a model we mostly train in batches. In a batch, there can be sentences of different length. So, we pad the shorter sentences with &lt;pad&gt; token to make length of all sequences in the batch equal.\n\nAt line 5, we set special_first=True. Which means &lt;pad&gt; will get index 0, &lt;sos&gt; index 1, &lt;eos&gt; index 2, and &lt;unk&gt; will get index 3 in the vocabulary.\nAt line 7, we set default index as index of &lt;unk&gt;. That means if some word is not in vocabulary, we will use &lt;unk&gt; instead of that unknown word.\n\nSimilarly, we will build vocabulary for target sentences:\n\ntarget_vocab = build_vocab_from_iterator(\n    getTokens(data_pipe,1),\n    min_freq=2,\n    specials= ['&lt;pad&gt;', '&lt;sos&gt;', '&lt;eos&gt;', '&lt;unk&gt;'],\n    special_first=True\n)\ntarget_vocab.set_default_index(target_vocab['&lt;unk&gt;'])\n\nNote that the example above shows how can we add special tokens to our vocabulary. The special tokens may change based on the requirements.\nNow, we can verify that special tokens are placed at the beginning and then other words. In the below code, source_vocab.get_itos() returns a list with tokens at index based on vocabulary.\n\nprint(source_vocab.get_itos()[:9])\n\n\n\nNumericalize sentences using vocabulary\nAfter building the vocabulary, we need to convert our sentences to corresponding indices. Let us define some functions for this:\n\ndef getTransform(vocab):\n    \"\"\"\n    Create transforms based on given vocabulary. The returned transform is applied to sequence\n    of tokens.\n    \"\"\"\n    text_tranform = T.Sequential(\n        ## converts the sentences to indices based on given vocabulary\n        T.VocabTransform(vocab=vocab),\n        ## Add &lt;sos&gt; at beginning of each sentence. 1 because the index for &lt;sos&gt; in vocabulary is\n        # 1 as seen in previous section\n        T.AddToken(1, begin=True),\n        ## Add &lt;eos&gt; at beginning of each sentence. 2 because the index for &lt;eos&gt; in vocabulary is\n        # 2 as seen in previous section\n        T.AddToken(2, begin=False)\n    )\n    return text_tranform\n\nNow, let us see how to use the above function. The function returns an object of Transforms which we will use on our sentence. Let us take a random sentence and check how the transform works.\n\ntemp_list = list(data_pipe)\nsome_sentence = temp_list[798][0]\nprint(\"Some sentence=\", end=\"\")\nprint(some_sentence)\ntransformed_sentence = getTransform(source_vocab)(engTokenize(some_sentence))\nprint(\"Transformed sentence=\", end=\"\")\nprint(transformed_sentence)\nindex_to_string = source_vocab.get_itos()\nfor index in transformed_sentence:\n    print(index_to_string[index], end=\" \")\n\nIn the above code,:\n\nAt line 2, we take a source sentence from list that we created from data_pipe at line 1\nAt line 5, we get a transform based on a source vocabulary and apply it to a tokenized sentence. Note that transforms take list of words and not a sentence.\nAt line 8, we get the mapping of index to string and then use it get the transformed sentence\n\nNow we will use DataPipe functions to apply transform to all our sentences. Let us define some more functions for this.\n\ndef applyTransform(sequence_pair):\n    \"\"\"\n    Apply transforms to sequence of tokens in a sequence pair\n    \"\"\"\n\n    return (\n        getTransform(source_vocab)(engTokenize(sequence_pair[0])),\n        getTransform(target_vocab)(deTokenize(sequence_pair[1]))\n    )\ndata_pipe = data_pipe.map(applyTransform) ## Apply the function to each element in the iterator\ntemp_list = list(data_pipe)\nprint(temp_list[0])\n\n\n\nMake batches (with bucket batch)\nGenerally, we train models in batches. While working for sequence to sequence models, it is recommended to keep the length of sequences in a batch similar. For that we will use bucketbatch function of data_pipe.\nLet us define some functions that will be used by the bucketbatch function.\n\ndef sortBucket(bucket):\n    \"\"\"\n    Function to sort a given bucket. Here, we want to sort based on the length of\n    source and target sequence.\n    \"\"\"\n    return sorted(bucket, key=lambda x: (len(x[0]), len(x[1])))\n\nNow, we will apply the bucketbatch function:\n\ndata_pipe = data_pipe.bucketbatch(\n    batch_size = 4, batch_num=5,  bucket_num=1,\n    use_in_batch_shuffle=False, sort_key=sortBucket\n)\n\nIn the above code block:\n\n\nWe keep batch size = 4.\nbatch_num is the number of batches to keep in a bucket\nbucket_num is the number of buckets to keep in a pool for shuffling\nsort_key specifies the function that takes a bucket and sorts it\n\n\nNow, let us consider a batch of source sentences as X and a batch of target sentences as y. Generally, while training a model, we predict on a batch of X and compare the result with y. But, a batch in our data_pipe is of the form `[(X_1,y_1), (X_2,y_2), (X_3,y_3), (X_4,y_4)]`:\n\nprint(list(data_pipe)[0])\n\nSo, we will now convert them into the form: ((X_1,X_2,X_3,X_4), (y_1,y_2,y_3,y_4)). For this we will write a small function:\n\ndef separateSourceTarget(sequence_pairs):\n    \"\"\"\n    input of form: `[(X_1,y_1), (X_2,y_2), (X_3,y_3), (X_4,y_4)]`\n    output of form: `((X_1,X_2,X_3,X_4), (y_1,y_2,y_3,y_4))`\n    \"\"\"\n    sources,targets = zip(*sequence_pairs)\n    return sources,targets\n\n## Apply the function to each element in the iterator\ndata_pipe = data_pipe.map(separateSourceTarget)\nprint(list(data_pipe)[0])\n\nNow, we have the data as desired.\n\n\nPadding\nAs discussed earlier while building vocabulary, we need to pad shorter sentences in a batch to make all the sequences in a batch of equal length. We can perform padding as follows:\n\ndef applyPadding(pair_of_sequences):\n    \"\"\"\n    Convert sequences to tensors and apply padding\n    \"\"\"\n    return (T.ToTensor(0)(list(pair_of_sequences[0])), T.ToTensor(0)(list(pair_of_sequences[1])))\n## `T.ToTensor(0)` returns a transform that converts the sequence to `torch.tensor` and also applies\n# padding. Here, `0` is passed to the constructor to specify the index of the `&lt;pad&gt;` token in the\n# vocabulary.\ndata_pipe = data_pipe.map(applyPadding)\n\nNow, we can use the index to string mapping to see how the sequence would look with tokens instead of indices:\n\nsource_index_to_string = source_vocab.get_itos()\ntarget_index_to_string = target_vocab.get_itos()\n\ndef showSomeTransformedSentences(data_pipe):\n    \"\"\"\n    Function to show how the sentences look like after applying all transforms.\n    Here we try to print actual words instead of corresponding index\n    \"\"\"\n    for sources,targets in data_pipe:\n        if sources[0][-1] != 0:\n            continue # Just to visualize padding of shorter sentences\n        for i in range(4):\n            source = \"\"\n            for token in sources[i]:\n                source += \" \" + source_index_to_string[token]\n            target = \"\"\n            for token in targets[i]:\n                target += \" \" + target_index_to_string[token]\n            print(f\"Source: {source}\")\n            print(f\"Traget: {target}\")\n        break\n\nshowSomeTransformedSentences(data_pipe)\n\nIn the above output we can observe that the shorter sentences are padded with &lt;pad&gt;. Now, we can use data_pipe while writing our training function.\nSome parts of this tutorial was inspired from this article.\n\n\n\n\n Back to top",
    "crumbs": [
      "Pytorch",
      "Text",
      "Preprocess custom text dataset using Torchtext"
    ]
  },
  {
    "objectID": "Pytorch/Text/03_char_rnn_generation_tutorial.html",
    "href": "Pytorch/Text/03_char_rnn_generation_tutorial.html",
    "title": "NLP From Scratch: Generating Names with a Character-Level RNN",
    "section": "",
    "text": "Photo by DATAIDEA\n# For tips on running notebooks in Google Colab, see\n# https://pytorch.org/tutorials/beginner/colab",
    "crumbs": [
      "Pytorch",
      "Text",
      "NLP From Scratch: Generating Names with a Character-Level RNN"
    ]
  },
  {
    "objectID": "Pytorch/Text/03_char_rnn_generation_tutorial.html#preparing-the-data",
    "href": "Pytorch/Text/03_char_rnn_generation_tutorial.html#preparing-the-data",
    "title": "NLP From Scratch: Generating Names with a Character-Level RNN",
    "section": "Preparing the Data",
    "text": "Preparing the Data\n\nNOTE:\n\n\n\nDownload the data fromhereand extract it to the current directory.\n\n\nSee the last tutorial for more detail of this process. In short, there are a bunch of plain text files data/names/[Language].txt with a name per line. We split lines into an array, convert Unicode to ASCII, and end up with a dictionary {language: [names ...]}.\n\nfrom io import open\nimport glob\nimport os\nimport unicodedata\nimport string\n\nall_letters = string.ascii_letters + \" .,;'-\"\nn_letters = len(all_letters) + 1 # Plus EOS marker\n\ndef findFiles(path): return glob.glob(path)\n\n# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\ndef unicodeToAscii(s):\n    return ''.join(\n        c for c in unicodedata.normalize('NFD', s)\n        if unicodedata.category(c) != 'Mn'\n        and c in all_letters\n    )\n\n# Read a file and split into lines\ndef readLines(filename):\n    with open(filename, encoding='utf-8') as some_file:\n        return [unicodeToAscii(line.strip()) for line in some_file]\n\n# Build the category_lines dictionary, a list of lines per category\ncategory_lines = {}\nall_categories = []\nfor filename in findFiles('data/names/*.txt'):\n    category = os.path.splitext(os.path.basename(filename))[0]\n    all_categories.append(category)\n    lines = readLines(filename)\n    category_lines[category] = lines\n\nn_categories = len(all_categories)\n\nif n_categories == 0:\n    raise RuntimeError('Data not found. Make sure that you downloaded data '\n        'from https://download.pytorch.org/tutorial/data.zip and extract it to '\n        'the current directory.')\n\nprint('# categories:', n_categories, all_categories)\nprint(unicodeToAscii(\"O'Néàl\"))",
    "crumbs": [
      "Pytorch",
      "Text",
      "NLP From Scratch: Generating Names with a Character-Level RNN"
    ]
  },
  {
    "objectID": "Pytorch/Text/03_char_rnn_generation_tutorial.html#preparing-for-training",
    "href": "Pytorch/Text/03_char_rnn_generation_tutorial.html#preparing-for-training",
    "title": "NLP From Scratch: Generating Names with a Character-Level RNN",
    "section": "Preparing for Training",
    "text": "Preparing for Training\nFirst of all, helper functions to get random pairs of (category, line):\n\nimport random\n\n# Random item from a list\ndef randomChoice(l):\n    return l[random.randint(0, len(l) - 1)]\n\n# Get a random category and random line from that category\ndef randomTrainingPair():\n    category = randomChoice(all_categories)\n    line = randomChoice(category_lines[category])\n    return category, line\n\nFor each timestep (that is, for each letter in a training word) the inputs of the network will be (category, current letter, hidden state) and the outputs will be (next letter, next hidden state). So for each training set, we'll need the category, a set of input letters, and a set of output/target letters.\nSince we are predicting the next letter from the current letter for each timestep, the letter pairs are groups of consecutive letters from the line - e.g. for \"ABCD&lt;EOS&gt;\" we would create (\"A\", \"B\"), (\"B\", \"C\"), (\"C\", \"D\"), (\"D\", \"EOS\").\n\nThe category tensor is a one-hot tensor of size &lt;1 x n_categories&gt;. When training we feed it to the network at every timestep - this is a design choice, it could have been included as part of initial hidden state or some other strategy.\n\n# One-hot vector for category\ndef categoryTensor(category):\n    li = all_categories.index(category)\n    tensor = torch.zeros(1, n_categories)\n    tensor[0][li] = 1\n    return tensor\n\n# One-hot matrix of first to last letters (not including EOS) for input\ndef inputTensor(line):\n    tensor = torch.zeros(len(line), 1, n_letters)\n    for li in range(len(line)):\n        letter = line[li]\n        tensor[li][0][all_letters.find(letter)] = 1\n    return tensor\n\n# ``LongTensor`` of second letter to end (EOS) for target\ndef targetTensor(line):\n    letter_indexes = [all_letters.find(line[li]) for li in range(1, len(line))]\n    letter_indexes.append(n_letters - 1) # EOS\n    return torch.LongTensor(letter_indexes)\n\nFor convenience during training we'll make a randomTrainingExample function that fetches a random (category, line) pair and turns them into the required (category, input, target) tensors.\n\n# Make category, input, and target tensors from a random category, line pair\ndef randomTrainingExample():\n    category, line = randomTrainingPair()\n    category_tensor = categoryTensor(category)\n    input_line_tensor = inputTensor(line)\n    target_line_tensor = targetTensor(line)\n    return category_tensor, input_line_tensor, target_line_tensor",
    "crumbs": [
      "Pytorch",
      "Text",
      "NLP From Scratch: Generating Names with a Character-Level RNN"
    ]
  },
  {
    "objectID": "Pytorch/Text/02_char_rnn_classification_tutorial.html",
    "href": "Pytorch/Text/02_char_rnn_classification_tutorial.html",
    "title": "NLP From Scratch: Classifying Names with a Character-Level RNN",
    "section": "",
    "text": "Photo by DATAIDEA\n# For tips on running notebooks in Google Colab, see\n# https://pytorch.org/tutorials/beginner/colab",
    "crumbs": [
      "Pytorch",
      "Text",
      "NLP From Scratch: Classifying Names with a Character-Level RNN"
    ]
  },
  {
    "objectID": "Pytorch/Text/02_char_rnn_classification_tutorial.html#recommended-preparation",
    "href": "Pytorch/Text/02_char_rnn_classification_tutorial.html#recommended-preparation",
    "title": "NLP From Scratch: Classifying Names with a Character-Level RNN",
    "section": "Recommended Preparation",
    "text": "Recommended Preparation\nBefore starting this tutorial it is recommended that you have installed PyTorch, and have a basic understanding of Python programming language and Tensors:\n\nhttps://pytorch.org/ For installation instructions\n/beginner/deep_learning_60min_blitz to get started with PyTorch in general and learn the basics of Tensors\n/beginner/pytorch_with_examples for a wide and deep overview\n/beginner/former_torchies_tutorial if you are former Lua Torch user\n\nIt would also be useful to know about RNNs and how they work:\n\nThe Unreasonable Effectiveness of Recurrent Neural Networks shows a bunch of real life examples\nUnderstanding LSTM Networks is about LSTMs specifically but also informative about RNNs in general",
    "crumbs": [
      "Pytorch",
      "Text",
      "NLP From Scratch: Classifying Names with a Character-Level RNN"
    ]
  },
  {
    "objectID": "Pytorch/Text/02_char_rnn_classification_tutorial.html#preparing-the-data",
    "href": "Pytorch/Text/02_char_rnn_classification_tutorial.html#preparing-the-data",
    "title": "NLP From Scratch: Classifying Names with a Character-Level RNN",
    "section": "Preparing the Data",
    "text": "Preparing the Data\n\nNOTE:\n\n\n\nDownload the data fromhereand extract it to the current directory.\n\n\nIncluded in the data/names directory are 18 text files named as [Language].txt. Each file contains a bunch of names, one name per line, mostly romanized (but we still need to convert from Unicode to ASCII).\nWe'll end up with a dictionary of lists of names per language, {language: [names ...]}. The generic variables \"category\" and \"line\" (for language and name in our case) are used for later extensibility.\n\nfrom io import open\nimport glob\nimport os\n\ndef findFiles(path): return glob.glob(path)\n\nprint(findFiles('data/names/*.txt'))\n\nimport unicodedata\nimport string\n\nall_letters = string.ascii_letters + \" .,;'\"\nn_letters = len(all_letters)\n\n# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\ndef unicodeToAscii(s):\n    return ''.join(\n        c for c in unicodedata.normalize('NFD', s)\n        if unicodedata.category(c) != 'Mn'\n        and c in all_letters\n    )\n\nprint(unicodeToAscii('Ślusàrski'))\n\n# Build the category_lines dictionary, a list of names per language\ncategory_lines = {}\nall_categories = []\n\n# Read a file and split into lines\ndef readLines(filename):\n    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n    return [unicodeToAscii(line) for line in lines]\n\nfor filename in findFiles('data/names/*.txt'):\n    category = os.path.splitext(os.path.basename(filename))[0]\n    all_categories.append(category)\n    lines = readLines(filename)\n    category_lines[category] = lines\n\nn_categories = len(all_categories)\n\nNow we have category_lines, a dictionary mapping each category (language) to a list of lines (names). We also kept track of all_categories (just a list of languages) and n_categories for later reference.\n\nprint(category_lines['Italian'][:5])",
    "crumbs": [
      "Pytorch",
      "Text",
      "NLP From Scratch: Classifying Names with a Character-Level RNN"
    ]
  },
  {
    "objectID": "Pytorch/Text/02_char_rnn_classification_tutorial.html#preparing-for-training",
    "href": "Pytorch/Text/02_char_rnn_classification_tutorial.html#preparing-for-training",
    "title": "NLP From Scratch: Classifying Names with a Character-Level RNN",
    "section": "Preparing for Training",
    "text": "Preparing for Training\nBefore going into training we should make a few helper functions. The first is to interpret the output of the network, which we know to be a likelihood of each category. We can use Tensor.topk to get the index of the greatest value:\n\ndef categoryFromOutput(output):\n    top_n, top_i = output.topk(1)\n    category_i = top_i[0].item()\n    return all_categories[category_i], category_i\n\nprint(categoryFromOutput(output))\n\nWe will also want a quick way to get a training example (a name and its language):\n\nimport random\n\ndef randomChoice(l):\n    return l[random.randint(0, len(l) - 1)]\n\ndef randomTrainingExample():\n    category = randomChoice(all_categories)\n    line = randomChoice(category_lines[category])\n    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n    line_tensor = lineToTensor(line)\n    return category, line, category_tensor, line_tensor\n\nfor i in range(10):\n    category, line, category_tensor, line_tensor = randomTrainingExample()\n    print('category =', category, '/ line =', line)",
    "crumbs": [
      "Pytorch",
      "Text",
      "NLP From Scratch: Classifying Names with a Character-Level RNN"
    ]
  },
  {
    "objectID": "Pytorch/Introduction/4.buildmodel_tutorial.html",
    "href": "Pytorch/Introduction/4.buildmodel_tutorial.html",
    "title": "DATAIDEA",
    "section": "",
    "text": "Photo by DATAIDEA\nimport os\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms",
    "crumbs": [
      "Pytorch",
      "Introduction",
      "Get Device for Training"
    ]
  },
  {
    "objectID": "Pytorch/Introduction/4.buildmodel_tutorial.html#get-device-for-training",
    "href": "Pytorch/Introduction/4.buildmodel_tutorial.html#get-device-for-training",
    "title": "DATAIDEA",
    "section": "Get Device for Training",
    "text": "Get Device for Training\nWe want to be able to train our model on a hardware accelerator like the GPU or MPS, if available. Let’s check to see if torch.cuda or torch.backends.mps are available, otherwise we use the CPU.\n\ndevice = (\n    \"cuda\"\n    if torch.cuda.is_available()\n    else \"mps\"\n    if torch.backends.mps.is_available()\n    else \"cpu\"\n)\nprint(f\"Using {device} device\")",
    "crumbs": [
      "Pytorch",
      "Introduction",
      "Get Device for Training"
    ]
  },
  {
    "objectID": "Pytorch/Introduction/4.buildmodel_tutorial.html#define-the-class",
    "href": "Pytorch/Introduction/4.buildmodel_tutorial.html#define-the-class",
    "title": "DATAIDEA",
    "section": "Define the Class",
    "text": "Define the Class\nWe define our neural network by subclassing nn.Module, and initialize the neural network layers in __init__. Every nn.Module subclass implements the operations on input data in the forward method.\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10),\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\nWe create an instance of NeuralNetwork, and move it to the device, and print its structure.\n\nmodel = NeuralNetwork().to(device)\nprint(model)\n\nTo use the model, we pass it the input data. This executes the model’s forward, along with some background operations. Do not call model.forward() directly!\nCalling the model on the input returns a 2-dimensional tensor with dim=0 corresponding to each output of 10 raw predicted values for each class, and dim=1 corresponding to the individual values of each output. We get the prediction probabilities by passing it through an instance of the nn.Softmax module.\n\nX = torch.rand(1, 28, 28, device=device)\nlogits = model(X)\npred_probab = nn.Softmax(dim=1)(logits)\ny_pred = pred_probab.argmax(1)\nprint(f\"Predicted class: {y_pred}\")",
    "crumbs": [
      "Pytorch",
      "Introduction",
      "Get Device for Training"
    ]
  },
  {
    "objectID": "Pytorch/Introduction/4.buildmodel_tutorial.html#model-layers",
    "href": "Pytorch/Introduction/4.buildmodel_tutorial.html#model-layers",
    "title": "DATAIDEA",
    "section": "Model Layers",
    "text": "Model Layers\nLet’s break down the layers in the FashionMNIST model. To illustrate it, we will take a sample minibatch of 3 images of size 28x28 and see what happens to it as we pass it through the network.\n\ninput_image = torch.rand(3,28,28)\nprint(input_image.size())\n\n\nnn.Flatten\nWe initialize the nn.Flatten layer to convert each 2D 28x28 image into a contiguous array of 784 pixel values ( the minibatch dimension (at dim=0) is maintained).\n\nflatten = nn.Flatten()\nflat_image = flatten(input_image)\nprint(flat_image.size())\n\n\n\nnn.Linear\nThe linear layer is a module that applies a linear transformation on the input using its stored weights and biases.\n\nlayer1 = nn.Linear(in_features=28*28, out_features=20)\nhidden1 = layer1(flat_image)\nprint(hidden1.size())\n\n\n\nnn.ReLU\nNon-linear activations are what create the complex mappings between the model’s inputs and outputs. They are applied after linear transformations to introduce nonlinearity, helping neural networks learn a wide variety of phenomena.\nIn this model, we use nn.ReLU between our linear layers, but there’s other activations to introduce non-linearity in your model.\n\nprint(f\"Before ReLU: {hidden1}\\n\\n\")\nhidden1 = nn.ReLU()(hidden1)\nprint(f\"After ReLU: {hidden1}\")\n\n\n\nnn.Sequential\nnn.Sequential is an ordered container of modules. The data is passed through all the modules in the same order as defined. You can use sequential containers to put together a quick network like seq_modules.\n\nseq_modules = nn.Sequential(\n    flatten,\n    layer1,\n    nn.ReLU(),\n    nn.Linear(20, 10)\n)\ninput_image = torch.rand(3,28,28)\nlogits = seq_modules(input_image)\n\n\n\nnn.Softmax\nThe last linear layer of the neural network returns logits - raw values in [-, ] - which are passed to the nn.Softmax module. The logits are scaled to values [0, 1] representing the model’s predicted probabilities for each class. dim parameter indicates the dimension along which the values must sum to 1.\n\nsoftmax = nn.Softmax(dim=1)\npred_probab = softmax(logits)",
    "crumbs": [
      "Pytorch",
      "Introduction",
      "Get Device for Training"
    ]
  },
  {
    "objectID": "Pytorch/Introduction/4.buildmodel_tutorial.html#model-parameters",
    "href": "Pytorch/Introduction/4.buildmodel_tutorial.html#model-parameters",
    "title": "DATAIDEA",
    "section": "Model Parameters",
    "text": "Model Parameters\nMany layers inside a neural network are parameterized, i.e. have associated weights and biases that are optimized during training. Subclassing nn.Module automatically tracks all fields defined inside your model object, and makes all parameters accessible using your model’s parameters() or named_parameters() methods.\nIn this example, we iterate over each parameter, and print its size and a preview of its values.\n\nprint(f\"Model structure: {model}\\n\\n\")\n\nfor name, param in model.named_parameters():\n    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")",
    "crumbs": [
      "Pytorch",
      "Introduction",
      "Get Device for Training"
    ]
  },
  {
    "objectID": "Pytorch/Introduction/4.buildmodel_tutorial.html#further-reading",
    "href": "Pytorch/Introduction/4.buildmodel_tutorial.html#further-reading",
    "title": "DATAIDEA",
    "section": "Further Reading",
    "text": "Further Reading\n\ntorch.nn API\n\nA few ads maybe displayed for income as resources are now offered freely. 🤝🤝🤝",
    "crumbs": [
      "Pytorch",
      "Introduction",
      "Get Device for Training"
    ]
  },
  {
    "objectID": "Pytorch/Introduction/5.autogradqs_tutorial.html",
    "href": "Pytorch/Introduction/5.autogradqs_tutorial.html",
    "title": "DATAIDEA",
    "section": "",
    "text": "Photo by DATAIDEA\nimport torch\n\nx = torch.ones(5)  # input tensor\ny = torch.zeros(3)  # expected output\nw = torch.randn(5, 3, requires_grad=True)\nb = torch.randn(3, requires_grad=True)\nz = torch.matmul(x, w)+b\nloss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)",
    "crumbs": [
      "Pytorch",
      "Introduction",
      "Tensors, Functions and Computational graph"
    ]
  },
  {
    "objectID": "Pytorch/Introduction/5.autogradqs_tutorial.html#tensors-functions-and-computational-graph",
    "href": "Pytorch/Introduction/5.autogradqs_tutorial.html#tensors-functions-and-computational-graph",
    "title": "DATAIDEA",
    "section": "Tensors, Functions and Computational graph",
    "text": "Tensors, Functions and Computational graph\nThis code defines the following computational graph:\n.. figure:: /_static/img/basics/comp-graph.png :alt:\nIn this network, w and b are parameters, which we need to optimize. Thus, we need to be able to compute the gradients of loss function with respect to those variables. In order to do that, we set the requires_grad property of those tensors.\n\n\nNote\n\n\nYou can set the value of requires_grad when creating a tensor, or later by using x.requires_grad_(True) method.\n\n\nA function that we apply to tensors to construct computational graph is in fact an object of class Function. This object knows how to compute the function in the forward direction, and also how to compute its derivative during the backward propagation step. A reference to the backward propagation function is stored in grad_fn property of a tensor. You can find more information of Function in the documentation_.\n\nprint(f\"Gradient function for z = {z.grad_fn}\")\nprint(f\"Gradient function for loss = {loss.grad_fn}\")",
    "crumbs": [
      "Pytorch",
      "Introduction",
      "Tensors, Functions and Computational graph"
    ]
  },
  {
    "objectID": "Pytorch/Introduction/5.autogradqs_tutorial.html#computing-gradients",
    "href": "Pytorch/Introduction/5.autogradqs_tutorial.html#computing-gradients",
    "title": "DATAIDEA",
    "section": "Computing Gradients",
    "text": "Computing Gradients\nTo optimize weights of parameters in the neural network, we need to compute the derivatives of our loss function with respect to parameters, namely, we need \\(\\frac{\\partial loss}{\\partial w}\\) and \\(\\frac{\\partial loss}{\\partial b}\\) under some fixed values of x and y. To compute those derivatives, we call loss.backward(), and then retrieve the values from w.grad and b.grad:\n\nloss.backward()\nprint(w.grad)\nprint(b.grad)\n\n\n\nNote\n\n\n\nWe can only obtain the grad properties for the leaf nodes of the computational graph, which have requires_grad property set to True. For all other nodes in our graph, gradients will not be available.\n\nWe can only perform gradient calculations using backward once on a given graph, for performance reasons. If we need to do several backward calls on the same graph, we need to pass retain_graph=True to the backward call.",
    "crumbs": [
      "Pytorch",
      "Introduction",
      "Tensors, Functions and Computational graph"
    ]
  },
  {
    "objectID": "Pytorch/Introduction/5.autogradqs_tutorial.html#disabling-gradient-tracking",
    "href": "Pytorch/Introduction/5.autogradqs_tutorial.html#disabling-gradient-tracking",
    "title": "DATAIDEA",
    "section": "Disabling Gradient Tracking",
    "text": "Disabling Gradient Tracking\nBy default, all tensors with requires_grad=True are tracking their computational history and support gradient computation. However, there are some cases when we do not need to do that, for example, when we have trained the model and just want to apply it to some input data, i.e. we only want to do forward computations through the network. We can stop tracking computations by surrounding our computation code with torch.no_grad() block:\n\nz = torch.matmul(x, w)+b\nprint(z.requires_grad)\n\nwith torch.no_grad():\n    z = torch.matmul(x, w)+b\nprint(z.requires_grad)\n\nAnother way to achieve the same result is to use the detach() method on the tensor:\n\nz = torch.matmul(x, w)+b\nz_det = z.detach()\nprint(z_det.requires_grad)\n\nThere are reasons you might want to disable gradient tracking: - To mark some parameters in your neural network as frozen parameters. - To speed up computations when you are only doing forward pass, because computations on tensors that do not track gradients would be more efficient.",
    "crumbs": [
      "Pytorch",
      "Introduction",
      "Tensors, Functions and Computational graph"
    ]
  },
  {
    "objectID": "Pytorch/Introduction/5.autogradqs_tutorial.html#more-on-computational-graphs",
    "href": "Pytorch/Introduction/5.autogradqs_tutorial.html#more-on-computational-graphs",
    "title": "DATAIDEA",
    "section": "More on Computational Graphs",
    "text": "More on Computational Graphs\nConceptually, autograd keeps a record of data (tensors) and all executed operations (along with the resulting new tensors) in a directed acyclic graph (DAG) consisting of Function_ objects. In this DAG, leaves are the input tensors, roots are the output tensors. By tracing this graph from roots to leaves, you can automatically compute the gradients using the chain rule.\nIn a forward pass, autograd does two things simultaneously:\n\nrun the requested operation to compute a resulting tensor\nmaintain the operation’s gradient function in the DAG.\n\nThe backward pass kicks off when .backward() is called on the DAG root. autograd then:\n\ncomputes the gradients from each .grad_fn,\naccumulates them in the respective tensor’s .grad attribute\nusing the chain rule, propagates all the way to the leaf tensors.\n\n\n\nNote\n\n\nDAGs are dynamic in PyTorch An important thing to note is that the graph is recreated from scratch; after each .backward() call, autograd starts populating a new graph. This is exactly what allows you to use control flow statements in your model; you can change the shape, size and operations at every iteration if needed.",
    "crumbs": [
      "Pytorch",
      "Introduction",
      "Tensors, Functions and Computational graph"
    ]
  },
  {
    "objectID": "Pytorch/Introduction/5.autogradqs_tutorial.html#optional-reading-tensor-gradients-and-jacobian-products",
    "href": "Pytorch/Introduction/5.autogradqs_tutorial.html#optional-reading-tensor-gradients-and-jacobian-products",
    "title": "DATAIDEA",
    "section": "Optional Reading: Tensor Gradients and Jacobian Products",
    "text": "Optional Reading: Tensor Gradients and Jacobian Products\nIn many cases, we have a scalar loss function, and we need to compute the gradient with respect to some parameters. However, there are cases when the output function is an arbitrary tensor. In this case, PyTorch allows you to compute so-called Jacobian product, and not the actual gradient.\nFor a vector function \\(\\vec{y}=f(\\vec{x})\\), where \\(\\vec{x}=\\langle x_1,\\dots,x_n\\rangle\\) and \\(\\vec{y}=\\langle y_1,\\dots,y_m\\rangle\\), a gradient of \\(\\vec{y}\\) with respect to \\(\\vec{x}\\) is given by Jacobian matrix:\n\\[\\begin{align}J=\\left(\\begin{array}{ccc}\n      \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n      \\vdots & \\ddots & \\vdots\\\\\n      \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n      \\end{array}\\right)\\end{align}\\]\nInstead of computing the Jacobian matrix itself, PyTorch allows you to compute Jacobian Product \\(v^T\\cdot J\\) for a given input vector \\(v=(v_1 \\dots v_m)\\). This is achieved by calling backward with \\(v\\) as an argument. The size of \\(v\\) should be the same as the size of the original tensor, with respect to which we want to compute the product:\n\ninp = torch.eye(4, 5, requires_grad=True)\nout = (inp+1).pow(2).t()\nout.backward(torch.ones_like(out), retain_graph=True)\nprint(f\"First call\\n{inp.grad}\")\nout.backward(torch.ones_like(out), retain_graph=True)\nprint(f\"\\nSecond call\\n{inp.grad}\")\ninp.grad.zero_()\nout.backward(torch.ones_like(out), retain_graph=True)\nprint(f\"\\nCall after zeroing gradients\\n{inp.grad}\")\n\nNotice that when we call backward for the second time with the same argument, the value of the gradient is different. This happens because when doing backward propagation, PyTorch accumulates the gradients, i.e. the value of computed gradients is added to the grad property of all leaf nodes of computational graph. If you want to compute the proper gradients, you need to zero out the grad property before. In real-life training an optimizer helps us to do this.\n\n\nNote\n\n\nPreviously we were calling backward() function without parameters. This is essentially equivalent to calling backward(torch.tensor(1.0)), which is a useful way to compute the gradients in case of a scalar-valued function, such as loss during neural network training.\n\n\n\n\nFurther Reading\n\nAutograd Mechanics\n\nA few ads maybe displayed for income as resources are now offered freely. 🤝🤝🤝",
    "crumbs": [
      "Pytorch",
      "Introduction",
      "Tensors, Functions and Computational graph"
    ]
  },
  {
    "objectID": "Pytorch/Introduction/2.data_tutorial.html",
    "href": "Pytorch/Introduction/2.data_tutorial.html",
    "title": "Datasets & DataLoaders",
    "section": "",
    "text": "Photo by DATAIDEA\nCode for processing data samples can get messy and hard to maintain; we ideally want our dataset code to be decoupled from our model training code for better readability and modularity. PyTorch provides two data primitives: torch.utils.data.DataLoader and torch.utils.data.Dataset that allow you to use pre-loaded datasets as well as your own data. Dataset stores the samples and their corresponding labels, and DataLoader wraps an iterable around the Dataset to enable easy access to the samples.\nPyTorch domain libraries provide a number of pre-loaded datasets (such as FashionMNIST) that subclass torch.utils.data.Dataset and implement functions specific to the particular data. They can be used to prototype and benchmark your model. You can find them here: Image Datasets, Text Datasets, and Audio Datasets",
    "crumbs": [
      "Pytorch",
      "Introduction",
      "Datasets & DataLoaders"
    ]
  },
  {
    "objectID": "Pytorch/Introduction/2.data_tutorial.html#loading-a-dataset",
    "href": "Pytorch/Introduction/2.data_tutorial.html#loading-a-dataset",
    "title": "Datasets & DataLoaders",
    "section": "Loading a Dataset",
    "text": "Loading a Dataset\nHere is an example of how to load the Fashion-MNIST dataset from TorchVision. Fashion-MNIST is a dataset of Zalando’s article images consisting of 60,000 training examples and 10,000 test examples. Each example comprises a 28×28 grayscale image and an associated label from one of 10 classes.\nWe load the FashionMNIST Dataset with the following parameters: - root is the path where the train/test data is stored, - train specifies training or test dataset, - download=True downloads the data from the internet if it’s not available at root. - transform and target_transform specify the feature and label transformations\n\nimport torch\nfrom torch.utils.data import Dataset\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\nimport matplotlib.pyplot as plt\n\n# set download=True to download the data\ntraining_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=True,\n    download=False,\n    transform=ToTensor()\n)\n\ntest_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=False,\n    download=False,\n    transform=ToTensor()\n)",
    "crumbs": [
      "Pytorch",
      "Introduction",
      "Datasets & DataLoaders"
    ]
  },
  {
    "objectID": "Pytorch/Introduction/2.data_tutorial.html#iterating-and-visualizing-the-dataset",
    "href": "Pytorch/Introduction/2.data_tutorial.html#iterating-and-visualizing-the-dataset",
    "title": "Datasets & DataLoaders",
    "section": "Iterating and Visualizing the Dataset",
    "text": "Iterating and Visualizing the Dataset\nWe can index Datasets manually like a list: training_data[index]. We use matplotlib to visualize some samples in our training data.\n\nlabels_map = {\n    0: \"T-Shirt\",\n    1: \"Trouser\",\n    2: \"Pullover\",\n    3: \"Dress\",\n    4: \"Coat\",\n    5: \"Sandal\",\n    6: \"Shirt\",\n    7: \"Sneaker\",\n    8: \"Bag\",\n    9: \"Ankle Boot\",\n}\nfigure = plt.figure(figsize=(8, 8))\ncols, rows = 3, 3\nfor i in range(1, cols * rows + 1):\n    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n    img, label = training_data[sample_idx]\n    figure.add_subplot(rows, cols, i)\n    plt.title(labels_map[label])\n    plt.axis(\"off\")\n    plt.imshow(img.squeeze(), cmap=\"gray\")\nplt.show()\n\n.. .. figure:: /_static/img/basics/fashion_mnist.png :alt: fashion_mnist",
    "crumbs": [
      "Pytorch",
      "Introduction",
      "Datasets & DataLoaders"
    ]
  },
  {
    "objectID": "Pytorch/Introduction/2.data_tutorial.html#creating-a-custom-dataset-for-your-files",
    "href": "Pytorch/Introduction/2.data_tutorial.html#creating-a-custom-dataset-for-your-files",
    "title": "Datasets & DataLoaders",
    "section": "Creating a Custom Dataset for your files",
    "text": "Creating a Custom Dataset for your files\nA custom Dataset class must implement three functions: __init__, __len__, and __getitem__. Take a look at this implementation; the FashionMNIST images are stored in a directory img_dir, and their labels are stored separately in a CSV file annotations_file.\nIn the next sections, we’ll break down what’s happening in each of these functions.\n\nimport os\nimport pandas as pd\nfrom torchvision.io import read_image\n\nclass CustomImageDataset(Dataset):\n    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n        self.img_labels = pd.read_csv(annotations_file)\n        self.img_dir = img_dir\n        self.transform = transform\n        self.target_transform = target_transform\n\n    def __len__(self):\n        return len(self.img_labels)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n        image = read_image(img_path)\n        label = self.img_labels.iloc[idx, 1]\n        if self.transform:\n            image = self.transform(image)\n        if self.target_transform:\n            label = self.target_transform(label)\n        return image, label\n\n\ninit\nThe init function is run once when instantiating the Dataset object. We initialize the directory containing the images, the annotations file, and both transforms (covered in more detail in the next section).\nThe labels.csv file looks like: ::\ntshirt1.jpg, 0\ntshirt2.jpg, 0\n......\nankleboot999.jpg, 9\n\ndef __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n    self.img_labels = pd.read_csv(annotations_file)\n    self.img_dir = img_dir\n    self.transform = transform\n    self.target_transform = target_transform\n\n\n\nlen\nThe len function returns the number of samples in our dataset.\nExample:\n\ndef __len__(self):\n    return len(self.img_labels)\n\n\n\ngetitem\nThe getitem function loads and returns a sample from the dataset at the given index idx. Based on the index, it identifies the image’s location on disk, converts that to a tensor using read_image, retrieves the corresponding label from the csv data in self.img_labels, calls the transform functions on them (if applicable), and returns the tensor image and corresponding label in a tuple.\n\ndef __getitem__(self, idx):\n    img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n    image = read_image(img_path)\n    label = self.img_labels.iloc[idx, 1]\n    if self.transform:\n        image = self.transform(image)\n    if self.target_transform:\n        label = self.target_transform(label)\n    return image, label",
    "crumbs": [
      "Pytorch",
      "Introduction",
      "Datasets & DataLoaders"
    ]
  },
  {
    "objectID": "Pytorch/Introduction/2.data_tutorial.html#preparing-your-data-for-training-with-dataloaders",
    "href": "Pytorch/Introduction/2.data_tutorial.html#preparing-your-data-for-training-with-dataloaders",
    "title": "Datasets & DataLoaders",
    "section": "Preparing your data for training with DataLoaders",
    "text": "Preparing your data for training with DataLoaders\nThe Dataset retrieves our dataset’s features and labels one sample at a time. While training a model, we typically want to pass samples in “minibatches”, reshuffle the data at every epoch to reduce model overfitting, and use Python’s multiprocessing to speed up data retrieval.\nDataLoader is an iterable that abstracts this complexity for us in an easy API.\n\nfrom torch.utils.data import DataLoader\n\ntrain_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\ntest_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)",
    "crumbs": [
      "Pytorch",
      "Introduction",
      "Datasets & DataLoaders"
    ]
  },
  {
    "objectID": "Pytorch/Introduction/2.data_tutorial.html#iterate-through-the-dataloader",
    "href": "Pytorch/Introduction/2.data_tutorial.html#iterate-through-the-dataloader",
    "title": "Datasets & DataLoaders",
    "section": "Iterate through the DataLoader",
    "text": "Iterate through the DataLoader\nWe have loaded that dataset into the DataLoader and can iterate through the dataset as needed. Each iteration below returns a batch of train_features and train_labels (containing batch_size=64 features and labels respectively). Because we specified shuffle=True, after we iterate over all batches the data is shuffled (for finer-grained control over the data loading order, take a look at Samplers).\n\n# Display image and label.\ntrain_features, train_labels = next(iter(train_dataloader))\nprint(f\"Feature batch shape: {train_features.size()}\")\nprint(f\"Labels batch shape: {train_labels.size()}\")\nimg = train_features[0].squeeze()\nlabel = train_labels[0]\nplt.imshow(img, cmap=\"gray\")\nplt.show()\nprint(f\"Label: {label}\")",
    "crumbs": [
      "Pytorch",
      "Introduction",
      "Datasets & DataLoaders"
    ]
  },
  {
    "objectID": "Pytorch/Introduction/2.data_tutorial.html#further-reading",
    "href": "Pytorch/Introduction/2.data_tutorial.html#further-reading",
    "title": "Datasets & DataLoaders",
    "section": "Further Reading",
    "text": "Further Reading\n\ntorch.utils.data API",
    "crumbs": [
      "Pytorch",
      "Introduction",
      "Datasets & DataLoaders"
    ]
  },
  {
    "objectID": "Pytorch/Introduction/6.optimization_tutorial.html",
    "href": "Pytorch/Introduction/6.optimization_tutorial.html",
    "title": "Optimizing Model Parameters",
    "section": "",
    "text": "Photo by DATAIDEA\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\n\n# set download=True to download the data\ntraining_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=True,\n    download=False,\n    transform=ToTensor()\n)\n\ntest_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=False,\n    download=False,\n    transform=ToTensor()\n)\n\ntrain_dataloader = DataLoader(training_data, batch_size=64)\ntest_dataloader = DataLoader(test_data, batch_size=64)\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10),\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\nmodel = NeuralNetwork()",
    "crumbs": [
      "Pytorch",
      "Introduction",
      "Optimizing Model Parameters"
    ]
  },
  {
    "objectID": "Pytorch/Introduction/6.optimization_tutorial.html#hyperparameters",
    "href": "Pytorch/Introduction/6.optimization_tutorial.html#hyperparameters",
    "title": "Optimizing Model Parameters",
    "section": "Hyperparameters",
    "text": "Hyperparameters\nHyperparameters are adjustable parameters that let you control the model optimization process. Different hyperparameter values can impact model training and convergence rates (read more_ about hyperparameter tuning)\nWe define the following hyperparameters for training: - Number of Epochs - the number times to iterate over the dataset - Batch Size - the number of data samples propagated through the network before the parameters are updated - Learning Rate - how much to update models parameters at each batch/epoch. Smaller values yield slow learning speed, while large values may result in unpredictable behavior during training.\n\nlearning_rate = 1e-3\nbatch_size = 64\nepochs = 5",
    "crumbs": [
      "Pytorch",
      "Introduction",
      "Optimizing Model Parameters"
    ]
  },
  {
    "objectID": "Pytorch/Introduction/6.optimization_tutorial.html#optimization-loop",
    "href": "Pytorch/Introduction/6.optimization_tutorial.html#optimization-loop",
    "title": "Optimizing Model Parameters",
    "section": "Optimization Loop",
    "text": "Optimization Loop\nOnce we set our hyperparameters, we can then train and optimize our model with an optimization loop. Each iteration of the optimization loop is called an epoch.\nEach epoch consists of two main parts: - The Train Loop - iterate over the training dataset and try to converge to optimal parameters. - The Validation/Test Loop - iterate over the test dataset to check if model performance is improving.\nLet’s briefly familiarize ourselves with some of the concepts used in the training loop. Jump ahead to see the full-impl-label of the optimization loop.\n\nLoss Function\nWhen presented with some training data, our untrained network is likely not to give the correct answer. Loss function measures the degree of dissimilarity of obtained result to the target value, and it is the loss function that we want to minimize during training. To calculate the loss we make a prediction using the inputs of our given data sample and compare it against the true data label value.\nCommon loss functions include nn.MSELoss (Mean Square Error) for regression tasks, and nn.NLLLoss (Negative Log Likelihood) for classification. nn.CrossEntropyLoss combines nn.LogSoftmax and nn.NLLLoss.\nWe pass our model’s output logits to nn.CrossEntropyLoss, which will normalize the logits and compute the prediction error.\n\n# Initialize the loss function\nloss_fn = nn.CrossEntropyLoss()\n\n\n\nOptimizer\nOptimization is the process of adjusting model parameters to reduce model error in each training step. Optimization algorithms define how this process is performed (in this example we use Stochastic Gradient Descent). All optimization logic is encapsulated in the optimizer object. Here, we use the SGD optimizer; additionally, there are many different optimizers available in PyTorch such as ADAM and RMSProp, that work better for different kinds of models and data.\nWe initialize the optimizer by registering the model’s parameters that need to be trained, and passing in the learning rate hyperparameter.\n\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n\nInside the training loop, optimization happens in three steps: * Call optimizer.zero_grad() to reset the gradients of model parameters. Gradients by default add up; to prevent double-counting, we explicitly zero them at each iteration. * Backpropagate the prediction loss with a call to loss.backward(). PyTorch deposits the gradients of the loss w.r.t. each parameter. * Once we have our gradients, we call optimizer.step() to adjust the parameters by the gradients collected in the backward pass.",
    "crumbs": [
      "Pytorch",
      "Introduction",
      "Optimizing Model Parameters"
    ]
  },
  {
    "objectID": "Pytorch/Introduction/6.optimization_tutorial.html#full-implementation",
    "href": "Pytorch/Introduction/6.optimization_tutorial.html#full-implementation",
    "title": "Optimizing Model Parameters",
    "section": "Full Implementation",
    "text": "Full Implementation\nWe define train_loop that loops over our optimization code, and test_loop that evaluates the model’s performance against our test data.\n\ndef train_loop(dataloader, model, loss_fn, optimizer):\n    size = len(dataloader.dataset)\n    for batch, (X, y) in enumerate(dataloader):\n        # Compute prediction and loss\n        pred = model(X)\n        loss = loss_fn(pred, y)\n\n        # Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), (batch + 1) * len(X)\n            print(f\"loss: {loss:&gt;7f}  [{current:&gt;5d}/{size:&gt;5d}]\")\n\n\ndef test_loop(dataloader, model, loss_fn):\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    test_loss, correct = 0, 0\n\n    with torch.no_grad():\n        for X, y in dataloader:\n            pred = model(X)\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n\n    test_loss /= num_batches\n    correct /= size\n    print(f\"Test Error: \\n Accuracy: {(100*correct):&gt;0.1f}%, Avg loss: {test_loss:&gt;8f} \\n\")\n\nWe initialize the loss function and optimizer, and pass it to train_loop and test_loop. Feel free to increase the number of epochs to track the model’s improving performance.\n\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n\nepochs = 10\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train_loop(train_dataloader, model, loss_fn, optimizer)\n    test_loop(test_dataloader, model, loss_fn)\nprint(\"Done!\")",
    "crumbs": [
      "Pytorch",
      "Introduction",
      "Optimizing Model Parameters"
    ]
  },
  {
    "objectID": "Pytorch/Introduction/6.optimization_tutorial.html#further-reading",
    "href": "Pytorch/Introduction/6.optimization_tutorial.html#further-reading",
    "title": "Optimizing Model Parameters",
    "section": "Further Reading",
    "text": "Further Reading\n\nLoss Functions\ntorch.optim\nWarmstart Training a Model\n\nA few ads maybe displayed for income as resources are now offered freely. 🤝🤝🤝",
    "crumbs": [
      "Pytorch",
      "Introduction",
      "Optimizing Model Parameters"
    ]
  },
  {
    "objectID": "Pytorch/pytorch_outline.html",
    "href": "Pytorch/pytorch_outline.html",
    "title": "DATAIDEA",
    "section": "",
    "text": "Photo by DATAIDEA",
    "crumbs": [
      "Pytorch",
      "Pytorch Course Outline"
    ]
  },
  {
    "objectID": "Pytorch/pytorch_outline.html#pytorch-course-outline",
    "href": "Pytorch/pytorch_outline.html#pytorch-course-outline",
    "title": "DATAIDEA",
    "section": "Pytorch Course Outline",
    "text": "Pytorch Course Outline\n\nIntroduction\n\nQuickstart: Overview of PyTorch, and how to get started quickly with installation and basic usage.\nTensors: Introduction to PyTorch tensors, their properties, and basic tensor operations.\nData: Handling data in PyTorch, including loading datasets, creating custom datasets.\nTransforms: Preprocessing and data augmentation techniques using transforms in PyTorch.\nBuilding Models: Creating and building neural network models in PyTorch using the nn.Module API.\nAutograd: Understanding automatic differentiation in PyTorch and how it enables gradient-based optimization.\nOptimization: Optimization techniques and algorithms for training neural networks.\nSaving, Loading, and Running Models: Techniques for saving and loading trained models in PyTorch.",
    "crumbs": [
      "Pytorch",
      "Pytorch Course Outline"
    ]
  },
  {
    "objectID": "Pytorch/pytorch_outline.html#text",
    "href": "Pytorch/pytorch_outline.html#text",
    "title": "DATAIDEA",
    "section": "Text",
    "text": "Text\n\nFast Transformer Inference with Better Transformer\nNLP From Scratch: Classifying Names with a Character-Level RNN\nNLP From Scratch: Generating Names with a Character-Level RNN\nNLP From Scratch: Translation with a Sequence to Sequence Network and Attention\nText classification with the torchtext library\nLanguage Translation with nn.Transformer and torchtext\nPreprocess custom text dataset using Torchtext",
    "crumbs": [
      "Pytorch",
      "Pytorch Course Outline"
    ]
  },
  {
    "objectID": "Pytorch/pytorch_outline.html#vision",
    "href": "Pytorch/pytorch_outline.html#vision",
    "title": "DATAIDEA",
    "section": "Vision",
    "text": "Vision\n\nTorchVision Object Detection Finetuning Tutorial\nTransfer Learning for Computer Vision Tutorial\nAdversarial Example Generation\nDCGAN Tutorial\nSpatial Transformer Networks Tutorial\nOptimizing Vision Transformer Model for Deployment\n\n\n\nDon’t miss out on any updates and developments! Subscribe to the DATAIDEA Newsletter it’s easy and safe.",
    "crumbs": [
      "Pytorch",
      "Pytorch Course Outline"
    ]
  },
  {
    "objectID": "Pytorch/Vision/fgsm_tutorial.html",
    "href": "Pytorch/Vision/fgsm_tutorial.html",
    "title": "Adversarial Example Generation",
    "section": "",
    "text": "Photo by DATAIDEA",
    "crumbs": [
      "Pytorch",
      "Vision",
      "Adversarial Example Generation"
    ]
  },
  {
    "objectID": "Pytorch/Vision/fgsm_tutorial.html#threat-model",
    "href": "Pytorch/Vision/fgsm_tutorial.html#threat-model",
    "title": "Adversarial Example Generation",
    "section": "Threat Model",
    "text": "Threat Model\nFor context, there are many categories of adversarial attacks, each with a different goal and assumption of the attacker’s knowledge. However, in general the overarching goal is to add the least amount of perturbation to the input data to cause the desired misclassification. There are several kinds of assumptions of the attacker’s knowledge, two of which are: white-box and black-box. A white-box attack assumes the attacker has full knowledge and access to the model, including architecture, inputs, outputs, and weights. A black-box attack assumes the attacker only has access to the inputs and outputs of the model, and knows nothing about the underlying architecture or weights. There are also several types of goals, including misclassification and source/target misclassification. A goal of misclassification means the adversary only wants the output classification to be wrong but does not care what the new classification is. A source/target misclassification means the adversary wants to alter an image that is originally of a specific source class so that it is classified as a specific target class.\nIn this case, the FGSM attack is a white-box attack with the goal of misclassification. With this background information, we can now discuss the attack in detail.",
    "crumbs": [
      "Pytorch",
      "Vision",
      "Adversarial Example Generation"
    ]
  },
  {
    "objectID": "Pytorch/Vision/fgsm_tutorial.html#fast-gradient-sign-attack",
    "href": "Pytorch/Vision/fgsm_tutorial.html#fast-gradient-sign-attack",
    "title": "Adversarial Example Generation",
    "section": "Fast Gradient Sign Attack",
    "text": "Fast Gradient Sign Attack\nOne of the first and most popular adversarial attacks to date is referred to as the Fast Gradient Sign Attack (FGSM) and is described by Goodfellow et. al. in Explaining and Harnessing Adversarial Examples_. The attack is remarkably powerful, and yet intuitive. It is designed to attack neural networks by leveraging the way they learn, gradients. The idea is simple, rather than working to minimize the loss by adjusting the weights based on the backpropagated gradients, the attack adjusts the input data to maximize the loss based on the same backpropagated gradients. In other words, the attack uses the gradient of the loss w.r.t the input data, then adjusts the input data to maximize the loss.\nBefore we jump into the code, let’s look at the famous FGSM_ panda example and extract some notation.\n.. figure:: /_static/img/fgsm_panda_image.png :alt: fgsm_panda_image\nFrom the figure, \\(\\mathbf{x}\\) is the original input image correctly classified as a “panda”, \\(y\\) is the ground truth label for \\(\\mathbf{x}\\), \\(\\mathbf{\\theta}\\) represents the model parameters, and \\(J(\\mathbf{\\theta}, \\mathbf{x}, y)\\) is the loss that is used to train the network. The attack backpropagates the gradient back to the input data to calculate \\(\\nabla_{x} J(\\mathbf{\\theta}, \\mathbf{x}, y)\\). Then, it adjusts the input data by a small step (\\(\\epsilon\\) or \\(0.007\\) in the picture) in the direction (i.e. \\(sign(\\nabla_{x} J(\\mathbf{\\theta}, \\mathbf{x}, y))\\)) that will maximize the loss. The resulting perturbed image, \\(x'\\), is then misclassified by the target network as a “gibbon” when it is still clearly a “panda”.\nHopefully now the motivation for this tutorial is clear, so lets jump into the implementation.\n\nfrom __future__ import print_function\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# NOTE: This is a hack to get around \"User-agent\" limitations when downloading MNIST datasets\n#       see, https://github.com/pytorch/vision/issues/3497 for more information\nfrom six.moves import urllib\nopener = urllib.request.build_opener()\nopener.addheaders = [('User-agent', 'Mozilla/5.0')]\nurllib.request.install_opener(opener)",
    "crumbs": [
      "Pytorch",
      "Vision",
      "Adversarial Example Generation"
    ]
  },
  {
    "objectID": "Pytorch/Vision/fgsm_tutorial.html#implementation",
    "href": "Pytorch/Vision/fgsm_tutorial.html#implementation",
    "title": "Adversarial Example Generation",
    "section": "Implementation",
    "text": "Implementation\nIn this section, we will discuss the input parameters for the tutorial, define the model under attack, then code the attack and run some tests.\n\nInputs\nThere are only three inputs for this tutorial, and are defined as follows:\n\nepsilons - List of epsilon values to use for the run. It is important to keep 0 in the list because it represents the model performance on the original test set. Also, intuitively we would expect the larger the epsilon, the more noticeable the perturbations but the more effective the attack in terms of degrading model accuracy. Since the data range here is \\([0,1]\\), no epsilon value should exceed 1.\npretrained_model - path to the pretrained MNIST model which was trained with pytorch/examples/mnist. For simplicity, download the pretrained model here.\nuse_cuda - boolean flag to use CUDA if desired and available. Note, a GPU with CUDA is not critical for this tutorial as a CPU will not take much time.\n\n\nepsilons = [0, .05, .1, .15, .2, .25, .3]\npretrained_model = \"data/lenet_mnist_model.pth\"\nuse_cuda=True\n\n\n\nModel Under Attack\nAs mentioned, the model under attack is the same MNIST model from pytorch/examples/mnist_. You may train and save your own MNIST model or you can download and use the provided model. The Net definition and test dataloader here have been copied from the MNIST example. The purpose of this section is to define the model and dataloader, then initialize the model and load the pretrained weights.\n\n# LeNet Model definition\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n\n# MNIST Test dataset and dataloader declaration\ntest_loader = torch.utils.data.DataLoader(\n    datasets.MNIST('../data', train=False, download=True, transform=transforms.Compose([\n            transforms.ToTensor(),\n            ])), \n        batch_size=1, shuffle=True)\n\n# Define what device we are using\nprint(\"CUDA Available: \",torch.cuda.is_available())\ndevice = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")\n\n# Initialize the network\nmodel = Net().to(device)\n\n# Load the pretrained model\nmodel.load_state_dict(torch.load(pretrained_model, map_location='cpu'))\n\n# Set the model in evaluation mode. In this case this is for the Dropout layers\nmodel.eval()\n\n\n\nFGSM Attack\nNow, we can define the function that creates the adversarial examples by perturbing the original inputs. The fgsm_attack function takes three inputs, image is the original clean image (\\(x\\)), epsilon is the pixel-wise perturbation amount (\\(\\epsilon\\)), and data_grad is gradient of the loss w.r.t the input image (\\(\\nabla_{x} J(\\mathbf{\\theta}, \\mathbf{x}, y)\\)). The function then creates perturbed image as\n\\[\\begin{align}perturbed\\_image = image + epsilon*sign(data\\_grad) = x + \\epsilon * sign(\\nabla_{x} J(\\mathbf{\\theta}, \\mathbf{x}, y))\\end{align}\\]\nFinally, in order to maintain the original range of the data, the perturbed image is clipped to range \\([0,1]\\).\n\n# FGSM attack code\ndef fgsm_attack(image, epsilon, data_grad):\n    # Collect the element-wise sign of the data gradient\n    sign_data_grad = data_grad.sign()\n    # Create the perturbed image by adjusting each pixel of the input image\n    perturbed_image = image + epsilon*sign_data_grad\n    # Adding clipping to maintain [0,1] range\n    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n    # Return the perturbed image\n    return perturbed_image\n\n\n\nTesting Function\nFinally, the central result of this tutorial comes from the test function. Each call to this test function performs a full test step on the MNIST test set and reports a final accuracy. However, notice that this function also takes an epsilon input. This is because the test function reports the accuracy of a model that is under attack from an adversary with strength \\(\\epsilon\\). More specifically, for each sample in the test set, the function computes the gradient of the loss w.r.t the input data (\\(data\\_grad\\)), creates a perturbed image with fgsm_attack (\\(perturbed\\_data\\)), then checks to see if the perturbed example is adversarial. In addition to testing the accuracy of the model, the function also saves and returns some successful adversarial examples to be visualized later.\n\ndef test( model, device, test_loader, epsilon ):\n\n    # Accuracy counter\n    correct = 0\n    adv_examples = []\n\n    # Loop over all examples in test set\n    for data, target in test_loader:\n\n        # Send the data and label to the device\n        data, target = data.to(device), target.to(device)\n\n        # Set requires_grad attribute of tensor. Important for Attack\n        data.requires_grad = True\n\n        # Forward pass the data through the model\n        output = model(data)\n        init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n\n        # If the initial prediction is wrong, don't bother attacking, just move on\n        if init_pred.item() != target.item():\n            continue\n\n        # Calculate the loss\n        loss = F.nll_loss(output, target)\n\n        # Zero all existing gradients\n        model.zero_grad()\n\n        # Calculate gradients of model in backward pass\n        loss.backward()\n\n        # Collect ``datagrad``\n        data_grad = data.grad.data\n\n        # Call FGSM Attack\n        perturbed_data = fgsm_attack(data, epsilon, data_grad)\n\n        # Re-classify the perturbed image\n        output = model(perturbed_data)\n\n        # Check for success\n        final_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n        if final_pred.item() == target.item():\n            correct += 1\n            # Special case for saving 0 epsilon examples\n            if (epsilon == 0) and (len(adv_examples) &lt; 5):\n                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n        else:\n            # Save some adv examples for visualization later\n            if len(adv_examples) &lt; 5:\n                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n\n    # Calculate final accuracy for this epsilon\n    final_acc = correct/float(len(test_loader))\n    print(\"Epsilon: {}\\tTest Accuracy = {} / {} = {}\".format(epsilon, correct, len(test_loader), final_acc))\n\n    # Return the accuracy and an adversarial example\n    return final_acc, adv_examples\n\n\n\nRun Attack\nThe last part of the implementation is to actually run the attack. Here, we run a full test step for each epsilon value in the epsilons input. For each epsilon we also save the final accuracy and some successful adversarial examples to be plotted in the coming sections. Notice how the printed accuracies decrease as the epsilon value increases. Also, note the \\(\\epsilon=0\\) case represents the original test accuracy, with no attack.\n\naccuracies = []\nexamples = []\n\n# Run test for each epsilon\nfor eps in epsilons:\n    acc, ex = test(model, device, test_loader, eps)\n    accuracies.append(acc)\n    examples.append(ex)",
    "crumbs": [
      "Pytorch",
      "Vision",
      "Adversarial Example Generation"
    ]
  },
  {
    "objectID": "Pytorch/Vision/fgsm_tutorial.html#results",
    "href": "Pytorch/Vision/fgsm_tutorial.html#results",
    "title": "Adversarial Example Generation",
    "section": "Results",
    "text": "Results\n\nAccuracy vs Epsilon\nThe first result is the accuracy versus epsilon plot. As alluded to earlier, as epsilon increases we expect the test accuracy to decrease. This is because larger epsilons mean we take a larger step in the direction that will maximize the loss. Notice the trend in the curve is not linear even though the epsilon values are linearly spaced. For example, the accuracy at \\(\\epsilon=0.05\\) is only about 4% lower than \\(\\epsilon=0\\), but the accuracy at \\(\\epsilon=0.2\\) is 25% lower than \\(\\epsilon=0.15\\). Also, notice the accuracy of the model hits random accuracy for a 10-class classifier between \\(\\epsilon=0.25\\) and \\(\\epsilon=0.3\\).\n\nplt.figure(figsize=(5,5))\nplt.plot(epsilons, accuracies, \"*-\")\nplt.yticks(np.arange(0, 1.1, step=0.1))\nplt.xticks(np.arange(0, .35, step=0.05))\nplt.title(\"Accuracy vs Epsilon\")\nplt.xlabel(\"Epsilon\")\nplt.ylabel(\"Accuracy\")\nplt.show()\n\n\n\nSample Adversarial Examples\nRemember the idea of no free lunch? In this case, as epsilon increases the test accuracy decreases BUT the perturbations become more easily perceptible. In reality, there is a tradeoff between accuracy degradation and perceptibility that an attacker must consider. Here, we show some examples of successful adversarial examples at each epsilon value. Each row of the plot shows a different epsilon value. The first row is the \\(\\epsilon=0\\) examples which represent the original “clean” images with no perturbation. The title of each image shows the “original classification -&gt; adversarial classification.” Notice, the perturbations start to become evident at \\(\\epsilon=0.15\\) and are quite evident at \\(\\epsilon=0.3\\). However, in all cases humans are still capable of identifying the correct class despite the added noise.\n\n# Plot several examples of adversarial samples at each epsilon\ncnt = 0\nplt.figure(figsize=(8,10))\nfor i in range(len(epsilons)):\n    for j in range(len(examples[i])):\n        cnt += 1\n        plt.subplot(len(epsilons),len(examples[0]),cnt)\n        plt.xticks([], [])\n        plt.yticks([], [])\n        if j == 0:\n            plt.ylabel(\"Eps: {}\".format(epsilons[i]), fontsize=14)\n        orig,adv,ex = examples[i][j]\n        plt.title(\"{} -&gt; {}\".format(orig, adv))\n        plt.imshow(ex, cmap=\"gray\")\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Pytorch",
      "Vision",
      "Adversarial Example Generation"
    ]
  },
  {
    "objectID": "Pytorch/Vision/fgsm_tutorial.html#where-to-go-next",
    "href": "Pytorch/Vision/fgsm_tutorial.html#where-to-go-next",
    "title": "Adversarial Example Generation",
    "section": "Where to go next?",
    "text": "Where to go next?\nHopefully this tutorial gives some insight into the topic of adversarial machine learning. There are many potential directions to go from here. This attack represents the very beginning of adversarial attack research and since there have been many subsequent ideas for how to attack and defend ML models from an adversary. In fact, at NIPS 2017 there was an adversarial attack and defense competition and many of the methods used in the competition are described in this paper: Adversarial Attacks and Defences Competition_. The work on defense also leads into the idea of making machine learning models more robust in general, to both naturally perturbed and adversarially crafted inputs.\nAnother direction to go is adversarial attacks and defense in different domains. Adversarial research is not limited to the image domain, check out this_ attack on speech-to-text models. But perhaps the best way to learn more about adversarial machine learning is to get your hands dirty. Try to implement a different attack from the NIPS 2017 competition, and see how it differs from FGSM. Then, try to defend the model from your own attacks.\nA few ads maybe displayed for income as resources are now offered freely. 🤝🤝🤝",
    "crumbs": [
      "Pytorch",
      "Vision",
      "Adversarial Example Generation"
    ]
  },
  {
    "objectID": "Pytorch/Vision/vt_tutorial.html",
    "href": "Pytorch/Vision/vt_tutorial.html",
    "title": "Optimizing Vision Transformer Model for Deployment",
    "section": "",
    "text": "Photo by DATAIDEA",
    "crumbs": [
      "Pytorch",
      "Vision",
      "Optimizing Vision Transformer Model for Deployment"
    ]
  },
  {
    "objectID": "Pytorch/Vision/vt_tutorial.html#what-is-deit",
    "href": "Pytorch/Vision/vt_tutorial.html#what-is-deit",
    "title": "Optimizing Vision Transformer Model for Deployment",
    "section": "What is DeiT",
    "text": "What is DeiT\nConvolutional Neural Networks (CNNs) have been the main models for image classification since deep learning took off in 2012, but CNNs typically require hundreds of millions of images for training to achieve the SOTA results. DeiT is a vision transformer model that requires a lot less data and computing resources for training to compete with the leading CNNs in performing image classification, which is made possible by two key components of of DeiT:\n\nData augmentation that simulates training on a much larger dataset;\nNative distillation that allows the transformer network to learn from a CNN’s output.\n\nDeiT shows that Transformers can be successfully applied to computer vision tasks, with limited access to data and resources. For more details on DeiT, see the repo and paper.",
    "crumbs": [
      "Pytorch",
      "Vision",
      "Optimizing Vision Transformer Model for Deployment"
    ]
  },
  {
    "objectID": "Pytorch/Vision/vt_tutorial.html#classifying-images-with-deit",
    "href": "Pytorch/Vision/vt_tutorial.html#classifying-images-with-deit",
    "title": "Optimizing Vision Transformer Model for Deployment",
    "section": "Classifying Images with DeiT",
    "text": "Classifying Images with DeiT\nFollow the README.md at the DeiT repository for detailed information on how to classify images using DeiT, or for a quick test, first install the required packages:\npip install torch torchvision timm pandas requests\nTo run in Google Colab, install dependencies by running the following command:\n!pip install timm pandas requests\nthen run the script below:\n\nfrom PIL import Image\nimport torch\nimport timm\nimport requests\nimport torchvision.transforms as transforms\nfrom timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n\nprint(torch.__version__)\n# should be 1.8.0\n\n\nmodel = torch.hub.load('facebookresearch/deit:main', 'deit_base_patch16_224', pretrained=True)\nmodel.eval()\n\ntransform = transforms.Compose([\n    transforms.Resize(256, interpolation=3),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD),\n])\n\nimg = Image.open(requests.get(\"https://raw.githubusercontent.com/pytorch/ios-demo-app/master/HelloWorld/HelloWorld/HelloWorld/image.png\", stream=True).raw)\nimg = transform(img)[None,]\nout = model(img)\nclsidx = torch.argmax(out)\nprint(clsidx.item())\n\nThe output should be 269, which, according to the ImageNet list of class index to labels file, maps to timber wolf, grey wolf, gray wolf, Canis lupus.\nNow that we have verified that we can use the DeiT model to classify images, let’s see how to modify the model so it can run on iOS and Android apps.",
    "crumbs": [
      "Pytorch",
      "Vision",
      "Optimizing Vision Transformer Model for Deployment"
    ]
  },
  {
    "objectID": "Pytorch/Vision/vt_tutorial.html#scripting-deit",
    "href": "Pytorch/Vision/vt_tutorial.html#scripting-deit",
    "title": "Optimizing Vision Transformer Model for Deployment",
    "section": "Scripting DeiT",
    "text": "Scripting DeiT\nTo use the model on mobile, we first need to script the model. See the Script and Optimize recipe for a quick overview. Run the code below to convert the DeiT model used in the previous step to the TorchScript format that can run on mobile.\n\nmodel = torch.hub.load('facebookresearch/deit:main', 'deit_base_patch16_224', pretrained=True)\nmodel.eval()\nscripted_model = torch.jit.script(model)\nscripted_model.save(\"fbdeit_scripted.pt\")\n\nThe scripted model file fbdeit_scripted.pt of size about 346MB is generated.",
    "crumbs": [
      "Pytorch",
      "Vision",
      "Optimizing Vision Transformer Model for Deployment"
    ]
  },
  {
    "objectID": "Pytorch/Vision/vt_tutorial.html#quantizing-deit",
    "href": "Pytorch/Vision/vt_tutorial.html#quantizing-deit",
    "title": "Optimizing Vision Transformer Model for Deployment",
    "section": "Quantizing DeiT",
    "text": "Quantizing DeiT\nTo reduce the trained model size significantly while keeping the inference accuracy about the same, quantization can be applied to the model. Thanks to the transformer model used in DeiT, we can easily apply dynamic-quantization to the model, because dynamic quantization works best for LSTM and transformer models (see here for more details).\nNow run the code below:\n\n# Use 'x86' for server inference (the old 'fbgemm' is still available but 'x86' is the recommended default) and ``qnnpack`` for mobile inference.\nbackend = \"x86\" # replaced with ``qnnpack`` causing much worse inference speed for quantized model on this notebook\nmodel.qconfig = torch.quantization.get_default_qconfig(backend)\ntorch.backends.quantized.engine = backend\n\nquantized_model = torch.quantization.quantize_dynamic(model, qconfig_spec={torch.nn.Linear}, dtype=torch.qint8)\nscripted_quantized_model = torch.jit.script(quantized_model)\nscripted_quantized_model.save(\"fbdeit_scripted_quantized.pt\")\n\nThis generates the scripted and quantized version of the model fbdeit_quantized_scripted.pt, with size about 89MB, a 74% reduction of the non-quantized model size of 346MB!\nYou can use the scripted_quantized_model to generate the same inference result:\n\nout = scripted_quantized_model(img)\nclsidx = torch.argmax(out)\nprint(clsidx.item())\n# The same output 269 should be printed",
    "crumbs": [
      "Pytorch",
      "Vision",
      "Optimizing Vision Transformer Model for Deployment"
    ]
  },
  {
    "objectID": "Pytorch/Vision/vt_tutorial.html#optimizing-deit",
    "href": "Pytorch/Vision/vt_tutorial.html#optimizing-deit",
    "title": "Optimizing Vision Transformer Model for Deployment",
    "section": "Optimizing DeiT",
    "text": "Optimizing DeiT\nThe final step before using the quantized and scripted model on mobile is to optimize it:\n\nfrom torch.utils.mobile_optimizer import optimize_for_mobile\noptimized_scripted_quantized_model = optimize_for_mobile(scripted_quantized_model)\noptimized_scripted_quantized_model.save(\"fbdeit_optimized_scripted_quantized.pt\")\n\nThe generated fbdeit_optimized_scripted_quantized.pt file has about the same size as the quantized, scripted, but non-optimized model. The inference result remains the same.\n\nout = optimized_scripted_quantized_model(img)\nclsidx = torch.argmax(out)\nprint(clsidx.item())\n# Again, the same output 269 should be printed",
    "crumbs": [
      "Pytorch",
      "Vision",
      "Optimizing Vision Transformer Model for Deployment"
    ]
  },
  {
    "objectID": "Pytorch/Vision/vt_tutorial.html#using-lite-interpreter",
    "href": "Pytorch/Vision/vt_tutorial.html#using-lite-interpreter",
    "title": "Optimizing Vision Transformer Model for Deployment",
    "section": "Using Lite Interpreter",
    "text": "Using Lite Interpreter\nTo see how much model size reduction and inference speed up the Lite Interpreter can result in, let’s create the lite version of the model.\n\noptimized_scripted_quantized_model._save_for_lite_interpreter(\"fbdeit_optimized_scripted_quantized_lite.ptl\")\nptl = torch.jit.load(\"fbdeit_optimized_scripted_quantized_lite.ptl\")\n\nAlthough the lite model size is comparable to the non-lite version, when running the lite version on mobile, the inference speed up is expected.",
    "crumbs": [
      "Pytorch",
      "Vision",
      "Optimizing Vision Transformer Model for Deployment"
    ]
  },
  {
    "objectID": "Pytorch/Vision/vt_tutorial.html#comparing-inference-speed",
    "href": "Pytorch/Vision/vt_tutorial.html#comparing-inference-speed",
    "title": "Optimizing Vision Transformer Model for Deployment",
    "section": "Comparing Inference Speed",
    "text": "Comparing Inference Speed\nTo see how the inference speed differs for the four models - the original model, the scripted model, the quantized-and-scripted model, the optimized-quantized-and-scripted model - run the code below:\n\nwith torch.autograd.profiler.profile(use_cuda=False) as prof1:\n    out = model(img)\nwith torch.autograd.profiler.profile(use_cuda=False) as prof2:\n    out = scripted_model(img)\nwith torch.autograd.profiler.profile(use_cuda=False) as prof3:\n    out = scripted_quantized_model(img)\nwith torch.autograd.profiler.profile(use_cuda=False) as prof4:\n    out = optimized_scripted_quantized_model(img)\nwith torch.autograd.profiler.profile(use_cuda=False) as prof5:\n    out = ptl(img)\n\nprint(\"original model: {:.2f}ms\".format(prof1.self_cpu_time_total/1000))\nprint(\"scripted model: {:.2f}ms\".format(prof2.self_cpu_time_total/1000))\nprint(\"scripted & quantized model: {:.2f}ms\".format(prof3.self_cpu_time_total/1000))\nprint(\"scripted & quantized & optimized model: {:.2f}ms\".format(prof4.self_cpu_time_total/1000))\nprint(\"lite model: {:.2f}ms\".format(prof5.self_cpu_time_total/1000))\n\nThe results running on a Google Colab are:\n::\noriginal model: 1236.69ms scripted model: 1226.72ms scripted & quantized model: 593.19ms scripted & quantized & optimized model: 598.01ms lite model: 600.72ms\nThe following results summarize the inference time taken by each model and the percentage reduction of each model relative to the original model.\n\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'Model': ['original model','scripted model', 'scripted & quantized model', 'scripted & quantized & optimized model', 'lite model']})\ndf = pd.concat([df, pd.DataFrame([\n    [\"{:.2f}ms\".format(prof1.self_cpu_time_total/1000), \"0%\"],\n    [\"{:.2f}ms\".format(prof2.self_cpu_time_total/1000),\n     \"{:.2f}%\".format((prof1.self_cpu_time_total-prof2.self_cpu_time_total)/prof1.self_cpu_time_total*100)],\n    [\"{:.2f}ms\".format(prof3.self_cpu_time_total/1000),\n     \"{:.2f}%\".format((prof1.self_cpu_time_total-prof3.self_cpu_time_total)/prof1.self_cpu_time_total*100)],\n    [\"{:.2f}ms\".format(prof4.self_cpu_time_total/1000),\n     \"{:.2f}%\".format((prof1.self_cpu_time_total-prof4.self_cpu_time_total)/prof1.self_cpu_time_total*100)],\n    [\"{:.2f}ms\".format(prof5.self_cpu_time_total/1000),\n     \"{:.2f}%\".format((prof1.self_cpu_time_total-prof5.self_cpu_time_total)/prof1.self_cpu_time_total*100)]],\n    columns=['Inference Time', 'Reduction'])], axis=1)\n\nprint(df)\n\n\"\"\"\n        Model                             Inference Time    Reduction\n0   original model                             1236.69ms           0%\n1   scripted model                             1226.72ms        0.81%\n2   scripted & quantized model                  593.19ms       52.03%\n3   scripted & quantized & optimized model      598.01ms       51.64%\n4   lite model                                  600.72ms       51.43%\n\"\"\"\n\n\nLearn More\n\nFacebook Data-efficient Image Transformers_\nVision Transformer with ImageNet and MNIST on iOS_\nVision Transformer with ImageNet and MNIST on Android_\n\nA few ads maybe displayed for income as resources are now offered freely. 🤝🤝🤝",
    "crumbs": [
      "Pytorch",
      "Vision",
      "Optimizing Vision Transformer Model for Deployment"
    ]
  },
  {
    "objectID": "Pytorch/Vision/01_torchvision_tutorial.html",
    "href": "Pytorch/Vision/01_torchvision_tutorial.html",
    "title": "TorchVision Object Detection Finetuning Tutorial",
    "section": "",
    "text": "Photo by DATAIDEA\n# For tips on running notebooks in Google Colab, see\n# https://pytorch.org/tutorials/beginner/colab",
    "crumbs": [
      "Pytorch",
      "Vision",
      "TorchVision Object Detection Finetuning Tutorial"
    ]
  },
  {
    "objectID": "Pytorch/Vision/01_torchvision_tutorial.html#writing-a-custom-dataset-for-pennfudan",
    "href": "Pytorch/Vision/01_torchvision_tutorial.html#writing-a-custom-dataset-for-pennfudan",
    "title": "TorchVision Object Detection Finetuning Tutorial",
    "section": "Writing a custom dataset for PennFudan",
    "text": "Writing a custom dataset for PennFudan\nLet’s write a dataset for the PennFudan dataset. First, let's download the dataset and extract the zip file:\nwget https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip -P data\ncd data && unzip PennFudanPed.zip\nWe have the following folder structure:\nPennFudanPed/\n  PedMasks/\n    FudanPed00001_mask.png\n    FudanPed00002_mask.png\n    FudanPed00003_mask.png\n    FudanPed00004_mask.png\n    ...\n  PNGImages/\n    FudanPed00001.png\n    FudanPed00002.png\n    FudanPed00003.png\n    FudanPed00004.png\nHere is one example of a pair of images and segmentation masks\n\nimport matplotlib.pyplot as plt\nfrom torchvision.io import read_image\n\n\nimage = read_image(\"data/PennFudanPed/PNGImages/FudanPed00046.png\")\nmask = read_image(\"data/PennFudanPed/PedMasks/FudanPed00046_mask.png\")\n\nplt.figure(figsize=(16, 8))\nplt.subplot(121)\nplt.title(\"Image\")\nplt.imshow(image.permute(1, 2, 0))\nplt.subplot(122)\nplt.title(\"Mask\")\nplt.imshow(mask.permute(1, 2, 0))\n\nSo each image has a corresponding segmentation mask, where each color correspond to a different instance. Let’s write a torch.utils.data.Dataset class for this dataset. In the code below, we are wrapping images, bounding boxes and masks into torchvision.tv_tensors.TVTensor classes so that we will be able to apply torchvision built-in transformations (new Transforms API) for the given object detection and segmentation task. Namely, image tensors will be wrapped by torchvision.tv_tensors.Image, bounding boxes into torchvision.tv_tensors.BoundingBoxes and masks into torchvision.tv_tensors.Mask. As torchvision.tv_tensors.TVTensor are torch.Tensor subclasses, wrapped objects are also tensors and inherit the plain torch.Tensor API. For more information about torchvision tv_tensors see this documentation.\n\nimport os\nimport torch\n\nfrom torchvision.io import read_image\nfrom torchvision.ops.boxes import masks_to_boxes\nfrom torchvision import tv_tensors\nfrom torchvision.transforms.v2 import functional as F\n\n\nclass PennFudanDataset(torch.utils.data.Dataset):\n    def __init__(self, root, transforms):\n        self.root = root\n        self.transforms = transforms\n        # load all image files, sorting them to\n        # ensure that they are aligned\n        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n\n    def __getitem__(self, idx):\n        # load images and masks\n        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n        img = read_image(img_path)\n        mask = read_image(mask_path)\n        # instances are encoded as different colors\n        obj_ids = torch.unique(mask)\n        # first id is the background, so remove it\n        obj_ids = obj_ids[1:]\n        num_objs = len(obj_ids)\n\n        # split the color-encoded mask into a set\n        # of binary masks\n        masks = (mask == obj_ids[:, None, None]).to(dtype=torch.uint8)\n\n        # get bounding box coordinates for each mask\n        boxes = masks_to_boxes(masks)\n\n        # there is only one class\n        labels = torch.ones((num_objs,), dtype=torch.int64)\n\n        image_id = idx\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        # suppose all instances are not crowd\n        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n\n        # Wrap sample and targets into torchvision tv_tensors:\n        img = tv_tensors.Image(img)\n\n        target = {}\n        target[\"boxes\"] = tv_tensors.BoundingBoxes(boxes, format=\"XYXY\", canvas_size=F.get_size(img))\n        target[\"masks\"] = tv_tensors.Mask(masks)\n        target[\"labels\"] = labels\n        target[\"image_id\"] = image_id\n        target[\"area\"] = area\n        target[\"iscrowd\"] = iscrowd\n\n        if self.transforms is not None:\n            img, target = self.transforms(img, target)\n\n        return img, target\n\n    def __len__(self):\n        return len(self.imgs)\n\nThat’s all for the dataset. Now let’s define a model that can perform predictions on this dataset.",
    "crumbs": [
      "Pytorch",
      "Vision",
      "TorchVision Object Detection Finetuning Tutorial"
    ]
  },
  {
    "objectID": "Pytorch/Vision/01_torchvision_tutorial.html#finetuning-from-a-pretrained-model",
    "href": "Pytorch/Vision/01_torchvision_tutorial.html#finetuning-from-a-pretrained-model",
    "title": "TorchVision Object Detection Finetuning Tutorial",
    "section": "1 - Finetuning from a pretrained model",
    "text": "1 - Finetuning from a pretrained model\nLet’s suppose that you want to start from a model pre-trained on COCO and want to finetune it for your particular classes. Here is a possible way of doing it:\n\nimport torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n\n# load a model pre-trained on COCO\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n\n# replace the classifier with a new one, that has\n# num_classes which is user-defined\nnum_classes = 2  # 1 class (person) + background\n# get number of input features for the classifier\nin_features = model.roi_heads.box_predictor.cls_score.in_features\n# replace the pre-trained head with a new one\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)",
    "crumbs": [
      "Pytorch",
      "Vision",
      "TorchVision Object Detection Finetuning Tutorial"
    ]
  },
  {
    "objectID": "Pytorch/Vision/dcgan_faces_tutorial.html",
    "href": "Pytorch/Vision/dcgan_faces_tutorial.html",
    "title": "DCGAN Tutorial",
    "section": "",
    "text": "Photo by DATAIDEA",
    "crumbs": [
      "Pytorch",
      "Vision",
      "DCGAN Tutorial"
    ]
  },
  {
    "objectID": "Pytorch/Vision/dcgan_faces_tutorial.html#introduction",
    "href": "Pytorch/Vision/dcgan_faces_tutorial.html#introduction",
    "title": "DCGAN Tutorial",
    "section": "Introduction",
    "text": "Introduction\nThis tutorial will give an introduction to DCGANs through an example. We will train a generative adversarial network (GAN) to generate new celebrities after showing it pictures of many real celebrities. Most of the code here is from the DCGAN implementation in pytorch/examples_, and this document will give a thorough explanation of the implementation and shed light on how and why this model works. But don’t worry, no prior knowledge of GANs is required, but it may require a first-timer to spend some time reasoning about what is actually happening under the hood. Also, for the sake of time it will help to have a GPU, or two. Lets start from the beginning.",
    "crumbs": [
      "Pytorch",
      "Vision",
      "DCGAN Tutorial"
    ]
  },
  {
    "objectID": "Pytorch/Vision/dcgan_faces_tutorial.html#generative-adversarial-networks",
    "href": "Pytorch/Vision/dcgan_faces_tutorial.html#generative-adversarial-networks",
    "title": "DCGAN Tutorial",
    "section": "Generative Adversarial Networks",
    "text": "Generative Adversarial Networks\n\nWhat is a GAN?\nGANs are a framework for teaching a deep learning model to capture the training data distribution so we can generate new data from that same distribution. GANs were invented by Ian Goodfellow in 2014 and first described in the paper Generative Adversarial Nets_. They are made of two distinct models, a generator and a discriminator. The job of the generator is to spawn ‘fake’ images that look like the training images. The job of the discriminator is to look at an image and output whether or not it is a real training image or a fake image from the generator. During training, the generator is constantly trying to outsmart the discriminator by generating better and better fakes, while the discriminator is working to become a better detective and correctly classify the real and fake images. The equilibrium of this game is when the generator is generating perfect fakes that look as if they came directly from the training data, and the discriminator is left to always guess at 50% confidence that the generator output is real or fake.\nNow, lets define some notation to be used throughout tutorial starting with the discriminator. Let \\(x\\) be data representing an image. \\(D(x)\\) is the discriminator network which outputs the (scalar) probability that \\(x\\) came from training data rather than the generator. Here, since we are dealing with images, the input to \\(D(x)\\) is an image of CHW size 3x64x64. Intuitively, \\(D(x)\\) should be HIGH when \\(x\\) comes from training data and LOW when \\(x\\) comes from the generator. \\(D(x)\\) can also be thought of as a traditional binary classifier.\nFor the generator’s notation, let \\(z\\) be a latent space vector sampled from a standard normal distribution. \\(G(z)\\) represents the generator function which maps the latent vector \\(z\\) to data-space. The goal of \\(G\\) is to estimate the distribution that the training data comes from (\\(p_{data}\\)) so it can generate fake samples from that estimated distribution (\\(p_g\\)).\nSo, \\(D(G(z))\\) is the probability (scalar) that the output of the generator \\(G\\) is a real image. As described in Goodfellow’s paper_, \\(D\\) and \\(G\\) play a minimax game in which \\(D\\) tries to maximize the probability it correctly classifies reals and fakes (\\(logD(x)\\)), and \\(G\\) tries to minimize the probability that \\(D\\) will predict its outputs are fake (\\(log(1-D(G(z)))\\)). From the paper, the GAN loss function is\n\\[\\begin{align}\\underset{G}{\\text{min}} \\underset{D}{\\text{max}}V(D,G) = \\mathbb{E}_{x\\sim p_{data}(x)}\\big[logD(x)\\big] + \\mathbb{E}_{z\\sim p_{z}(z)}\\big[log(1-D(G(z)))\\big]\\end{align}\\]\nIn theory, the solution to this minimax game is where \\(p_g = p_{data}\\), and the discriminator guesses randomly if the inputs are real or fake. However, the convergence theory of GANs is still being actively researched and in reality models do not always train to this point.\n\n\nWhat is a DCGAN?\nA DCGAN is a direct extension of the GAN described above, except that it explicitly uses convolutional and convolutional-transpose layers in the discriminator and generator, respectively. It was first described by Radford et. al. in the paper Unsupervised Representation Learning With Deep Convolutional Generative Adversarial Networks. The discriminator is made up of strided convolution layers, batch norm layers, and LeakyReLU activations. The input is a 3x64x64 input image and the output is a scalar probability that the input is from the real data distribution. The generator is comprised of convolutional-transpose layers, batch norm layers, and ReLU activations. The input is a latent vector, \\(z\\), that is drawn from a standard normal distribution and the output is a 3x64x64 RGB image. The strided conv-transpose layers allow the latent vector to be transformed into a volume with the same shape as an image. In the paper, the authors also give some tips about how to setup the optimizers, how to calculate the loss functions, and how to initialize the model weights, all of which will be explained in the coming sections.\n\nfrom __future__ import print_function\n#%matplotlib inline\nimport argparse\nimport os\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim as optim\nimport torch.utils.data\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom IPython.display import HTML\n\n# Set random seed for reproducibility\nmanualSeed = 999\n#manualSeed = random.randint(1, 10000) # use if you want new results\nprint(\"Random Seed: \", manualSeed)\nrandom.seed(manualSeed)\ntorch.manual_seed(manualSeed)\n\nRandom Seed:  999\n\n\n&lt;torch._C.Generator at 0x7a2bcd9a9310&gt;",
    "crumbs": [
      "Pytorch",
      "Vision",
      "DCGAN Tutorial"
    ]
  },
  {
    "objectID": "Pytorch/Vision/dcgan_faces_tutorial.html#inputs",
    "href": "Pytorch/Vision/dcgan_faces_tutorial.html#inputs",
    "title": "DCGAN Tutorial",
    "section": "Inputs",
    "text": "Inputs\nLet’s define some inputs for the run:\n\ndataroot - the path to the root of the dataset folder. We will talk more about the dataset in the next section.\nworkers - the number of worker threads for loading the data with the DataLoader.\nbatch_size - the batch size used in training. The DCGAN paper uses a batch size of 128.\nimage_size - the spatial size of the images used for training. This implementation defaults to 64x64. If another size is desired, the structures of D and G must be changed. See here_ for more details.\nnc - number of color channels in the input images. For color images this is 3.\nnz - length of latent vector.\nngf - relates to the depth of feature maps carried through the generator.\nndf - sets the depth of feature maps propagated through the discriminator.\nnum_epochs - number of training epochs to run. Training for longer will probably lead to better results but will also take much longer.\nlr - learning rate for training. As described in the DCGAN paper, this number should be 0.0002.\nbeta1 - beta1 hyperparameter for Adam optimizers. As described in paper, this number should be 0.5.\nngpu - number of GPUs available. If this is 0, code will run in CPU mode. If this number is greater than 0 it will run on that number of GPUs.\n\n\n# Root directory for dataset\ndataroot = \"data/celeba\"\n\n# Number of workers for dataloader\nworkers = 2\n\n# Batch size during training\nbatch_size = 128\n\n# Spatial size of training images. All images will be resized to this\n#   size using a transformer.\nimage_size = 64\n\n# Number of channels in the training images. For color images this is 3\nnc = 3\n\n# Size of z latent vector (i.e. size of generator input)\nnz = 100\n\n# Size of feature maps in generator\nngf = 64\n\n# Size of feature maps in discriminator\nndf = 64\n\n# Number of training epochs\nnum_epochs = 5\n\n# Learning rate for optimizers\nlr = 0.0002\n\n# Beta1 hyperparameter for Adam optimizers\nbeta1 = 0.5\n\n# Number of GPUs available. Use 0 for CPU mode.\nngpu = 0",
    "crumbs": [
      "Pytorch",
      "Vision",
      "DCGAN Tutorial"
    ]
  },
  {
    "objectID": "Pytorch/Vision/dcgan_faces_tutorial.html#data",
    "href": "Pytorch/Vision/dcgan_faces_tutorial.html#data",
    "title": "DCGAN Tutorial",
    "section": "Data",
    "text": "Data\nIn this tutorial we will use the Celeb-A Faces dataset_ which can be downloaded at the linked site, or in Google Drive_. The dataset will download as a file named img_align_celeba.zip. Once downloaded, create a directory named celeba and extract the zip file into that directory. Then, set the dataroot input for this notebook to the celeba directory you just created. The resulting directory structure should be:\n::\n/path/to/celeba -&gt; img_align_celeba\n-&gt; 188242.jpg -&gt; 173822.jpg -&gt; 284702.jpg -&gt; 537394.jpg …\nThis is an important step because we will be using the ImageFolder dataset class, which requires there to be subdirectories in the dataset root folder. Now, we can create the dataset, create the dataloader, set the device to run on, and finally visualize some of the training data.\n\n# from google.colab import drive\n# drive.mount('/content/drive')\n\nMounted at /content/drive\n\n\n\n# dataroot = '/content/drive/MyDrive/Data sets/celeba/img_align_celeba/'\n\n\ndataroot = '/data/celeba/img_align_celeba/'\n\n\n# We can use an image folder dataset the way we have it setup.\n# Create the dataset\ndataset = dset.ImageFolder(root=dataroot,\n                           transform=transforms.Compose([\n                               transforms.Resize(image_size),\n                               transforms.CenterCrop(image_size),\n                               transforms.ToTensor(),\n                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n                           ]))\n# Create the dataloader\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n                                         shuffle=True, num_workers=workers)\n\n# Decide which device we want to run on\ndevice = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu &gt; 0) else \"cpu\")\n\n# Plot some training images\nreal_batch = next(iter(dataloader))\nplt.figure(figsize=(8,8))\nplt.axis(\"off\")\nplt.title(\"Training Images\")\nplt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))",
    "crumbs": [
      "Pytorch",
      "Vision",
      "DCGAN Tutorial"
    ]
  },
  {
    "objectID": "Pytorch/Vision/dcgan_faces_tutorial.html#implementation",
    "href": "Pytorch/Vision/dcgan_faces_tutorial.html#implementation",
    "title": "DCGAN Tutorial",
    "section": "Implementation",
    "text": "Implementation\nWith our input parameters set and the dataset prepared, we can now get into the implementation. We will start with the weight initialization strategy, then talk about the generator, discriminator, loss functions, and training loop in detail.\n\nWeight Initialization\nFrom the DCGAN paper, the authors specify that all model weights shall be randomly initialized from a Normal distribution with mean=0, stdev=0.02. The weights_init function takes an initialized model as input and reinitializes all convolutional, convolutional-transpose, and batch normalization layers to meet this criteria. This function is applied to the models immediately after initialization.\n\n# custom weights initialization called on ``netG`` and ``netD``\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)\n\n\n\nGenerator\nThe generator, \\(G\\), is designed to map the latent space vector (\\(z\\)) to data-space. Since our data are images, converting \\(z\\) to data-space means ultimately creating a RGB image with the same size as the training images (i.e. 3x64x64). In practice, this is accomplished through a series of strided two dimensional convolutional transpose layers, each paired with a 2d batch norm layer and a relu activation. The output of the generator is fed through a tanh function to return it to the input data range of \\([-1,1]\\). It is worth noting the existence of the batch norm functions after the conv-transpose layers, as this is a critical contribution of the DCGAN paper. These layers help with the flow of gradients during training. An image of the generator from the DCGAN paper is shown below.\n.. figure:: /_static/img/dcgan_generator.png :alt: dcgan_generator\nNotice, how the inputs we set in the input section (nz, ngf, and nc) influence the generator architecture in code. nz is the length of the z input vector, ngf relates to the size of the feature maps that are propagated through the generator, and nc is the number of channels in the output image (set to 3 for RGB images). Below is the code for the generator.\n\n# Generator Code\n\nclass Generator(nn.Module):\n    def __init__(self, ngpu):\n        super(Generator, self).__init__()\n        self.ngpu = ngpu\n        self.main = nn.Sequential(\n            # input is Z, going into a convolution\n            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(ngf * 8),\n            nn.ReLU(True),\n            # state size. ``(ngf*8) x 4 x 4``\n            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(True),\n            # state size. ``(ngf*4) x 8 x 8``\n            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 2),\n            nn.ReLU(True),\n            # state size. ``(ngf*2) x 16 x 16``\n            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.ReLU(True),\n            # state size. ``(ngf) x 32 x 32``\n            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n            nn.Tanh()\n            # state size. ``(nc) x 64 x 64``\n        )\n\n    def forward(self, input):\n        return self.main(input)\n\nNow, we can instantiate the generator and apply the weights_init function. Check out the printed model to see how the generator object is structured.\n\n# Create the generator\nnetG = Generator(ngpu).to(device)\n\n# Handle multi-GPU if desired\nif (device.type == 'cuda') and (ngpu &gt; 1):\n    netG = nn.DataParallel(netG, list(range(ngpu)))\n\n# Apply the ``weights_init`` function to randomly initialize all weights\n#  to ``mean=0``, ``stdev=0.02``.\nnetG.apply(weights_init)\n\n# Print the model\nprint(netG)\n\nGenerator(\n  (main): Sequential(\n    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (5): ReLU(inplace=True)\n    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (8): ReLU(inplace=True)\n    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (11): ReLU(inplace=True)\n    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (13): Tanh()\n  )\n)\n\n\n\n\nDiscriminator\nAs mentioned, the discriminator, \\(D\\), is a binary classification network that takes an image as input and outputs a scalar probability that the input image is real (as opposed to fake). Here, \\(D\\) takes a 3x64x64 input image, processes it through a series of Conv2d, BatchNorm2d, and LeakyReLU layers, and outputs the final probability through a Sigmoid activation function. This architecture can be extended with more layers if necessary for the problem, but there is significance to the use of the strided convolution, BatchNorm, and LeakyReLUs. The DCGAN paper mentions it is a good practice to use strided convolution rather than pooling to downsample because it lets the network learn its own pooling function. Also batch norm and leaky relu functions promote healthy gradient flow which is critical for the learning process of both \\(G\\) and \\(D\\).\nDiscriminator Code\n\nclass Discriminator(nn.Module):\n    def __init__(self, ngpu):\n        super(Discriminator, self).__init__()\n        self.ngpu = ngpu\n        self.main = nn.Sequential(\n            # input is ``(nc) x 64 x 64``\n            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. ``(ndf) x 32 x 32``\n            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. ``(ndf*2) x 16 x 16``\n            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. ``(ndf*4) x 8 x 8``\n            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 8),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. ``(ndf*8) x 4 x 4``\n            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, input):\n        return self.main(input)\n\nNow, as with the generator, we can create the discriminator, apply the weights_init function, and print the model’s structure.\n\n# Create the Discriminator\nnetD = Discriminator(ngpu).to(device)\n\n# Handle multi-GPU if desired\nif (device.type == 'cuda') and (ngpu &gt; 1):\n    netD = nn.DataParallel(netD, list(range(ngpu)))\n    \n# Apply the ``weights_init`` function to randomly initialize all weights\n# like this: ``to mean=0, stdev=0.2``.\nnetD.apply(weights_init)\n\n# Print the model\nprint(netD)\n\nDiscriminator(\n  (main): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n    (12): Sigmoid()\n  )\n)\n\n\n\n\nLoss Functions and Optimizers\nWith \\(D\\) and \\(G\\) setup, we can specify how they learn through the loss functions and optimizers. We will use the Binary Cross Entropy loss (BCELoss_) function which is defined in PyTorch as:\n\\[\\begin{align}\\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad l_n = - \\left[ y_n \\cdot \\log x_n + (1 - y_n) \\cdot \\log (1 - x_n) \\right]\\end{align}\\]\nNotice how this function provides the calculation of both log components in the objective function (i.e. \\(log(D(x))\\) and \\(log(1-D(G(z)))\\)). We can specify what part of the BCE equation to use with the \\(y\\) input. This is accomplished in the training loop which is coming up soon, but it is important to understand how we can choose which component we wish to calculate just by changing \\(y\\) (i.e. GT labels).\nNext, we define our real label as 1 and the fake label as 0. These labels will be used when calculating the losses of \\(D\\) and \\(G\\), and this is also the convention used in the original GAN paper. Finally, we set up two separate optimizers, one for \\(D\\) and one for \\(G\\). As specified in the DCGAN paper, both are Adam optimizers with learning rate 0.0002 and Beta1 = 0.5. For keeping track of the generator’s learning progression, we will generate a fixed batch of latent vectors that are drawn from a Gaussian distribution (i.e. fixed_noise) . In the training loop, we will periodically input this fixed_noise into \\(G\\), and over the iterations we will see images form out of the noise.\n\n# Initialize the ``BCELoss`` function\ncriterion = nn.BCELoss()\n\n# Create batch of latent vectors that we will use to visualize\n#  the progression of the generator\nfixed_noise = torch.randn(64, nz, 1, 1, device=device)\n\n# Establish convention for real and fake labels during training\nreal_label = 1.\nfake_label = 0.\n\n# Setup Adam optimizers for both G and D\noptimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\noptimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n\n\n\nTraining\nFinally, now that we have all of the parts of the GAN framework defined, we can train it. Be mindful that training GANs is somewhat of an art form, as incorrect hyperparameter settings lead to mode collapse with little explanation of what went wrong. Here, we will closely follow Algorithm 1 from the Goodfellow’s paper, while abiding by some of the best practices shown in ganhacks. Namely, we will “construct different mini-batches for real and fake” images, and also adjust G’s objective function to maximize \\(log(D(G(z)))\\). Training is split up into two main parts. Part 1 updates the Discriminator and Part 2 updates the Generator.\nPart 1 - Train the Discriminator\nRecall, the goal of training the discriminator is to maximize the probability of correctly classifying a given input as real or fake. In terms of Goodfellow, we wish to “update the discriminator by ascending its stochastic gradient”. Practically, we want to maximize \\(log(D(x)) + log(1-D(G(z)))\\). Due to the separate mini-batch suggestion from ganhacks_, we will calculate this in two steps. First, we will construct a batch of real samples from the training set, forward pass through \\(D\\), calculate the loss (\\(log(D(x))\\)), then calculate the gradients in a backward pass. Secondly, we will construct a batch of fake samples with the current generator, forward pass this batch through \\(D\\), calculate the loss (\\(log(1-D(G(z)))\\)), and accumulate the gradients with a backward pass. Now, with the gradients accumulated from both the all-real and all-fake batches, we call a step of the Discriminator’s optimizer.\nPart 2 - Train the Generator\nAs stated in the original paper, we want to train the Generator by minimizing \\(log(1-D(G(z)))\\) in an effort to generate better fakes. As mentioned, this was shown by Goodfellow to not provide sufficient gradients, especially early in the learning process. As a fix, we instead wish to maximize \\(log(D(G(z)))\\). In the code we accomplish this by: classifying the Generator output from Part 1 with the Discriminator, computing G’s loss using real labels as GT, computing G’s gradients in a backward pass, and finally updating G’s parameters with an optimizer step. It may seem counter-intuitive to use the real labels as GT labels for the loss function, but this allows us to use the \\(log(x)\\) part of the BCELoss (rather than the \\(log(1-x)\\) part) which is exactly what we want.\nFinally, we will do some statistic reporting and at the end of each epoch we will push our fixed_noise batch through the generator to visually track the progress of G’s training. The training statistics reported are:\n\nLoss_D - discriminator loss calculated as the sum of losses for the all real and all fake batches (\\(log(D(x)) + log(1 - D(G(z)))\\)).\nLoss_G - generator loss calculated as \\(log(D(G(z)))\\)\nD(x) - the average output (across the batch) of the discriminator for the all real batch. This should start close to 1 then theoretically converge to 0.5 when G gets better. Think about why this is.\nD(G(z)) - average discriminator outputs for the all fake batch. The first number is before D is updated and the second number is after D is updated. These numbers should start near 0 and converge to 0.5 as G gets better. Think about why this is.\n\nNote: This step might take a while, depending on how many epochs you run and if you removed some data from the dataset.\n\n# Training Loop\n\n# Lists to keep track of progress\nimg_list = []\nG_losses = []\nD_losses = []\niters = 0\n\nprint(\"Starting Training Loop...\")\n# For each epoch\nfor epoch in range(num_epochs):\n    # For each batch in the dataloader\n    for i, data in enumerate(dataloader, 0):\n        \n        ############################\n        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n        ###########################\n        ## Train with all-real batch\n        netD.zero_grad()\n        # Format batch\n        real_cpu = data[0].to(device)\n        b_size = real_cpu.size(0)\n        label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n        # Forward pass real batch through D\n        output = netD(real_cpu).view(-1)\n        # Calculate loss on all-real batch\n        errD_real = criterion(output, label)\n        # Calculate gradients for D in backward pass\n        errD_real.backward()\n        D_x = output.mean().item()\n\n        ## Train with all-fake batch\n        # Generate batch of latent vectors\n        noise = torch.randn(b_size, nz, 1, 1, device=device)\n        # Generate fake image batch with G\n        fake = netG(noise)\n        label.fill_(fake_label)\n        # Classify all fake batch with D\n        output = netD(fake.detach()).view(-1)\n        # Calculate D's loss on the all-fake batch\n        errD_fake = criterion(output, label)\n        # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n        errD_fake.backward()\n        D_G_z1 = output.mean().item()\n        # Compute error of D as sum over the fake and the real batches\n        errD = errD_real + errD_fake\n        # Update D\n        optimizerD.step()\n\n        ############################\n        # (2) Update G network: maximize log(D(G(z)))\n        ###########################\n        netG.zero_grad()\n        label.fill_(real_label)  # fake labels are real for generator cost\n        # Since we just updated D, perform another forward pass of all-fake batch through D\n        output = netD(fake).view(-1)\n        # Calculate G's loss based on this output\n        errG = criterion(output, label)\n        # Calculate gradients for G\n        errG.backward()\n        D_G_z2 = output.mean().item()\n        # Update G\n        optimizerG.step()\n        \n        # Output training stats\n        if i % 50 == 0:\n            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n\n                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n        \n        # Save Losses for plotting later\n        G_losses.append(errG.item())\n        D_losses.append(errD.item())\n        \n        # Check how the generator is doing by saving G's output on fixed_noise\n        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n            with torch.no_grad():\n                fake = netG(fixed_noise).detach().cpu()\n            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n            \n        iters += 1",
    "crumbs": [
      "Pytorch",
      "Vision",
      "DCGAN Tutorial"
    ]
  },
  {
    "objectID": "Pytorch/Vision/dcgan_faces_tutorial.html#results",
    "href": "Pytorch/Vision/dcgan_faces_tutorial.html#results",
    "title": "DCGAN Tutorial",
    "section": "Results",
    "text": "Results\nFinally, lets check out how we did. Here, we will look at three different results. First, we will see how D and G’s losses changed during training. Second, we will visualize G’s output on the fixed_noise batch for every epoch. And third, we will look at a batch of real data next to a batch of fake data from G.\nLoss versus training iteration\nBelow is a plot of D & G’s losses versus training iterations.\n\nplt.figure(figsize=(10,5))\nplt.title(\"Generator and Discriminator Loss During Training\")\nplt.plot(G_losses,label=\"G\")\nplt.plot(D_losses,label=\"D\")\nplt.xlabel(\"iterations\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nVisualization of G’s progression\nRemember how we saved the generator’s output on the fixed_noise batch after every epoch of training. Now, we can visualize the training progression of G with an animation. Press the play button to start the animation.\n\nfig = plt.figure(figsize=(8,8))\nplt.axis(\"off\")\nims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]\nani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n\nHTML(ani.to_jshtml())\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\nReal Images vs. Fake Images\nFinally, lets take a look at some real images and fake images side by side.\n\n# Grab a batch of real images from the dataloader\nreal_batch = next(iter(dataloader))\n\n# Plot the real images\nplt.figure(figsize=(15,15))\nplt.subplot(1,2,1)\nplt.axis(\"off\")\nplt.title(\"Real Images\")\nplt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n\n# Plot the fake images from the last epoch\nplt.subplot(1,2,2)\nplt.axis(\"off\")\nplt.title(\"Fake Images\")\nplt.imshow(np.transpose(img_list[-1],(1,2,0)))\nplt.show()",
    "crumbs": [
      "Pytorch",
      "Vision",
      "DCGAN Tutorial"
    ]
  },
  {
    "objectID": "Pytorch/Vision/dcgan_faces_tutorial.html#where-to-go-next",
    "href": "Pytorch/Vision/dcgan_faces_tutorial.html#where-to-go-next",
    "title": "DCGAN Tutorial",
    "section": "Where to Go Next",
    "text": "Where to Go Next\nWe have reached the end of our journey, but there are several places you could go from here. You could:\n\nTrain for longer to see how good the results get\nModify this model to take a different dataset and possibly change the size of the images and the model architecture\nCheck out some other cool GAN projects here_\nCreate GANs that generate music_",
    "crumbs": [
      "Pytorch",
      "Vision",
      "DCGAN Tutorial"
    ]
  },
  {
    "objectID": "Pytorch/Vision/transfer_learning_tutorial.html",
    "href": "Pytorch/Vision/transfer_learning_tutorial.html",
    "title": "Transfer Learning for Computer Vision Tutorial",
    "section": "",
    "text": "Photo by DATAIDEA\n# License: BSD\n# Author: Sasank Chilamkurthy\n\nfrom __future__ import print_function, division\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nimport torch.backends.cudnn as cudnn\nimport numpy as np\nimport torchvision\nfrom torchvision import datasets, models, transforms\nimport matplotlib.pyplot as plt\nimport time\nimport os\nimport copy\n\ncudnn.benchmark = True\nplt.ion()   # interactive mode\n\n&lt;contextlib.ExitStack at 0x7f121e037eb0&gt;",
    "crumbs": [
      "Pytorch",
      "Vision",
      "Transfer Learning for Computer Vision Tutorial"
    ]
  },
  {
    "objectID": "Pytorch/Vision/transfer_learning_tutorial.html#load-data",
    "href": "Pytorch/Vision/transfer_learning_tutorial.html#load-data",
    "title": "Transfer Learning for Computer Vision Tutorial",
    "section": "Load Data",
    "text": "Load Data\nWe will use torchvision and torch.utils.data packages for loading the data.\nThe problem we’re going to solve today is to train a model to classify ants and bees. We have about 120 training images each for ants and bees. There are 75 validation images for each class. Usually, this is a very small dataset to generalize upon, if trained from scratch. Since we are using transfer learning, we should be able to generalize reasonably well.\nThis dataset is a very small subset of imagenet.\n.. Note :: Download the data from here and extract it to the current directory.\n\n# Data augmentation and normalization for training\n# Just normalization for validation\ndata_transforms = {\n    'train': transforms.Compose([\n        transforms.RandomResizedCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'val': transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}\n\ndata_dir = '/content/drive/MyDrive/Data sets/hymenoptera_data'\nimage_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n                                          data_transforms[x])\n                  for x in ['train', 'val']}\ndataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n                                             shuffle=True, num_workers=4)\n              for x in ['train', 'val']}\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\nclass_names = image_datasets['train'].classes\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n\n\n\nVisualize a few images\nLet’s visualize a few training images so as to understand the data augmentations.\n\ndef imshow(inp, title=None):\n    \"\"\"Display image for Tensor.\"\"\"\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    plt.imshow(inp)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001)  # pause a bit so that plots are updated\n\n\n# Get a batch of training data\ninputs, classes = next(iter(dataloaders['train']))\n\n# Make a grid from batch\nout = torchvision.utils.make_grid(inputs)\n\nimshow(out, title=[class_names[x] for x in classes])",
    "crumbs": [
      "Pytorch",
      "Vision",
      "Transfer Learning for Computer Vision Tutorial"
    ]
  },
  {
    "objectID": "Pytorch/Vision/transfer_learning_tutorial.html#training-the-model",
    "href": "Pytorch/Vision/transfer_learning_tutorial.html#training-the-model",
    "title": "Transfer Learning for Computer Vision Tutorial",
    "section": "Training the model",
    "text": "Training the model\nNow, let’s write a general function to train a model. Here, we will illustrate:\n\nScheduling the learning rate\nSaving the best model\n\nIn the following, parameter scheduler is an LR scheduler object from torch.optim.lr_scheduler.\n\ndef train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n    since = time.time()\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print(f'Epoch {epoch}/{num_epochs - 1}')\n        print('-' * 10)\n\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()  # Set model to training mode\n            else:\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0\n\n            # Iterate over data.\n            for inputs, labels in dataloaders[phase]:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                # track history if only in train\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n\n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n\n                # statistics\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n            if phase == 'train':\n                scheduler.step()\n\n            epoch_loss = running_loss / dataset_sizes[phase]\n            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n\n            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n\n            # deep copy the model\n            if phase == 'val' and epoch_acc &gt; best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n\n        print()\n\n    time_elapsed = time.time() - since\n    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n    print(f'Best val Acc: {best_acc:4f}')\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model\n\n\nVisualizing the model predictions\nGeneric function to display predictions for a few images\n\ndef visualize_model(model, num_images=6):\n    was_training = model.training\n    model.eval()\n    images_so_far = 0\n    fig = plt.figure()\n\n    with torch.no_grad():\n        for i, (inputs, labels) in enumerate(dataloaders['val']):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n\n            for j in range(inputs.size()[0]):\n                images_so_far += 1\n                ax = plt.subplot(num_images//2, 2, images_so_far)\n                ax.axis('off')\n                ax.set_title(f'predicted: {class_names[preds[j]]}')\n                imshow(inputs.cpu().data[j])\n\n                if images_so_far == num_images:\n                    model.train(mode=was_training)\n                    return\n        model.train(mode=was_training)",
    "crumbs": [
      "Pytorch",
      "Vision",
      "Transfer Learning for Computer Vision Tutorial"
    ]
  },
  {
    "objectID": "Pytorch/Vision/transfer_learning_tutorial.html#finetuning-the-convnet",
    "href": "Pytorch/Vision/transfer_learning_tutorial.html#finetuning-the-convnet",
    "title": "Transfer Learning for Computer Vision Tutorial",
    "section": "Finetuning the ConvNet",
    "text": "Finetuning the ConvNet\nLoad a pretrained model and reset final fully connected layer.\n\nmodel_ft = models.resnet18(weights='IMAGENET1K_V1')\nnum_ftrs = model_ft.fc.in_features\n# Here the size of each output sample is set to 2.\n# Alternatively, it can be generalized to ``nn.Linear(num_ftrs, len(class_names))``.\nmodel_ft.fc = nn.Linear(num_ftrs, 2)\n\nmodel_ft = model_ft.to(device)\n\ncriterion = nn.CrossEntropyLoss()\n\n# Observe that all parameters are being optimized\noptimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n\n# Decay LR by a factor of 0.1 every 7 epochs\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|██████████| 44.7M/44.7M [00:00&lt;00:00, 99.5MB/s]\n\n\n\nTrain and evaluate\nIt should take around 15-25 min on CPU. On GPU though, it takes less than a minute.\n\nmodel_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n                       num_epochs=25)\n\nEpoch 0/24\n----------\ntrain Loss: 0.8007 Acc: 0.6598\nval Loss: 0.6836 Acc: 0.7255\n\nEpoch 1/24\n----------\ntrain Loss: 0.5015 Acc: 0.8074\nval Loss: 0.2226 Acc: 0.9150\n\nEpoch 2/24\n----------\ntrain Loss: 0.5366 Acc: 0.7951\nval Loss: 0.7209 Acc: 0.8366\n\nEpoch 3/24\n----------\ntrain Loss: 0.5986 Acc: 0.7828\nval Loss: 0.3783 Acc: 0.9020\n\nEpoch 4/24\n----------\ntrain Loss: 0.6283 Acc: 0.7377\nval Loss: 1.7980 Acc: 0.6078\n\nEpoch 5/24\n----------\ntrain Loss: 0.4772 Acc: 0.8443\nval Loss: 0.3713 Acc: 0.9085\n\nEpoch 6/24\n----------\ntrain Loss: 0.4942 Acc: 0.8402\nval Loss: 0.2495 Acc: 0.9085\n\nEpoch 7/24\n----------\ntrain Loss: 0.3238 Acc: 0.8730\nval Loss: 0.2499 Acc: 0.9085\n\nEpoch 8/24\n----------\ntrain Loss: 0.3906 Acc: 0.8484\nval Loss: 0.1990 Acc: 0.9150\n\nEpoch 9/24\n----------\ntrain Loss: 0.3608 Acc: 0.8525\nval Loss: 0.2252 Acc: 0.9216\n\nEpoch 10/24\n----------\ntrain Loss: 0.2628 Acc: 0.9057\nval Loss: 0.2955 Acc: 0.9085\n\nEpoch 11/24\n----------\ntrain Loss: 0.3554 Acc: 0.8525\nval Loss: 0.2471 Acc: 0.8954\n\nEpoch 12/24\n----------\ntrain Loss: 0.2963 Acc: 0.8934\nval Loss: 0.2155 Acc: 0.9150\n\nEpoch 13/24\n----------\ntrain Loss: 0.3050 Acc: 0.8648\nval Loss: 0.2004 Acc: 0.9150\n\nEpoch 14/24\n----------\ntrain Loss: 0.3896 Acc: 0.8443\nval Loss: 0.2157 Acc: 0.9281\n\nEpoch 15/24\n----------\ntrain Loss: 0.2073 Acc: 0.9221\nval Loss: 0.2530 Acc: 0.9085\n\nEpoch 16/24\n----------\ntrain Loss: 0.3348 Acc: 0.8648\nval Loss: 0.2108 Acc: 0.9216\n\nEpoch 17/24\n----------\ntrain Loss: 0.2891 Acc: 0.8811\nval Loss: 0.2366 Acc: 0.9020\n\nEpoch 18/24\n----------\ntrain Loss: 0.2959 Acc: 0.8607\nval Loss: 0.2332 Acc: 0.9020\n\nEpoch 19/24\n----------\ntrain Loss: 0.2301 Acc: 0.8975\nval Loss: 0.2198 Acc: 0.9085\n\nEpoch 20/24\n----------\ntrain Loss: 0.3115 Acc: 0.8770\nval Loss: 0.2091 Acc: 0.9150\n\nEpoch 21/24\n----------\ntrain Loss: 0.3361 Acc: 0.8525\nval Loss: 0.2017 Acc: 0.9216\n\nEpoch 22/24\n----------\ntrain Loss: 0.3469 Acc: 0.8566\nval Loss: 0.1988 Acc: 0.9281\n\nEpoch 23/24\n----------\ntrain Loss: 0.2733 Acc: 0.8852\nval Loss: 0.1996 Acc: 0.9216\n\nEpoch 24/24\n----------\ntrain Loss: 0.2138 Acc: 0.9180\nval Loss: 0.2245 Acc: 0.9216\n\nTraining complete in 2m 43s\nBest val Acc: 0.928105\n\n\n\nvisualize_model(model_ft)",
    "crumbs": [
      "Pytorch",
      "Vision",
      "Transfer Learning for Computer Vision Tutorial"
    ]
  },
  {
    "objectID": "Pytorch/Vision/transfer_learning_tutorial.html#convnet-as-fixed-feature-extractor",
    "href": "Pytorch/Vision/transfer_learning_tutorial.html#convnet-as-fixed-feature-extractor",
    "title": "Transfer Learning for Computer Vision Tutorial",
    "section": "ConvNet as fixed feature extractor",
    "text": "ConvNet as fixed feature extractor\nHere, we need to freeze all the network except the final layer. We need to set requires_grad = False to freeze the parameters so that the gradients are not computed in backward().\nYou can read more about this in the documentation here_.\n\nmodel_conv = torchvision.models.resnet18(weights='IMAGENET1K_V1')\nfor param in model_conv.parameters():\n    param.requires_grad = False\n\n# Parameters of newly constructed modules have requires_grad=True by default\nnum_ftrs = model_conv.fc.in_features\nmodel_conv.fc = nn.Linear(num_ftrs, 2)\n\nmodel_conv = model_conv.to(device)\n\ncriterion = nn.CrossEntropyLoss()\n\n# Observe that only parameters of final layer are being optimized as\n# opposed to before.\noptimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n\n# Decay LR by a factor of 0.1 every 7 epochs\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)\n\n\nTrain and evaluate\nOn CPU this will take about half the time compared to previous scenario. This is expected as gradients don’t need to be computed for most of the network. However, forward does need to be computed.\n\nmodel_conv = train_model(model_conv, criterion, optimizer_conv,\n                         exp_lr_scheduler, num_epochs=25)\n\nEpoch 0/24\n----------\ntrain Loss: 0.6860 Acc: 0.6475\nval Loss: 0.2267 Acc: 0.9477\n\nEpoch 1/24\n----------\ntrain Loss: 0.5464 Acc: 0.7500\nval Loss: 0.1699 Acc: 0.9608\n\nEpoch 2/24\n----------\ntrain Loss: 0.7593 Acc: 0.7131\nval Loss: 0.2103 Acc: 0.9281\n\nEpoch 3/24\n----------\ntrain Loss: 0.5382 Acc: 0.7664\nval Loss: 0.4601 Acc: 0.8105\n\nEpoch 4/24\n----------\ntrain Loss: 0.4835 Acc: 0.7869\nval Loss: 0.1896 Acc: 0.9477\n\nEpoch 5/24\n----------\ntrain Loss: 0.5646 Acc: 0.7705\nval Loss: 0.2554 Acc: 0.9085\n\nEpoch 6/24\n----------\ntrain Loss: 0.5630 Acc: 0.7705\nval Loss: 0.2555 Acc: 0.9085\n\nEpoch 7/24\n----------\ntrain Loss: 0.4168 Acc: 0.7992\nval Loss: 0.2047 Acc: 0.9412\n\nEpoch 8/24\n----------\ntrain Loss: 0.3179 Acc: 0.8607\nval Loss: 0.1955 Acc: 0.9477\n\nEpoch 9/24\n----------\ntrain Loss: 0.3325 Acc: 0.8730\nval Loss: 0.1902 Acc: 0.9477\n\nEpoch 10/24\n----------\ntrain Loss: 0.3124 Acc: 0.8730\nval Loss: 0.1861 Acc: 0.9346\n\nEpoch 11/24\n----------\ntrain Loss: 0.4282 Acc: 0.8033\nval Loss: 0.2036 Acc: 0.9346\n\nEpoch 12/24\n----------\ntrain Loss: 0.3637 Acc: 0.8361\nval Loss: 0.1756 Acc: 0.9477\n\nEpoch 13/24\n----------\ntrain Loss: 0.3700 Acc: 0.8279\nval Loss: 0.1944 Acc: 0.9412\n\nEpoch 14/24\n----------\ntrain Loss: 0.3731 Acc: 0.8279\nval Loss: 0.1969 Acc: 0.9346\n\nEpoch 15/24\n----------\ntrain Loss: 0.2870 Acc: 0.8852\nval Loss: 0.1903 Acc: 0.9608\n\nEpoch 16/24\n----------\ntrain Loss: 0.3013 Acc: 0.8770\nval Loss: 0.1867 Acc: 0.9412\n\nEpoch 17/24\n----------\ntrain Loss: 0.3239 Acc: 0.8443\nval Loss: 0.1931 Acc: 0.9412\n\nEpoch 18/24\n----------\ntrain Loss: 0.3365 Acc: 0.8811\nval Loss: 0.1851 Acc: 0.9477\n\nEpoch 19/24\n----------\ntrain Loss: 0.3547 Acc: 0.8525\nval Loss: 0.1846 Acc: 0.9346\n\nEpoch 20/24\n----------\ntrain Loss: 0.2705 Acc: 0.8730\nval Loss: 0.1932 Acc: 0.9542\n\nEpoch 21/24\n----------\ntrain Loss: 0.3195 Acc: 0.8566\nval Loss: 0.1954 Acc: 0.9346\n\nEpoch 22/24\n----------\ntrain Loss: 0.3331 Acc: 0.8443\nval Loss: 0.1822 Acc: 0.9542\n\nEpoch 23/24\n----------\ntrain Loss: 0.3553 Acc: 0.8279\nval Loss: 0.2076 Acc: 0.9346\n\nEpoch 24/24\n----------\ntrain Loss: 0.2916 Acc: 0.8770\nval Loss: 0.2060 Acc: 0.9346\n\nTraining complete in 1m 57s\nBest val Acc: 0.960784\n\n\n\nvisualize_model(model_conv)\n\nplt.ioff()\nplt.show()",
    "crumbs": [
      "Pytorch",
      "Vision",
      "Transfer Learning for Computer Vision Tutorial"
    ]
  },
  {
    "objectID": "Pytorch/Vision/transfer_learning_tutorial.html#further-learning",
    "href": "Pytorch/Vision/transfer_learning_tutorial.html#further-learning",
    "title": "Transfer Learning for Computer Vision Tutorial",
    "section": "Further Learning",
    "text": "Further Learning\nIf you would like to learn more about the applications of transfer learning, checkout our Quantized Transfer Learning for Computer Vision Tutorial.\nA few ads maybe displayed for income as resources are now offered freely. 🤝🤝🤝",
    "crumbs": [
      "Pytorch",
      "Vision",
      "Transfer Learning for Computer Vision Tutorial"
    ]
  },
  {
    "objectID": "Pytorch/Vision/torchvision_finetuning_instance_segmentation.html",
    "href": "Pytorch/Vision/torchvision_finetuning_instance_segmentation.html",
    "title": "TorchVision Instance Segmentation Finetuning Tutorial",
    "section": "",
    "text": "Photo by DATAIDEA\npip install cython\n# Install pycocotools, the version by default in Colab\n# has a bug fixed in https://github.com/cocodataset/cocoapi/pull/354\npip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n\nLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nRequirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (0.29.34)\nLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nCollecting git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI\n  Cloning https://github.com/cocodataset/cocoapi.git to /tmp/pip-req-build-6wfz80da\n  Running command git clone --filter=blob:none --quiet https://github.com/cocodataset/cocoapi.git /tmp/pip-req-build-6wfz80da\n  Resolved https://github.com/cocodataset/cocoapi.git to commit 8c9bcc3cf640524c4c20a9c40e89cb6a2f2fa0e9\n  Preparing metadata (setup.py) ... done\nRequirement already satisfied: setuptools&gt;=18.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools==2.0) (67.7.2)\nRequirement already satisfied: cython&gt;=0.27.3 in /usr/local/lib/python3.10/dist-packages (from pycocotools==2.0) (0.29.34)\nRequirement already satisfied: matplotlib&gt;=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools==2.0) (3.7.1)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib&gt;=2.1.0-&gt;pycocotools==2.0) (1.0.7)\nRequirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib&gt;=2.1.0-&gt;pycocotools==2.0) (0.11.0)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib&gt;=2.1.0-&gt;pycocotools==2.0) (4.39.3)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib&gt;=2.1.0-&gt;pycocotools==2.0) (1.4.4)\nRequirement already satisfied: numpy&gt;=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib&gt;=2.1.0-&gt;pycocotools==2.0) (1.22.4)\nRequirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib&gt;=2.1.0-&gt;pycocotools==2.0) (23.1)\nRequirement already satisfied: pillow&gt;=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib&gt;=2.1.0-&gt;pycocotools==2.0) (8.4.0)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib&gt;=2.1.0-&gt;pycocotools==2.0) (3.0.9)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib&gt;=2.1.0-&gt;pycocotools==2.0) (2.8.2)\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil&gt;=2.7-&gt;matplotlib&gt;=2.1.0-&gt;pycocotools==2.0) (1.16.0)\nBuilding wheels for collected packages: pycocotools\n  Building wheel for pycocotools (setup.py) ... done\n  Created wheel for pycocotools: filename=pycocotools-2.0-cp310-cp310-linux_x86_64.whl size=386204 sha256=7ccd4688aec01da06f419b238484385985a87bcd3faf2dde64f31acce9f5f767\n  Stored in directory: /tmp/pip-ephem-wheel-cache-i0vt6v36/wheels/39/61/b4/480fbddb4d3d6bc34083e7397bc6f5d1381f79acc68e9f3511\nSuccessfully built pycocotools\nInstalling collected packages: pycocotools\n  Attempting uninstall: pycocotools\n    Found existing installation: pycocotools 2.0.6\n    Uninstalling pycocotools-2.0.6:\n      Successfully uninstalled pycocotools-2.0.6\nSuccessfully installed pycocotools-2.0",
    "crumbs": [
      "Pytorch",
      "Vision",
      "TorchVision Instance Segmentation Finetuning Tutorial"
    ]
  },
  {
    "objectID": "Pytorch/Vision/torchvision_finetuning_instance_segmentation.html#defining-the-dataset",
    "href": "Pytorch/Vision/torchvision_finetuning_instance_segmentation.html#defining-the-dataset",
    "title": "TorchVision Instance Segmentation Finetuning Tutorial",
    "section": "Defining the Dataset",
    "text": "Defining the Dataset\nThe torchvision reference scripts for training object detection, instance segmentation and person keypoint detection allows for easily supporting adding new custom datasets. The dataset should inherit from the standard torch.utils.data.Dataset class, and implement __len__ and __getitem__.\nThe only specificity that we require is that the dataset __getitem__ should return:\n\nimage: a PIL Image of size (H, W)\ntarget: a dict containing the following fields\n\nboxes (FloatTensor[N, 4]): the coordinates of the N bounding boxes in [x0, y0, x1, y1] format, ranging from 0 to W and 0 to H\nlabels (Int64Tensor[N]): the label for each bounding box\nimage_id (Int64Tensor[1]): an image identifier. It should be unique between all the images in the dataset, and is used during evaluation\narea (Tensor[N]): The area of the bounding box. This is used during evaluation with the COCO metric, to separate the metric scores between small, medium and large boxes.\niscrowd (UInt8Tensor[N]): instances with iscrowd=True will be ignored during evaluation.\n(optionally) masks (UInt8Tensor[N, H, W]): The segmentation masks for each one of the objects\n(optionally) keypoints (FloatTensor[N, K, 3]): For each one of the N objects, it contains the K keypoints in [x, y, visibility] format, defining the object. visibility=0 means that the keypoint is not visible. Note that for data augmentation, the notion of flipping a keypoint is dependent on the data representation, and you should probably adapt references/detection/transforms.py for your new keypoint representation\n\n\nIf your model returns the above methods, they will make it work for both training and evaluation, and will use the evaluation scripts from pycocotools.\nOne note on the labels. The model considers class 0 as background. If your dataset does not contain the background class, you should not have 0 in your labels. For example, assuming you have just two classes, cat and dog, you can define 1 (not 0) to represent cats and 2 to represent dogs. So, for instance, if one of the images has both classes, your labels tensor should look like [1,2].\nAdditionally, if you want to use aspect ratio grouping during training (so that each batch only contains images with similar aspect ratio), then it is recommended to also implement a get_height_and_width method, which returns the height and the width of the image. If this method is not provided, we query all elements of the dataset via __getitem__ , which loads the image in memory and is slower than if a custom method is provided.\n\nWriting a custom dataset for Penn-Fudan\nLet’s write a dataset for the Penn-Fudan dataset.\nFirst, let’s download and extract the data, present in a zip file at https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip\n\n# download the Penn-Fudan dataset\nwget https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip .\n# extract it in the current folder\nunzip PennFudanPed.zip\n\n--2023-05-10 14:19:23--  https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip\nResolving www.cis.upenn.edu (www.cis.upenn.edu)... 158.130.69.163, 2607:f470:8:64:5ea5::d\nConnecting to www.cis.upenn.edu (www.cis.upenn.edu)|158.130.69.163|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 53723336 (51M) [application/zip]\nSaving to: ‘PennFudanPed.zip’\n\nPennFudanPed.zip    100%[===================&gt;]  51.23M  62.6MB/s    in 0.8s    \n\n2023-05-10 14:19:24 (62.6 MB/s) - ‘PennFudanPed.zip’ saved [53723336/53723336]\n\n--2023-05-10 14:19:24--  http://./\nResolving . (.)... failed: No address associated with hostname.\nwget: unable to resolve host address ‘.’\nFINISHED --2023-05-10 14:19:24--\nTotal wall clock time: 1.0s\nDownloaded: 1 files, 51M in 0.8s (62.6 MB/s)\nArchive:  PennFudanPed.zip\n   creating: PennFudanPed/\n  inflating: PennFudanPed/added-object-list.txt  \n   creating: PennFudanPed/Annotation/\n  inflating: PennFudanPed/Annotation/FudanPed00001.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00002.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00003.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00004.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00005.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00006.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00007.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00008.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00009.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00010.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00011.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00012.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00013.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00014.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00015.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00016.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00017.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00018.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00019.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00020.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00021.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00022.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00023.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00024.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00025.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00026.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00027.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00028.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00029.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00030.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00031.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00032.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00033.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00034.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00035.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00036.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00037.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00038.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00039.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00040.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00041.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00042.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00043.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00044.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00045.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00046.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00047.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00048.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00049.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00050.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00051.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00052.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00053.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00054.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00055.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00056.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00057.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00058.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00059.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00060.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00061.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00062.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00063.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00064.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00065.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00066.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00067.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00068.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00069.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00070.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00071.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00072.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00073.txt  \n  inflating: PennFudanPed/Annotation/FudanPed00074.txt  \n  inflating: PennFudanPed/Annotation/PennPed00001.txt  \n  inflating: PennFudanPed/Annotation/PennPed00002.txt  \n  inflating: PennFudanPed/Annotation/PennPed00003.txt  \n  inflating: PennFudanPed/Annotation/PennPed00004.txt  \n  inflating: PennFudanPed/Annotation/PennPed00005.txt  \n  inflating: PennFudanPed/Annotation/PennPed00006.txt  \n  inflating: PennFudanPed/Annotation/PennPed00007.txt  \n  inflating: PennFudanPed/Annotation/PennPed00008.txt  \n  inflating: PennFudanPed/Annotation/PennPed00009.txt  \n  inflating: PennFudanPed/Annotation/PennPed00010.txt  \n  inflating: PennFudanPed/Annotation/PennPed00011.txt  \n  inflating: PennFudanPed/Annotation/PennPed00012.txt  \n  inflating: PennFudanPed/Annotation/PennPed00013.txt  \n  inflating: PennFudanPed/Annotation/PennPed00014.txt  \n  inflating: PennFudanPed/Annotation/PennPed00015.txt  \n  inflating: PennFudanPed/Annotation/PennPed00016.txt  \n  inflating: PennFudanPed/Annotation/PennPed00017.txt  \n  inflating: PennFudanPed/Annotation/PennPed00018.txt  \n  inflating: PennFudanPed/Annotation/PennPed00019.txt  \n  inflating: PennFudanPed/Annotation/PennPed00020.txt  \n  inflating: PennFudanPed/Annotation/PennPed00021.txt  \n  inflating: PennFudanPed/Annotation/PennPed00022.txt  \n  inflating: PennFudanPed/Annotation/PennPed00023.txt  \n  inflating: PennFudanPed/Annotation/PennPed00024.txt  \n  inflating: PennFudanPed/Annotation/PennPed00025.txt  \n  inflating: PennFudanPed/Annotation/PennPed00026.txt  \n  inflating: PennFudanPed/Annotation/PennPed00027.txt  \n  inflating: PennFudanPed/Annotation/PennPed00028.txt  \n  inflating: PennFudanPed/Annotation/PennPed00029.txt  \n  inflating: PennFudanPed/Annotation/PennPed00030.txt  \n  inflating: PennFudanPed/Annotation/PennPed00031.txt  \n  inflating: PennFudanPed/Annotation/PennPed00032.txt  \n  inflating: PennFudanPed/Annotation/PennPed00033.txt  \n  inflating: PennFudanPed/Annotation/PennPed00034.txt  \n  inflating: PennFudanPed/Annotation/PennPed00035.txt  \n  inflating: PennFudanPed/Annotation/PennPed00036.txt  \n  inflating: PennFudanPed/Annotation/PennPed00037.txt  \n  inflating: PennFudanPed/Annotation/PennPed00038.txt  \n  inflating: PennFudanPed/Annotation/PennPed00039.txt  \n  inflating: PennFudanPed/Annotation/PennPed00040.txt  \n  inflating: PennFudanPed/Annotation/PennPed00041.txt  \n  inflating: PennFudanPed/Annotation/PennPed00042.txt  \n  inflating: PennFudanPed/Annotation/PennPed00043.txt  \n  inflating: PennFudanPed/Annotation/PennPed00044.txt  \n  inflating: PennFudanPed/Annotation/PennPed00045.txt  \n  inflating: PennFudanPed/Annotation/PennPed00046.txt  \n  inflating: PennFudanPed/Annotation/PennPed00047.txt  \n  inflating: PennFudanPed/Annotation/PennPed00048.txt  \n  inflating: PennFudanPed/Annotation/PennPed00049.txt  \n  inflating: PennFudanPed/Annotation/PennPed00050.txt  \n  inflating: PennFudanPed/Annotation/PennPed00051.txt  \n  inflating: PennFudanPed/Annotation/PennPed00052.txt  \n  inflating: PennFudanPed/Annotation/PennPed00053.txt  \n  inflating: PennFudanPed/Annotation/PennPed00054.txt  \n  inflating: PennFudanPed/Annotation/PennPed00055.txt  \n  inflating: PennFudanPed/Annotation/PennPed00056.txt  \n  inflating: PennFudanPed/Annotation/PennPed00057.txt  \n  inflating: PennFudanPed/Annotation/PennPed00058.txt  \n  inflating: PennFudanPed/Annotation/PennPed00059.txt  \n  inflating: PennFudanPed/Annotation/PennPed00060.txt  \n  inflating: PennFudanPed/Annotation/PennPed00061.txt  \n  inflating: PennFudanPed/Annotation/PennPed00062.txt  \n  inflating: PennFudanPed/Annotation/PennPed00063.txt  \n  inflating: PennFudanPed/Annotation/PennPed00064.txt  \n  inflating: PennFudanPed/Annotation/PennPed00065.txt  \n  inflating: PennFudanPed/Annotation/PennPed00066.txt  \n  inflating: PennFudanPed/Annotation/PennPed00067.txt  \n  inflating: PennFudanPed/Annotation/PennPed00068.txt  \n  inflating: PennFudanPed/Annotation/PennPed00069.txt  \n  inflating: PennFudanPed/Annotation/PennPed00070.txt  \n  inflating: PennFudanPed/Annotation/PennPed00071.txt  \n  inflating: PennFudanPed/Annotation/PennPed00072.txt  \n  inflating: PennFudanPed/Annotation/PennPed00073.txt  \n  inflating: PennFudanPed/Annotation/PennPed00074.txt  \n  inflating: PennFudanPed/Annotation/PennPed00075.txt  \n  inflating: PennFudanPed/Annotation/PennPed00076.txt  \n  inflating: PennFudanPed/Annotation/PennPed00077.txt  \n  inflating: PennFudanPed/Annotation/PennPed00078.txt  \n  inflating: PennFudanPed/Annotation/PennPed00079.txt  \n  inflating: PennFudanPed/Annotation/PennPed00080.txt  \n  inflating: PennFudanPed/Annotation/PennPed00081.txt  \n  inflating: PennFudanPed/Annotation/PennPed00082.txt  \n  inflating: PennFudanPed/Annotation/PennPed00083.txt  \n  inflating: PennFudanPed/Annotation/PennPed00084.txt  \n  inflating: PennFudanPed/Annotation/PennPed00085.txt  \n  inflating: PennFudanPed/Annotation/PennPed00086.txt  \n  inflating: PennFudanPed/Annotation/PennPed00087.txt  \n  inflating: PennFudanPed/Annotation/PennPed00088.txt  \n  inflating: PennFudanPed/Annotation/PennPed00089.txt  \n  inflating: PennFudanPed/Annotation/PennPed00090.txt  \n  inflating: PennFudanPed/Annotation/PennPed00091.txt  \n  inflating: PennFudanPed/Annotation/PennPed00092.txt  \n  inflating: PennFudanPed/Annotation/PennPed00093.txt  \n  inflating: PennFudanPed/Annotation/PennPed00094.txt  \n  inflating: PennFudanPed/Annotation/PennPed00095.txt  \n  inflating: PennFudanPed/Annotation/PennPed00096.txt  \n   creating: PennFudanPed/PedMasks/\n  inflating: PennFudanPed/PedMasks/FudanPed00001_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00002_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00003_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00004_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00005_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00006_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00007_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00008_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00009_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00010_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00011_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00012_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00013_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00014_mask.png  \n extracting: PennFudanPed/PedMasks/FudanPed00015_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00016_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00017_mask.png  \n extracting: PennFudanPed/PedMasks/FudanPed00018_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00019_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00020_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00021_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00022_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00023_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00024_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00025_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00026_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00027_mask.png  \n extracting: PennFudanPed/PedMasks/FudanPed00028_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00029_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00030_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00031_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00032_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00033_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00034_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00035_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00036_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00037_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00038_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00039_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00040_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00041_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00042_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00043_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00044_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00045_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00046_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00047_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00048_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00049_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00050_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00051_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00052_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00053_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00054_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00055_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00056_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00057_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00058_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00059_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00060_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00061_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00062_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00063_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00064_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00065_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00066_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00067_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00068_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00069_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00070_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00071_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00072_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00073_mask.png  \n  inflating: PennFudanPed/PedMasks/FudanPed00074_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00001_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00002_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00003_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00004_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00005_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00006_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00007_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00008_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00009_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00010_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00011_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00012_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00013_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00014_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00015_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00016_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00017_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00018_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00019_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00020_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00021_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00022_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00023_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00024_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00025_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00026_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00027_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00028_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00029_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00030_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00031_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00032_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00033_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00034_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00035_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00036_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00037_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00038_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00039_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00040_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00041_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00042_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00043_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00044_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00045_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00046_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00047_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00048_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00049_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00050_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00051_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00052_mask.png  \n extracting: PennFudanPed/PedMasks/PennPed00053_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00054_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00055_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00056_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00057_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00058_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00059_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00060_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00061_mask.png  \n extracting: PennFudanPed/PedMasks/PennPed00062_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00063_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00064_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00065_mask.png  \n extracting: PennFudanPed/PedMasks/PennPed00066_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00067_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00068_mask.png  \n extracting: PennFudanPed/PedMasks/PennPed00069_mask.png  \n extracting: PennFudanPed/PedMasks/PennPed00070_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00071_mask.png  \n extracting: PennFudanPed/PedMasks/PennPed00072_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00073_mask.png  \n extracting: PennFudanPed/PedMasks/PennPed00074_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00075_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00076_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00077_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00078_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00079_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00080_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00081_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00082_mask.png  \n extracting: PennFudanPed/PedMasks/PennPed00083_mask.png  \n extracting: PennFudanPed/PedMasks/PennPed00084_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00085_mask.png  \n extracting: PennFudanPed/PedMasks/PennPed00086_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00087_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00088_mask.png  \n extracting: PennFudanPed/PedMasks/PennPed00089_mask.png  \n extracting: PennFudanPed/PedMasks/PennPed00090_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00091_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00092_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00093_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00094_mask.png  \n  inflating: PennFudanPed/PedMasks/PennPed00095_mask.png  \n extracting: PennFudanPed/PedMasks/PennPed00096_mask.png  \n   creating: PennFudanPed/PNGImages/\n  inflating: PennFudanPed/PNGImages/FudanPed00001.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00002.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00003.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00004.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00005.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00006.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00007.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00008.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00009.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00010.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00011.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00012.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00013.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00014.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00015.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00016.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00017.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00018.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00019.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00020.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00021.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00022.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00023.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00024.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00025.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00026.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00027.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00028.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00029.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00030.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00031.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00032.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00033.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00034.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00035.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00036.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00037.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00038.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00039.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00040.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00041.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00042.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00043.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00044.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00045.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00046.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00047.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00048.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00049.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00050.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00051.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00052.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00053.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00054.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00055.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00056.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00057.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00058.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00059.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00060.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00061.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00062.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00063.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00064.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00065.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00066.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00067.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00068.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00069.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00070.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00071.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00072.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00073.png  \n  inflating: PennFudanPed/PNGImages/FudanPed00074.png  \n  inflating: PennFudanPed/PNGImages/PennPed00001.png  \n  inflating: PennFudanPed/PNGImages/PennPed00002.png  \n  inflating: PennFudanPed/PNGImages/PennPed00003.png  \n  inflating: PennFudanPed/PNGImages/PennPed00004.png  \n  inflating: PennFudanPed/PNGImages/PennPed00005.png  \n  inflating: PennFudanPed/PNGImages/PennPed00006.png  \n  inflating: PennFudanPed/PNGImages/PennPed00007.png  \n  inflating: PennFudanPed/PNGImages/PennPed00008.png  \n  inflating: PennFudanPed/PNGImages/PennPed00009.png  \n  inflating: PennFudanPed/PNGImages/PennPed00010.png  \n  inflating: PennFudanPed/PNGImages/PennPed00011.png  \n  inflating: PennFudanPed/PNGImages/PennPed00012.png  \n  inflating: PennFudanPed/PNGImages/PennPed00013.png  \n  inflating: PennFudanPed/PNGImages/PennPed00014.png  \n  inflating: PennFudanPed/PNGImages/PennPed00015.png  \n  inflating: PennFudanPed/PNGImages/PennPed00016.png  \n  inflating: PennFudanPed/PNGImages/PennPed00017.png  \n  inflating: PennFudanPed/PNGImages/PennPed00018.png  \n  inflating: PennFudanPed/PNGImages/PennPed00019.png  \n  inflating: PennFudanPed/PNGImages/PennPed00020.png  \n  inflating: PennFudanPed/PNGImages/PennPed00021.png  \n  inflating: PennFudanPed/PNGImages/PennPed00022.png  \n  inflating: PennFudanPed/PNGImages/PennPed00023.png  \n  inflating: PennFudanPed/PNGImages/PennPed00024.png  \n  inflating: PennFudanPed/PNGImages/PennPed00025.png  \n  inflating: PennFudanPed/PNGImages/PennPed00026.png  \n  inflating: PennFudanPed/PNGImages/PennPed00027.png  \n  inflating: PennFudanPed/PNGImages/PennPed00028.png  \n  inflating: PennFudanPed/PNGImages/PennPed00029.png  \n  inflating: PennFudanPed/PNGImages/PennPed00030.png  \n  inflating: PennFudanPed/PNGImages/PennPed00031.png  \n  inflating: PennFudanPed/PNGImages/PennPed00032.png  \n  inflating: PennFudanPed/PNGImages/PennPed00033.png  \n  inflating: PennFudanPed/PNGImages/PennPed00034.png  \n  inflating: PennFudanPed/PNGImages/PennPed00035.png  \n  inflating: PennFudanPed/PNGImages/PennPed00036.png  \n  inflating: PennFudanPed/PNGImages/PennPed00037.png  \n  inflating: PennFudanPed/PNGImages/PennPed00038.png  \n  inflating: PennFudanPed/PNGImages/PennPed00039.png  \n  inflating: PennFudanPed/PNGImages/PennPed00040.png  \n  inflating: PennFudanPed/PNGImages/PennPed00041.png  \n  inflating: PennFudanPed/PNGImages/PennPed00042.png  \n  inflating: PennFudanPed/PNGImages/PennPed00043.png  \n  inflating: PennFudanPed/PNGImages/PennPed00044.png  \n  inflating: PennFudanPed/PNGImages/PennPed00045.png  \n  inflating: PennFudanPed/PNGImages/PennPed00046.png  \n  inflating: PennFudanPed/PNGImages/PennPed00047.png  \n  inflating: PennFudanPed/PNGImages/PennPed00048.png  \n  inflating: PennFudanPed/PNGImages/PennPed00049.png  \n  inflating: PennFudanPed/PNGImages/PennPed00050.png  \n  inflating: PennFudanPed/PNGImages/PennPed00051.png  \n  inflating: PennFudanPed/PNGImages/PennPed00052.png  \n  inflating: PennFudanPed/PNGImages/PennPed00053.png  \n  inflating: PennFudanPed/PNGImages/PennPed00054.png  \n  inflating: PennFudanPed/PNGImages/PennPed00055.png  \n  inflating: PennFudanPed/PNGImages/PennPed00056.png  \n  inflating: PennFudanPed/PNGImages/PennPed00057.png  \n  inflating: PennFudanPed/PNGImages/PennPed00058.png  \n  inflating: PennFudanPed/PNGImages/PennPed00059.png  \n  inflating: PennFudanPed/PNGImages/PennPed00060.png  \n  inflating: PennFudanPed/PNGImages/PennPed00061.png  \n  inflating: PennFudanPed/PNGImages/PennPed00062.png  \n  inflating: PennFudanPed/PNGImages/PennPed00063.png  \n  inflating: PennFudanPed/PNGImages/PennPed00064.png  \n  inflating: PennFudanPed/PNGImages/PennPed00065.png  \n  inflating: PennFudanPed/PNGImages/PennPed00066.png  \n  inflating: PennFudanPed/PNGImages/PennPed00067.png  \n  inflating: PennFudanPed/PNGImages/PennPed00068.png  \n  inflating: PennFudanPed/PNGImages/PennPed00069.png  \n  inflating: PennFudanPed/PNGImages/PennPed00070.png  \n  inflating: PennFudanPed/PNGImages/PennPed00071.png  \n  inflating: PennFudanPed/PNGImages/PennPed00072.png  \n  inflating: PennFudanPed/PNGImages/PennPed00073.png  \n  inflating: PennFudanPed/PNGImages/PennPed00074.png  \n  inflating: PennFudanPed/PNGImages/PennPed00075.png  \n  inflating: PennFudanPed/PNGImages/PennPed00076.png  \n  inflating: PennFudanPed/PNGImages/PennPed00077.png  \n  inflating: PennFudanPed/PNGImages/PennPed00078.png  \n  inflating: PennFudanPed/PNGImages/PennPed00079.png  \n  inflating: PennFudanPed/PNGImages/PennPed00080.png  \n  inflating: PennFudanPed/PNGImages/PennPed00081.png  \n  inflating: PennFudanPed/PNGImages/PennPed00082.png  \n  inflating: PennFudanPed/PNGImages/PennPed00083.png  \n  inflating: PennFudanPed/PNGImages/PennPed00084.png  \n  inflating: PennFudanPed/PNGImages/PennPed00085.png  \n  inflating: PennFudanPed/PNGImages/PennPed00086.png  \n  inflating: PennFudanPed/PNGImages/PennPed00087.png  \n  inflating: PennFudanPed/PNGImages/PennPed00088.png  \n  inflating: PennFudanPed/PNGImages/PennPed00089.png  \n  inflating: PennFudanPed/PNGImages/PennPed00090.png  \n  inflating: PennFudanPed/PNGImages/PennPed00091.png  \n  inflating: PennFudanPed/PNGImages/PennPed00092.png  \n  inflating: PennFudanPed/PNGImages/PennPed00093.png  \n  inflating: PennFudanPed/PNGImages/PennPed00094.png  \n  inflating: PennFudanPed/PNGImages/PennPed00095.png  \n  inflating: PennFudanPed/PNGImages/PennPed00096.png  \n  inflating: PennFudanPed/readme.txt  \n\n\n\n\n\nLet’s have a look at the dataset and how it is layed down.\nThe data is structured as follows\nPennFudanPed/\n  PedMasks/\n    FudanPed00001_mask.png\n    FudanPed00002_mask.png\n    FudanPed00003_mask.png\n    FudanPed00004_mask.png\n    ...\n  PNGImages/\n    FudanPed00001.png\n    FudanPed00002.png\n    FudanPed00003.png\n    FudanPed00004.png\nHere is one example of an image in the dataset, with its corresponding instance segmentation mask\n\nfrom PIL import Image\nImage.open('PennFudanPed/PNGImages/FudanPed00001.png')\n\n\n\n\n\n\n\n\n\nmask = Image.open('PennFudanPed/PedMasks/FudanPed00001_mask.png')\n# each mask instance has a different color, from zero to N, where\n# N is the number of instances. In order to make visualization easier,\n# let's adda color palette to the mask.\nmask = mask.convert('P')\nmask.putpalette([\n    0, 0, 0, # black background\n    255, 0, 0, # index 1 is red\n    255, 255, 0, # index 2 is yellow\n    255, 153, 0, # index 3 is orange\n])\nmask\n\n\n\n\n\n\n\n\nSo each image has a corresponding segmentation mask, where each color correspond to a different instance. Let’s write a torch.utils.data.Dataset class for this dataset.\n\nimport os\nimport numpy as np\nimport torch\nimport torch.utils.data\nfrom PIL import Image\n\n\nclass PennFudanDataset(torch.utils.data.Dataset):\n    def __init__(self, root, transforms=None):\n        self.root = root\n        self.transforms = transforms\n        # load all image files, sorting them to\n        # ensure that they are aligned\n        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n\n    def __getitem__(self, idx):\n        # load images ad masks\n        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n        img = Image.open(img_path).convert(\"RGB\")\n        # note that we haven't converted the mask to RGB,\n        # because each color corresponds to a different instance\n        # with 0 being background\n        mask = Image.open(mask_path)\n\n        mask = np.array(mask)\n        # instances are encoded as different colors\n        obj_ids = np.unique(mask)\n        # first id is the background, so remove it\n        obj_ids = obj_ids[1:]\n\n        # split the color-encoded mask into a set\n        # of binary masks\n        masks = mask == obj_ids[:, None, None]\n\n        # get bounding box coordinates for each mask\n        num_objs = len(obj_ids)\n        boxes = []\n        for i in range(num_objs):\n            pos = np.where(masks[i])\n            xmin = np.min(pos[1])\n            xmax = np.max(pos[1])\n            ymin = np.min(pos[0])\n            ymax = np.max(pos[0])\n            boxes.append([xmin, ymin, xmax, ymax])\n\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        # there is only one class\n        labels = torch.ones((num_objs,), dtype=torch.int64)\n        masks = torch.as_tensor(masks, dtype=torch.uint8)\n\n        image_id = torch.tensor([idx])\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        # suppose all instances are not crowd\n        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n\n        target = {}\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n        target[\"masks\"] = masks\n        target[\"image_id\"] = image_id\n        target[\"area\"] = area\n        target[\"iscrowd\"] = iscrowd\n\n        if self.transforms is not None:\n            img, target = self.transforms(img, target)\n\n        return img, target\n\n    def __len__(self):\n        return len(self.imgs)\n\nThat’s all for the dataset. Let’s see how the outputs are structured for this dataset\n\ndataset = PennFudanDataset('PennFudanPed/')\ndataset[0]\n\n(&lt;PIL.Image.Image image mode=RGB size=559x536 at 0x7FD2682FFD90&gt;,\n {'boxes': tensor([[159., 181., 301., 430.],\n          [419., 170., 534., 485.]]),\n  'labels': tensor([1, 1]),\n  'masks': tensor([[[0, 0, 0,  ..., 0, 0, 0],\n           [0, 0, 0,  ..., 0, 0, 0],\n           [0, 0, 0,  ..., 0, 0, 0],\n           ...,\n           [0, 0, 0,  ..., 0, 0, 0],\n           [0, 0, 0,  ..., 0, 0, 0],\n           [0, 0, 0,  ..., 0, 0, 0]],\n  \n          [[0, 0, 0,  ..., 0, 0, 0],\n           [0, 0, 0,  ..., 0, 0, 0],\n           [0, 0, 0,  ..., 0, 0, 0],\n           ...,\n           [0, 0, 0,  ..., 0, 0, 0],\n           [0, 0, 0,  ..., 0, 0, 0],\n           [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8),\n  'image_id': tensor([0]),\n  'area': tensor([35358., 36225.]),\n  'iscrowd': tensor([0, 0])})\n\n\nSo we can see that by default, the dataset returns a PIL.Image and a dictionary containing several fields, including boxes, labels and masks.",
    "crumbs": [
      "Pytorch",
      "Vision",
      "TorchVision Instance Segmentation Finetuning Tutorial"
    ]
  },
  {
    "objectID": "Pytorch/Vision/torchvision_finetuning_instance_segmentation.html#defining-your-model",
    "href": "Pytorch/Vision/torchvision_finetuning_instance_segmentation.html#defining-your-model",
    "title": "TorchVision Instance Segmentation Finetuning Tutorial",
    "section": "Defining your model",
    "text": "Defining your model\nIn this tutorial, we will be using Mask R-CNN, which is based on top of Faster R-CNN. Faster R-CNN is a model that predicts both bounding boxes and class scores for potential objects in the image.\n\n\n\nFaster R-CNN\n\n\nMask R-CNN adds an extra branch into Faster R-CNN, which also predicts segmentation masks for each instance.\n\n\n\nMask R-CNN\n\n\nThere are two common situations where one might want to modify one of the available models in torchvision modelzoo. The first is when we want to start from a pre-trained model, and just finetune the last layer. The other is when we want to replace the backbone of the model with a different one (for faster predictions, for example).\nLet’s go see how we would do one or another in the following sections.\n\n1 - Finetuning from a pretrained model\nLet’s suppose that you want to start from a model pre-trained on COCO and want to finetune it for your particular classes. Here is a possible way of doing it:\nimport torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n\n# load a model pre-trained pre-trained on COCO\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n\n# replace the classifier with a new one, that has\n# num_classes which is user-defined\nnum_classes = 2  # 1 class (person) + background\n# get number of input features for the classifier\nin_features = model.roi_heads.box_predictor.cls_score.in_features\n# replace the pre-trained head with a new one\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) \n\n\n2 - Modifying the model to add a different backbone\nAnother common situation arises when the user wants to replace the backbone of a detection model with a different one. For example, the current default backbone (ResNet-50) might be too big for some applications, and smaller models might be necessary.\nHere is how we would go into leveraging the functions provided by torchvision to modify a backbone.\nimport torchvision\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\n\n# load a pre-trained model for classification and return\n# only the features\nbackbone = torchvision.models.mobilenet_v2(pretrained=True).features\n# FasterRCNN needs to know the number of\n# output channels in a backbone. For mobilenet_v2, it's 1280\n# so we need to add it here\nbackbone.out_channels = 1280\n\n# let's make the RPN generate 5 x 3 anchors per spatial\n# location, with 5 different sizes and 3 different aspect\n# ratios. We have a Tuple[Tuple[int]] because each feature\n# map could potentially have different sizes and\n# aspect ratios \nanchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n                                   aspect_ratios=((0.5, 1.0, 2.0),))\n\n# let's define what are the feature maps that we will\n# use to perform the region of interest cropping, as well as\n# the size of the crop after rescaling.\n# if your backbone returns a Tensor, featmap_names is expected to\n# be [0]. More generally, the backbone should return an\n# OrderedDict[Tensor], and in featmap_names you can choose which\n# feature maps to use.\nroi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[0],\n                                                output_size=7,\n                                                sampling_ratio=2)\n\n# put the pieces together inside a FasterRCNN model\nmodel = FasterRCNN(backbone,\n                   num_classes=2,\n                   rpn_anchor_generator=anchor_generator,\n                   box_roi_pool=roi_pooler)\n\n\nAn Instance segmentation model for PennFudan Dataset\nIn our case, we want to fine-tune from a pre-trained model, given that our dataset is very small. So we will be following approach number 1.\nHere we want to also compute the instance segmentation masks, so we will be using Mask R-CNN:\n\nimport torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n\n      \ndef get_instance_segmentation_model(num_classes):\n    # load an instance segmentation model pre-trained on COCO\n    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n\n    # get the number of input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    # replace the pre-trained head with a new one\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n    # now get the number of input features for the mask classifier\n    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n    hidden_layer = 256\n    # and replace the mask predictor with a new one\n    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n                                                       hidden_layer,\n                                                       num_classes)\n\n    return model\n\nThat’s it, this will make model be ready to be trained and evaluated on our custom dataset.",
    "crumbs": [
      "Pytorch",
      "Vision",
      "TorchVision Instance Segmentation Finetuning Tutorial"
    ]
  },
  {
    "objectID": "Pytorch/Vision/torchvision_finetuning_instance_segmentation.html#training-and-evaluation-functions",
    "href": "Pytorch/Vision/torchvision_finetuning_instance_segmentation.html#training-and-evaluation-functions",
    "title": "TorchVision Instance Segmentation Finetuning Tutorial",
    "section": "Training and evaluation functions",
    "text": "Training and evaluation functions\nIn references/detection/, we have a number of helper functions to simplify training and evaluating detection models. Here, we will use references/detection/engine.py, references/detection/utils.py and references/detection/transforms.py.\nLet’s copy those files (and their dependencies) in here so that they are available in the notebook\n\n# Download TorchVision repo to use some files from\n# references/detection\ngit clone https://github.com/pytorch/vision.git\ncd vision\ngit checkout v0.8.2\n\ncp references/detection/utils.py ../\ncp references/detection/transforms.py ../\ncp references/detection/coco_eval.py ../\ncp references/detection/engine.py ../\ncp references/detection/coco_utils.py ../\n\nfatal: destination path 'vision' already exists and is not an empty directory.\nHEAD is now at 2f40a483d7 [v0.8.X] .circleci: Add Python 3.9 to CI (#3063)\n\n\n\n\n\nLet’s write some helper functions for data augmentation / transformation, which leverages the functions in refereces/detection that we have just copied:\n\nfrom engine import train_one_epoch, evaluate\nimport utils\nimport transforms as T\n\n\ndef get_transform(train):\n    transforms = []\n    # converts the image, a PIL image, into a PyTorch Tensor\n    transforms.append(T.ToTensor())\n    if train:\n        # during training, randomly flip the training images\n        # and ground-truth for data augmentation\n        transforms.append(T.RandomHorizontalFlip(0.5))\n    return T.Compose(transforms)\n\nModuleNotFoundError: ignored\n\n\n\nTesting forward() method\nBefore iterating over the dataset, it’s good to see what the model expects during training and inference time on sample data.\n\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\ndataset = PennFudanDataset('PennFudanPed', get_transform(train=True))\ndata_loader = torch.utils.data.DataLoader(\n    dataset, batch_size=2, shuffle=True, num_workers=4,\n    collate_fn=utils.collate_fn\n)\n# For Training\nimages,targets = next(iter(data_loader))\nimages = list(image for image in images)\ntargets = [{k: v for k, v in t.items()} for t in targets]\noutput = model(images,targets)   # Returns losses and detections\n# For inference\nmodel.eval()\nx = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\npredictions = model(x)           # Returns predictions\n\n/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  max_num_worker_suggest,\n\n\nTypeError: ignored\n\n\n\n\nNote that we do not need to add a mean/std normalization nor image rescaling in the data transforms, as those are handled internally by the Mask R-CNN model.\n\n\nPutting everything together\nWe now have the dataset class, the models and the data transforms. Let’s instantiate them\n\n# use our dataset and defined transformations\ndataset = PennFudanDataset('PennFudanPed', get_transform(train=True))\ndataset_test = PennFudanDataset('PennFudanPed', get_transform(train=False))\n\n# split the dataset in train and test set\ntorch.manual_seed(1)\nindices = torch.randperm(len(dataset)).tolist()\ndataset = torch.utils.data.Subset(dataset, indices[:-50])\ndataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n\n# define training and validation data loaders\ndata_loader = torch.utils.data.DataLoader(\n    dataset, batch_size=2, shuffle=True, num_workers=4,\n    collate_fn=utils.collate_fn)\n\ndata_loader_test = torch.utils.data.DataLoader(\n    dataset_test, batch_size=1, shuffle=False, num_workers=4,\n    collate_fn=utils.collate_fn)\n\nNameError: ignored\n\n\nNow let’s instantiate the model and the optimizer\n\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n# our dataset has two classes only - background and person\nnum_classes = 2\n\n# get the model using our helper function\nmodel = get_instance_segmentation_model(num_classes)\n# move model to the right device\nmodel.to(device)\n\n# construct an optimizer\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.005,\n                            momentum=0.9, weight_decay=0.0005)\n\n# and a learning rate scheduler which decreases the learning rate by\n# 10x every 3 epochs\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n                                               step_size=3,\n                                               gamma=0.1)\n\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MaskRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=MaskRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\" to /root/.cache/torch/hub/checkpoints/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\n100%|██████████| 170M/170M [00:00&lt;00:00, 186MB/s]\n\n\nAnd now let’s train the model for 10 epochs, evaluating at the end of every epoch.\n\n# let's train it for 10 epochs\nfrom torch.optim.lr_scheduler import StepLR\nnum_epochs = 10\n\nfor epoch in range(num_epochs):\n    # train for one epoch, printing every 10 iterations\n    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n    # update the learning rate\n    lr_scheduler.step()\n    # evaluate on the test dataset\n    evaluate(model, data_loader_test, device=device)\n\nNameError: ignored\n\n\nNow that training has finished, let’s have a look at what it actually predicts in a test image\n\n# pick one image from the test set\nimg, _ = dataset_test[0]\n# put the model in evaluation mode\nmodel.eval()\nwith torch.no_grad():\n    prediction = model([img.to(device)])\n\nNameError: ignored\n\n\nPrinting the prediction shows that we have a list of dictionaries. Each element of the list corresponds to a different image. As we have a single image, there is a single dictionary in the list. The dictionary contains the predictions for the image we passed. In this case, we can see that it contains boxes, labels, masks and scores as fields.\n\nprediction\n\n[{'boxes': tensor([[ 62.4634,  41.9885, 194.4252, 324.7122],\n          [276.3589,  23.3166, 291.1442,  74.5785]], device='cuda:0'),\n  'labels': tensor([1, 1], device='cuda:0'),\n  'masks': tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n            [0., 0., 0.,  ..., 0., 0., 0.],\n            [0., 0., 0.,  ..., 0., 0., 0.],\n            ...,\n            [0., 0., 0.,  ..., 0., 0., 0.],\n            [0., 0., 0.,  ..., 0., 0., 0.],\n            [0., 0., 0.,  ..., 0., 0., 0.]]],\n  \n  \n          [[[0., 0., 0.,  ..., 0., 0., 0.],\n            [0., 0., 0.,  ..., 0., 0., 0.],\n            [0., 0., 0.,  ..., 0., 0., 0.],\n            ...,\n            [0., 0., 0.,  ..., 0., 0., 0.],\n            [0., 0., 0.,  ..., 0., 0., 0.],\n            [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0'),\n  'scores': tensor([0.9969, 0.7550], device='cuda:0')}]\n\n\nLet’s inspect the image and the predicted segmentation masks.\nFor that, we need to convert the image, which has been rescaled to 0-1 and had the channels flipped so that we have it in [C, H, W] format.\n\nImage.fromarray(img.mul(255).permute(1, 2, 0).byte().numpy())\n\n\n\n\n\n\n\n\nAnd let’s now visualize the top predicted segmentation mask. The masks are predicted as [N, 1, H, W], where N is the number of predictions, and are probability maps between 0-1.\n\nImage.fromarray(prediction[0]['masks'][0, 0].mul(255).byte().cpu().numpy())\n\n\n\n\n\n\n\n\nLooks pretty good!",
    "crumbs": [
      "Pytorch",
      "Vision",
      "TorchVision Instance Segmentation Finetuning Tutorial"
    ]
  },
  {
    "objectID": "Pytorch/Vision/torchvision_finetuning_instance_segmentation.html#wrapping-up",
    "href": "Pytorch/Vision/torchvision_finetuning_instance_segmentation.html#wrapping-up",
    "title": "TorchVision Instance Segmentation Finetuning Tutorial",
    "section": "Wrapping up",
    "text": "Wrapping up\nIn this tutorial, you have learned how to create your own training pipeline for instance segmentation models, on a custom dataset. For that, you wrote a torch.utils.data.Dataset class that returns the images and the ground truth boxes and segmentation masks. You also leveraged a Mask R-CNN model pre-trained on COCO train2017 in order to perform transfer learning on this new dataset.\nFor a more complete example, which includes multi-machine / multi-gpu training, check references/detection/train.py, which is present in the torchvision GitHub repo.\nA few ads maybe displayed for income as resources are now offered freely. 🤝🤝🤝",
    "crumbs": [
      "Pytorch",
      "Vision",
      "TorchVision Instance Segmentation Finetuning Tutorial"
    ]
  },
  {
    "objectID": "Pytorch/Vision/spatial_transformer_tutorial.html",
    "href": "Pytorch/Vision/spatial_transformer_tutorial.html",
    "title": "Spatial Transformer Networks Tutorial",
    "section": "",
    "text": "Photo by DATAIDEA\n# License: BSD\n# Author: Ghassen Hamrouni\n\nfrom __future__ import print_function\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision\nfrom torchvision import datasets, transforms\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nplt.ion()   # interactive mode",
    "crumbs": [
      "Pytorch",
      "Vision",
      "Spatial Transformer Networks Tutorial"
    ]
  },
  {
    "objectID": "Pytorch/Vision/spatial_transformer_tutorial.html#loading-the-data",
    "href": "Pytorch/Vision/spatial_transformer_tutorial.html#loading-the-data",
    "title": "Spatial Transformer Networks Tutorial",
    "section": "Loading the data",
    "text": "Loading the data\nIn this post we experiment with the classic MNIST dataset. Using a standard convolutional network augmented with a spatial transformer network.\n\nfrom six.moves import urllib\nopener = urllib.request.build_opener()\nopener.addheaders = [('User-agent', 'Mozilla/5.0')]\nurllib.request.install_opener(opener)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Training dataset\ntrain_loader = torch.utils.data.DataLoader(\n    datasets.MNIST(root='.', train=True, download=True,\n                   transform=transforms.Compose([\n                       transforms.ToTensor(),\n                       transforms.Normalize((0.1307,), (0.3081,))\n                   ])), batch_size=64, shuffle=True, num_workers=4)\n# Test dataset\ntest_loader = torch.utils.data.DataLoader(\n    datasets.MNIST(root='.', train=False, transform=transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))\n    ])), batch_size=64, shuffle=True, num_workers=4)",
    "crumbs": [
      "Pytorch",
      "Vision",
      "Spatial Transformer Networks Tutorial"
    ]
  },
  {
    "objectID": "Pytorch/Vision/spatial_transformer_tutorial.html#depicting-spatial-transformer-networks",
    "href": "Pytorch/Vision/spatial_transformer_tutorial.html#depicting-spatial-transformer-networks",
    "title": "Spatial Transformer Networks Tutorial",
    "section": "Depicting spatial transformer networks",
    "text": "Depicting spatial transformer networks\nSpatial transformer networks boils down to three main components :\n\nThe localization network is a regular CNN which regresses the transformation parameters. The transformation is never learned explicitly from this dataset, instead the network learns automatically the spatial transformations that enhances the global accuracy.\nThe grid generator generates a grid of coordinates in the input image corresponding to each pixel from the output image.\nThe sampler uses the parameters of the transformation and applies it to the input image.\n\n.. figure:: /_static/img/stn/stn-arch.png\n.. Note:: We need the latest version of PyTorch that contains affine_grid and grid_sample modules.\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n        # Spatial transformer localization-network\n        self.localization = nn.Sequential(\n            nn.Conv2d(1, 8, kernel_size=7),\n            nn.MaxPool2d(2, stride=2),\n            nn.ReLU(True),\n            nn.Conv2d(8, 10, kernel_size=5),\n            nn.MaxPool2d(2, stride=2),\n            nn.ReLU(True)\n        )\n\n        # Regressor for the 3 * 2 affine matrix\n        self.fc_loc = nn.Sequential(\n            nn.Linear(10 * 3 * 3, 32),\n            nn.ReLU(True),\n            nn.Linear(32, 3 * 2)\n        )\n\n        # Initialize the weights/bias with identity transformation\n        self.fc_loc[2].weight.data.zero_()\n        self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n\n    # Spatial transformer network forward function\n    def stn(self, x):\n        xs = self.localization(x)\n        xs = xs.view(-1, 10 * 3 * 3)\n        theta = self.fc_loc(xs)\n        theta = theta.view(-1, 2, 3)\n\n        grid = F.affine_grid(theta, x.size())\n        x = F.grid_sample(x, grid)\n\n        return x\n\n    def forward(self, x):\n        # transform the input\n        x = self.stn(x)\n\n        # Perform the usual forward pass\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n\n\nmodel = Net().to(device)",
    "crumbs": [
      "Pytorch",
      "Vision",
      "Spatial Transformer Networks Tutorial"
    ]
  },
  {
    "objectID": "Pytorch/Vision/spatial_transformer_tutorial.html#training-the-model",
    "href": "Pytorch/Vision/spatial_transformer_tutorial.html#training-the-model",
    "title": "Spatial Transformer Networks Tutorial",
    "section": "Training the model",
    "text": "Training the model\nNow, let’s use the SGD algorithm to train the model. The network is learning the classification task in a supervised way. In the same time the model is learning STN automatically in an end-to-end fashion.\n\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\n\ndef train(epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % 500 == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.item()))\n#\n# A simple test procedure to measure the STN performances on MNIST.\n#\n\n\ndef test():\n    with torch.no_grad():\n        model.eval()\n        test_loss = 0\n        correct = 0\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n\n            # sum up batch loss\n            test_loss += F.nll_loss(output, target, size_average=False).item()\n            # get the index of the max log-probability\n            pred = output.max(1, keepdim=True)[1]\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n        test_loss /= len(test_loader.dataset)\n        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'\n              .format(test_loss, correct, len(test_loader.dataset),\n                      100. * correct / len(test_loader.dataset)))",
    "crumbs": [
      "Pytorch",
      "Vision",
      "Spatial Transformer Networks Tutorial"
    ]
  },
  {
    "objectID": "Pytorch/Vision/spatial_transformer_tutorial.html#visualizing-the-stn-results",
    "href": "Pytorch/Vision/spatial_transformer_tutorial.html#visualizing-the-stn-results",
    "title": "Spatial Transformer Networks Tutorial",
    "section": "Visualizing the STN results",
    "text": "Visualizing the STN results\nNow, we will inspect the results of our learned visual attention mechanism.\nWe define a small helper function in order to visualize the transformations while training.\n\ndef convert_image_np(inp):\n    \"\"\"Convert a Tensor to numpy image.\"\"\"\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    return inp\n\n# We want to visualize the output of the spatial transformers layer\n# after the training, we visualize a batch of input images and\n# the corresponding transformed batch using STN.\n\n\ndef visualize_stn():\n    with torch.no_grad():\n        # Get a batch of training data\n        data = next(iter(test_loader))[0].to(device)\n\n        input_tensor = data.cpu()\n        transformed_input_tensor = model.stn(data).cpu()\n\n        in_grid = convert_image_np(\n            torchvision.utils.make_grid(input_tensor))\n\n        out_grid = convert_image_np(\n            torchvision.utils.make_grid(transformed_input_tensor))\n\n        # Plot the results side-by-side\n        f, axarr = plt.subplots(1, 2)\n        axarr[0].imshow(in_grid)\n        axarr[0].set_title('Dataset Images')\n\n        axarr[1].imshow(out_grid)\n        axarr[1].set_title('Transformed Images')\n\nfor epoch in range(1, 20 + 1):\n    train(epoch)\n    test()\n\n# Visualize the STN transformation on some input batch\nvisualize_stn()\n\nplt.ioff()\nplt.show()\n\nA few ads maybe displayed for income as resources are now offered freely. 🤝🤝🤝",
    "crumbs": [
      "Pytorch",
      "Vision",
      "Spatial Transformer Networks Tutorial"
    ]
  },
  {
    "objectID": "Pytorch/Introduction/0.quickstart_tutorial.html",
    "href": "Pytorch/Introduction/0.quickstart_tutorial.html",
    "title": "Quickstart",
    "section": "",
    "text": "Photo by DATAIDEA\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\nPyTorch offers domain-specific libraries such as TorchText, TorchVision, and TorchAudio, all of which include datasets. For this tutorial, we will be using a TorchVision dataset.\nThe torchvision.datasets module contains Dataset objects for many real-world vision data like CIFAR, COCO (full list here). In this tutorial, we use the FashionMNIST dataset. Every TorchVision Dataset includes two arguments: transform and target_transform to modify the samples and labels respectively.\n# Download training data from open datasets.\n# set download=True to download the data if it doesn't exist already.\ntraining_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=True,\n    download=False,\n    transform=ToTensor(),\n)\n\n# Download test data from open datasets.\ntest_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=False,\n    download=False,\n    transform=ToTensor(),\n)\nWe pass the Dataset as an argument to DataLoader. This wraps an iterable over our dataset, and supports automatic batching, sampling, shuffling and multiprocess data loading. Here we define a batch size of 64, i.e. each element in the dataloader iterable will return a batch of 64 features and labels.\nbatch_size = 64\n\n# Create data loaders.\ntrain_dataloader = DataLoader(training_data, batch_size=batch_size)\ntest_dataloader = DataLoader(test_data, batch_size=batch_size)\n\nfor X, y in test_dataloader:\n    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n    print(f\"Shape of y: {y.shape} {y.dtype}\")\n    break\n\nShape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\nShape of y: torch.Size([64]) torch.int64\nRead more about loading data in PyTorch.",
    "crumbs": [
      "Pytorch",
      "Introduction",
      "Quickstart"
    ]
  },
  {
    "objectID": "Pytorch/Introduction/0.quickstart_tutorial.html#creating-models",
    "href": "Pytorch/Introduction/0.quickstart_tutorial.html#creating-models",
    "title": "Quickstart",
    "section": "Creating Models",
    "text": "Creating Models\nTo define a neural network in PyTorch, we create a class that inherits from nn.Module. We define the layers of the network in the __init__ function and specify how data will pass through the network in the forward function. To accelerate operations in the neural network, we move it to the GPU or MPS if available.\n\n# Get cpu, gpu or mps device for training.\ndevice = (\n    \"cuda\"\n    if torch.cuda.is_available()\n    else \"mps\"\n    if torch.backends.mps.is_available()\n    else \"cpu\"\n)\nprint(f\"Using {device} device\")\n\n# Define model\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10)\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\nmodel = NeuralNetwork().to(device)\nprint(model)\n\nUsing cpu device\nNeuralNetwork(\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (linear_relu_stack): Sequential(\n    (0): Linear(in_features=784, out_features=512, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=512, out_features=512, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=512, out_features=10, bias=True)\n  )\n)\n\n\nRead more about building neural networks in PyTorch.",
    "crumbs": [
      "Pytorch",
      "Introduction",
      "Quickstart"
    ]
  },
  {
    "objectID": "Pytorch/Introduction/0.quickstart_tutorial.html#optimizing-the-model-parameters",
    "href": "Pytorch/Introduction/0.quickstart_tutorial.html#optimizing-the-model-parameters",
    "title": "Quickstart",
    "section": "Optimizing the Model Parameters",
    "text": "Optimizing the Model Parameters\nTo train a model, we need a loss function and an optimizer.\n\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n\nIn a single training loop, the model makes predictions on the training dataset (fed to it in batches), and backpropagates the prediction error to adjust the model’s parameters.\n\ndef train(dataloader, model, loss_fn, optimizer):\n    size = len(dataloader.dataset)\n    model.train()\n    for batch, (X, y) in enumerate(dataloader):\n        X, y = X.to(device), y.to(device)\n\n        # Compute prediction error\n        pred = model(X)\n        loss = loss_fn(pred, y)\n\n        # Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), (batch + 1) * len(X)\n            print(f\"loss: {loss:&gt;7f}  [{current:&gt;5d}/{size:&gt;5d}]\")\n\nWe also check the model’s performance against the test dataset to ensure it is learning.\n\ndef test(dataloader, model, loss_fn):\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    model.eval()\n    test_loss, correct = 0, 0\n    with torch.no_grad():\n        for X, y in dataloader:\n            X, y = X.to(device), y.to(device)\n            pred = model(X)\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n    test_loss /= num_batches\n    correct /= size\n    print(f\"Test Error: \\n Accuracy: {(100*correct):&gt;0.1f}%, Avg loss: {test_loss:&gt;8f} \\n\")\n\nThe training process is conducted over several iterations (epochs). During each epoch, the model learns parameters to make better predictions. We print the model’s accuracy and loss at each epoch; we’d like to see the accuracy increase and the loss decrease with every epoch.\n\nepochs = 5\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train(train_dataloader, model, loss_fn, optimizer)\n    test(test_dataloader, model, loss_fn)\nprint(\"Done!\")\n\nEpoch 1\n-------------------------------\nloss: 2.300198  [   64/60000]\nloss: 2.292366  [ 6464/60000]\nloss: 2.265211  [12864/60000]\nloss: 2.258336  [19264/60000]\nloss: 2.251250  [25664/60000]\nloss: 2.206823  [32064/60000]\nloss: 2.215364  [38464/60000]\nloss: 2.174910  [44864/60000]\nloss: 2.177999  [51264/60000]\nloss: 2.133787  [57664/60000]\nTest Error: \n Accuracy: 44.3%, Avg loss: 2.135173 \n\nEpoch 2\n-------------------------------\nloss: 2.150207  [   64/60000]\nloss: 2.143853  [ 6464/60000]\nloss: 2.078705  [12864/60000]\nloss: 2.092776  [19264/60000]\nloss: 2.046199  [25664/60000]\nloss: 1.973916  [32064/60000]\nloss: 1.999204  [38464/60000]\n\n\nRead more about Training your model.",
    "crumbs": [
      "Pytorch",
      "Introduction",
      "Quickstart"
    ]
  },
  {
    "objectID": "Pytorch/Introduction/0.quickstart_tutorial.html#saving-models",
    "href": "Pytorch/Introduction/0.quickstart_tutorial.html#saving-models",
    "title": "Quickstart",
    "section": "Saving Models",
    "text": "Saving Models\nA common way to save a model is to serialize the internal state dictionary (containing the model parameters).\n\ntorch.save(model.state_dict(), \"quickstart_model.pth\")\nprint(\"Saved PyTorch Model State to model.pth\")",
    "crumbs": [
      "Pytorch",
      "Introduction",
      "Quickstart"
    ]
  },
  {
    "objectID": "Pytorch/Introduction/0.quickstart_tutorial.html#loading-models",
    "href": "Pytorch/Introduction/0.quickstart_tutorial.html#loading-models",
    "title": "Quickstart",
    "section": "Loading Models",
    "text": "Loading Models\nThe process for loading a model includes re-creating the model structure and loading the state dictionary into it.\n\nmodel = NeuralNetwork().to(device)\nmodel.load_state_dict(torch.load(\"quickstart_model.pth\"))\n\nThis model can now be used to make predictions.\n\nclasses = [\n    \"T-shirt/top\",\n    \"Trouser\",\n    \"Pullover\",\n    \"Dress\",\n    \"Coat\",\n    \"Sandal\",\n    \"Shirt\",\n    \"Sneaker\",\n    \"Bag\",\n    \"Ankle boot\",\n]\n\nmodel.eval()\nx, y = test_data[0][0], test_data[0][1]\nwith torch.no_grad():\n    x = x.to(device)\n    pred = model(x)\n    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')\n\nRead more about Saving & Loading your model.\nA few ads maybe displayed for income as resources are now offered freely. 🤝🤝🤝",
    "crumbs": [
      "Pytorch",
      "Introduction",
      "Quickstart"
    ]
  },
  {
    "objectID": "Pytorch/Introduction/1.tensorqs_tutorial.html",
    "href": "Pytorch/Introduction/1.tensorqs_tutorial.html",
    "title": "Tensors",
    "section": "",
    "text": "Photo by DATAIDEA\nimport torch\nimport numpy as np",
    "crumbs": [
      "Pytorch",
      "Introduction",
      "Tensors"
    ]
  },
  {
    "objectID": "Pytorch/Introduction/1.tensorqs_tutorial.html#initializing-a-tensor",
    "href": "Pytorch/Introduction/1.tensorqs_tutorial.html#initializing-a-tensor",
    "title": "Tensors",
    "section": "Initializing a Tensor",
    "text": "Initializing a Tensor\nTensors can be initialized in various ways. Take a look at the following examples:\nDirectly from data\nTensors can be created directly from data. The data type is automatically inferred.\n\ndata = [[1, 2],[3, 4]]\nx_data = torch.tensor(data)\n\nFrom a NumPy array\nTensors can be created from NumPy arrays (and vice versa - see bridge-to-np-label).\n\nnp_array = np.array(data)\nx_np = torch.from_numpy(np_array)\n\nFrom another tensor:\nThe new tensor retains the properties (shape, datatype) of the argument tensor, unless explicitly overridden.\n\nx_ones = torch.ones_like(x_data) # retains the properties of x_data\nprint(f\"Ones Tensor: \\n {x_ones} \\n\")\n\nx_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\nprint(f\"Random Tensor: \\n {x_rand} \\n\")\n\nWith random or constant values:\nshape is a tuple of tensor dimensions. In the functions below, it determines the dimensionality of the output tensor.\n\nshape = (2,3,)\nrand_tensor = torch.rand(shape)\nones_tensor = torch.ones(shape)\nzeros_tensor = torch.zeros(shape)\n\nprint(f\"Random Tensor: \\n {rand_tensor} \\n\")\nprint(f\"Ones Tensor: \\n {ones_tensor} \\n\")\nprint(f\"Zeros Tensor: \\n {zeros_tensor}\")",
    "crumbs": [
      "Pytorch",
      "Introduction",
      "Tensors"
    ]
  },
  {
    "objectID": "Pytorch/Introduction/1.tensorqs_tutorial.html#attributes-of-a-tensor",
    "href": "Pytorch/Introduction/1.tensorqs_tutorial.html#attributes-of-a-tensor",
    "title": "Tensors",
    "section": "Attributes of a Tensor",
    "text": "Attributes of a Tensor\nTensor attributes describe their shape, datatype, and the device on which they are stored.\n\ntensor = torch.rand(3,4)\n\nprint(f\"Shape of tensor: {tensor.shape}\")\nprint(f\"Datatype of tensor: {tensor.dtype}\")\nprint(f\"Device tensor is stored on: {tensor.device}\")",
    "crumbs": [
      "Pytorch",
      "Introduction",
      "Tensors"
    ]
  },
  {
    "objectID": "Pytorch/Introduction/1.tensorqs_tutorial.html#operations-on-tensors",
    "href": "Pytorch/Introduction/1.tensorqs_tutorial.html#operations-on-tensors",
    "title": "Tensors",
    "section": "Operations on Tensors",
    "text": "Operations on Tensors\nOver 100 tensor operations, including arithmetic, linear algebra, matrix manipulation (transposing, indexing, slicing), sampling and more are comprehensively described here_.\nEach of these operations can be run on the GPU (at typically higher speeds than on a CPU). If you’re using Colab, allocate a GPU by going to Runtime &gt; Change runtime type &gt; GPU.\nBy default, tensors are created on the CPU. We need to explicitly move tensors to the GPU using .to method (after checking for GPU availability). Keep in mind that copying large tensors across devices can be expensive in terms of time and memory!\n\n# We move our tensor to the GPU if available\nif torch.cuda.is_available():\n    tensor = tensor.to(\"cuda\")\n\nTry out some of the operations from the list. If you’re familiar with the NumPy API, you’ll find the Tensor API a breeze to use.\nStandard numpy-like indexing and slicing:\n\ntensor = torch.ones(4, 4)\nprint(f\"First row: {tensor[0]}\")\nprint(f\"First column: {tensor[:, 0]}\")\nprint(f\"Last column: {tensor[..., -1]}\")\ntensor[:,1] = 0\nprint(tensor)\n\nJoining tensors You can use torch.cat to concatenate a sequence of tensors along a given dimension. See also torch.stack_, another tensor joining option that is subtly different from torch.cat.\n\nt1 = torch.cat([tensor, tensor, tensor], dim=1)\nprint(t1)\n\nArithmetic operations\n\n# This computes the matrix multiplication between two tensors. y1, y2, y3 will have the same value\n# ``tensor.T`` returns the transpose of a tensor\ny1 = tensor @ tensor.T\ny2 = tensor.matmul(tensor.T)\n\ny3 = torch.rand_like(y1)\ntorch.matmul(tensor, tensor.T, out=y3)\n\n\n# This computes the element-wise product. z1, z2, z3 will have the same value\nz1 = tensor * tensor\nz2 = tensor.mul(tensor)\n\nz3 = torch.rand_like(tensor)\ntorch.mul(tensor, tensor, out=z3)\n\nSingle-element tensors If you have a one-element tensor, for example by aggregating all values of a tensor into one value, you can convert it to a Python numerical value using item():\n\nagg = tensor.sum()\nagg_item = agg.item()\nprint(agg_item, type(agg_item))\n\nIn-place operations Operations that store the result into the operand are called in-place. They are denoted by a _ suffix. For example: x.copy_(y), x.t_(), will change x.\n\nprint(f\"{tensor} \\n\")\ntensor.add_(5)\nprint(tensor)\n\n\n\nNote\n\n\nIn-place operations save some memory, but can be problematic when computing derivatives because of an immediate loss of history. Hence, their use is discouraged.",
    "crumbs": [
      "Pytorch",
      "Introduction",
      "Tensors"
    ]
  },
  {
    "objectID": "Pytorch/Introduction/1.tensorqs_tutorial.html#bridge-with-numpy",
    "href": "Pytorch/Introduction/1.tensorqs_tutorial.html#bridge-with-numpy",
    "title": "Tensors",
    "section": "Bridge with NumPy",
    "text": "Bridge with NumPy\nTensors on the CPU and NumPy arrays can share their underlying memory locations, and changing one will change the other.\n\nTensor to NumPy array\n\nt = torch.ones(5)\nprint(f\"t: {t}\")\nn = t.numpy()\nprint(f\"n: {n}\")\n\nA change in the tensor reflects in the NumPy array.\n\nt.add_(1)\nprint(f\"t: {t}\")\nprint(f\"n: {n}\")\n\n\n\nNumPy array to Tensor\n\nn = np.ones(5)\nt = torch.from_numpy(n)\n\nChanges in the NumPy array reflects in the tensor.\n\nnp.add(n, 1, out=n)\nprint(f\"t: {t}\")\nprint(f\"n: {n}\")\n\nA few ads maybe displayed for income as resources are now offered freely. 🤝🤝🤝",
    "crumbs": [
      "Pytorch",
      "Introduction",
      "Tensors"
    ]
  },
  {
    "objectID": "Pytorch/Introduction/7.saveloadrun_tutorial.html",
    "href": "Pytorch/Introduction/7.saveloadrun_tutorial.html",
    "title": "Save and Load the Model",
    "section": "",
    "text": "Photo by DATAIDEA\nimport torch\nimport torchvision.models as models",
    "crumbs": [
      "Pytorch",
      "Introduction",
      "Save and Load the Model"
    ]
  },
  {
    "objectID": "Pytorch/Introduction/7.saveloadrun_tutorial.html#saving-and-loading-model-weights",
    "href": "Pytorch/Introduction/7.saveloadrun_tutorial.html#saving-and-loading-model-weights",
    "title": "Save and Load the Model",
    "section": "Saving and Loading Model Weights",
    "text": "Saving and Loading Model Weights\nPyTorch models store the learned parameters in an internal state dictionary, called state_dict. These can be persisted via the torch.save method:\n\nmodel = models.vgg16(weights='IMAGENET1K_V1')\ntorch.save(model.state_dict(), 'model_weights.pth')\n\nTo load model weights, you need to create an instance of the same model first, and then load the parameters using load_state_dict() method.\n\nmodel = models.vgg16() # we do not specify weights, i.e. create untrained model\nmodel.load_state_dict(torch.load('model_weights.pth'))\nmodel.eval()\n\n\n\nNote\n\n\nbe sure to call model.eval() method before inferencing to set the dropout and batch normalization layers to evaluation mode. Failing to do this will yield inconsistent inference results.",
    "crumbs": [
      "Pytorch",
      "Introduction",
      "Save and Load the Model"
    ]
  },
  {
    "objectID": "Pytorch/Introduction/7.saveloadrun_tutorial.html#saving-and-loading-models-with-shapes",
    "href": "Pytorch/Introduction/7.saveloadrun_tutorial.html#saving-and-loading-models-with-shapes",
    "title": "Save and Load the Model",
    "section": "Saving and Loading Models with Shapes",
    "text": "Saving and Loading Models with Shapes\nWhen loading model weights, we needed to instantiate the model class first, because the class defines the structure of a network. We might want to save the structure of this class together with the model, in which case we can pass model (and not model.state_dict()) to the saving function:\n\ntorch.save(model, 'model.pth')\n\nWe can then load the model like this:\n\nmodel = torch.load('model.pth')\n\n\n\nNote\n\n\nThis approach uses Python pickle module when serializing the model, thus it relies on the actual class definition to be available when loading the model.",
    "crumbs": [
      "Pytorch",
      "Introduction",
      "Save and Load the Model"
    ]
  },
  {
    "objectID": "Pytorch/Introduction/7.saveloadrun_tutorial.html#related-tutorials",
    "href": "Pytorch/Introduction/7.saveloadrun_tutorial.html#related-tutorials",
    "title": "Save and Load the Model",
    "section": "Related Tutorials",
    "text": "Related Tutorials\nSaving and Loading a General Checkpoint in PyTorch\nA few ads maybe displayed for income as resources are now offered freely. 🤝🤝🤝",
    "crumbs": [
      "Pytorch",
      "Introduction",
      "Save and Load the Model"
    ]
  },
  {
    "objectID": "Pytorch/Introduction/3.transforms_tutorial.html",
    "href": "Pytorch/Introduction/3.transforms_tutorial.html",
    "title": "Transforms",
    "section": "",
    "text": "Photo by DATAIDEA\nimport torch\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor, Lambda\n\n# set download=True to download the data\nds = datasets.FashionMNIST(\n    root=\"data\",\n    train=True,\n    download=False,\n    transform=ToTensor(),\n    target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n)",
    "crumbs": [
      "Pytorch",
      "Introduction",
      "Transforms"
    ]
  },
  {
    "objectID": "Pytorch/Introduction/3.transforms_tutorial.html#totensor",
    "href": "Pytorch/Introduction/3.transforms_tutorial.html#totensor",
    "title": "Transforms",
    "section": "ToTensor()",
    "text": "ToTensor()\nToTensor converts a PIL image or NumPy ndarray into a FloatTensor. and scales the image’s pixel intensity values in the range [0., 1.]",
    "crumbs": [
      "Pytorch",
      "Introduction",
      "Transforms"
    ]
  },
  {
    "objectID": "Pytorch/Introduction/3.transforms_tutorial.html#lambda-transforms",
    "href": "Pytorch/Introduction/3.transforms_tutorial.html#lambda-transforms",
    "title": "Transforms",
    "section": "Lambda Transforms",
    "text": "Lambda Transforms\nLambda transforms apply any user-defined lambda function. Here, we define a function to turn the integer into a one-hot encoded tensor. It first creates a zero tensor of size 10 (the number of labels in our dataset) and calls scatter_ which assigns a value=1 on the index as given by the label y.\n\ntarget_transform = Lambda(lambda y: torch.zeros(\n    10, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))\n\n\n\nFurther Reading\n\ntorchvision.transforms API\n\nA few ads maybe displayed for income as resources are now offered freely. 🤝🤝🤝",
    "crumbs": [
      "Pytorch",
      "Introduction",
      "Transforms"
    ]
  },
  {
    "objectID": "Pytorch/Text/06_seq2seq_translation_tutorial.html",
    "href": "Pytorch/Text/06_seq2seq_translation_tutorial.html",
    "title": "NLP From Scratch: Translation with a Sequence to Sequence Network and Attention",
    "section": "",
    "text": "Photo by DATAIDEA\n# For tips on running notebooks in Google Colab, see\n# https://pytorch.org/tutorials/beginner/colab",
    "crumbs": [
      "Pytorch",
      "Text",
      "NLP From Scratch: Translation with a Sequence to Sequence Network and Attention"
    ]
  },
  {
    "objectID": "Pytorch/Text/06_seq2seq_translation_tutorial.html#preparing-training-data",
    "href": "Pytorch/Text/06_seq2seq_translation_tutorial.html#preparing-training-data",
    "title": "NLP From Scratch: Translation with a Sequence to Sequence Network and Attention",
    "section": "Preparing Training Data",
    "text": "Preparing Training Data\nTo train, for each pair we will need an input tensor (indexes of the words in the input sentence) and target tensor (indexes of the words in the target sentence). While creating these vectors we will append the EOS token to both sequences.\n\ndef indexesFromSentence(lang, sentence):\n    return [lang.word2index[word] for word in sentence.split(' ')]\n\ndef tensorFromSentence(lang, sentence):\n    indexes = indexesFromSentence(lang, sentence)\n    indexes.append(EOS_token)\n    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n\ndef tensorsFromPair(pair):\n    input_tensor = tensorFromSentence(input_lang, pair[0])\n    target_tensor = tensorFromSentence(output_lang, pair[1])\n    return (input_tensor, target_tensor)\n\ndef get_dataloader(batch_size):\n    input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n\n    n = len(pairs)\n    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n\n    for idx, (inp, tgt) in enumerate(pairs):\n        inp_ids = indexesFromSentence(input_lang, inp)\n        tgt_ids = indexesFromSentence(output_lang, tgt)\n        inp_ids.append(EOS_token)\n        tgt_ids.append(EOS_token)\n        input_ids[idx, :len(inp_ids)] = inp_ids\n        target_ids[idx, :len(tgt_ids)] = tgt_ids\n\n    train_data = TensorDataset(torch.LongTensor(input_ids).to(device),\n                               torch.LongTensor(target_ids).to(device))\n\n    train_sampler = RandomSampler(train_data)\n    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n    return input_lang, output_lang, train_dataloader",
    "crumbs": [
      "Pytorch",
      "Text",
      "NLP From Scratch: Translation with a Sequence to Sequence Network and Attention"
    ]
  },
  {
    "objectID": "Pytorch/Text/better_transformer_with_torchtext_demo.html",
    "href": "Pytorch/Text/better_transformer_with_torchtext_demo.html",
    "title": "System Information",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\n#Load torchtext and initialize XLM-R model\n\nimport torch\nimport torch.nn as nn\nimport torchtext\n\nfrom torchtext.models import RobertaClassificationHead\nfrom torchtext.functional import to_tensor\n\nxlmr_large = torchtext.models.XLMR_LARGE_ENCODER\nclassifier_head = torchtext.models.RobertaClassificationHead(num_classes=2, input_dim = 1024)\nmodel = xlmr_large.get_model(head=classifier_head)\n\n# Put model into inference mode (reduces runtime even without BT - esp for GPU execution, required for Better Transformer)\nmodel.eval()\n\n# Define input transform\ntransform = xlmr_large.transform()\n\n\nimport platform\n\nDEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\ncpu = platform.processor()\ngpu = torch.cuda.get_device_name(DEVICE)\n\nprint(f\"torch version: {torch.__version__}\")\nprint(f\"torch cuda available: {torch.cuda.is_available()}\")\nprint(f\"CPU type: {cpu}\")\nprint(f\"GPU type: {gpu}\")\n\ntorch version: 1.12.0+cu113\ntorch cuda available: True\nCPU type: x86_64\nGPU type: Tesla T4\n\n\n\nCheck default sparsity support setting\nSparsity support enables transformers to skip padding in inputs.\n\nmodel.encoder.transformer.layers.enable_nested_tensor\n\nFalse\n\n\n\n\nBenchmark setup\n###Define inputs\n\nsmall_input_batch = [\n               \"Hello world\",\n               \"How are you!\"\n]\nbig_input_batch = [\n               \"Hello world\",\n               \"How are you!\",\n               \"\"\"`Well, Prince, so Genoa and Lucca are now just family estates of the\nBuonapartes. But I warn you, if you don't tell me that this means war,\nif you still try to defend the infamies and horrors perpetrated by\nthat Antichrist- I really believe he is Antichrist- I will have\nnothing more to do with you and you are no longer my friend, no longer\nmy 'faithful slave,' as you call yourself! But how do you do? I see\nI have frightened you- sit down and tell me all the news.`\n\nIt was in July, 1805, and the speaker was the well-known Anna\nPavlovna Scherer, maid of honor and favorite of the Empress Marya\nFedorovna. With these words she greeted Prince Vasili Kuragin, a man\nof high rank and importance, who was the first to arrive at her\nreception. Anna Pavlovna had had a cough for some days. She was, as\nshe said, suffering from la grippe; grippe being then a new word in\nSt. Petersburg, used only by the elite.\"\"\"\n]\n\n###Select small or big input set\nModify the assignment to input_batch below to select either the small_input_batch or big_inoput_batch, or substitute your own inputs.\n\ninput_batch=big_input_batch\n\nmodel_input = to_tensor(transform(input_batch), padding_value=1)\noutput = model(model_input)\noutput.shape\n\ntorch.Size([3, 2])\n\n\n###Iteration count for performance measurements\n\nITERATIONS=10\n\n#Measure CPU performance with slow and fast path, without and with sparsity\nSparsity support enables transformers to skip padding in inputs.\n\nCPU performance without BT sparsity\n\nmodel.encoder.transformer.layers.enable_nested_tensor = False\n\n\nprint(\"slow path:\")\nprint(\"==========\")\nwith torch.autograd.profiler.profile(use_cuda=False) as prof:\n  for i in range(ITERATIONS):\n    output = model(model_input)\nprint(prof.key_averages(group_by_stack_n=5).table(sort_by=\"self_cpu_time_total\", row_limit=5))\n\nprint(\"fast path:\")\nprint(\"==========\")\nwith torch.autograd.profiler.profile(use_cuda=False) as prof:\n  with torch.no_grad():\n    for i in range(ITERATIONS):\n      output = model(model_input)\nprint(prof.key_averages(group_by_stack_n=5).table(sort_by=\"self_cpu_time_total\", row_limit=5))\n\nslow path:\n==========\n---------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                       Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n---------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                aten::addmm        63.40%       41.691s        64.59%       42.478s      57.403ms           740  \n                   aten::mm        21.27%       13.990s        21.27%       13.990s      58.291ms           240  \n                aten::copy_         3.58%        2.356s         3.58%        2.356s     875.793us          2690  \n                  aten::bmm         2.75%        1.811s         2.75%        1.811s       7.547ms           240  \n             aten::_softmax         2.40%        1.578s         2.40%        1.578s       6.573ms           240  \n---------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \nSelf CPU time total: 65.761s\n\nfast path:\n==========\n----------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                                    Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n----------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                             aten::addmm        34.35%       23.131s        34.73%       23.386s      46.772ms           500  \n                 aten::_addmm_activation        27.26%       18.353s        29.42%       19.811s      82.548ms           240  \n                                aten::mm        20.61%       13.875s        20.61%       13.875s      57.811ms           240  \n                   aten::_masked_softmax         6.52%        4.392s         6.53%        4.395s      18.311ms           240  \n                               aten::bmm         4.33%        2.914s         4.33%        2.915s       6.072ms           480  \n----------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \nSelf CPU time total: 67.331s\n\n\n\n###CPU performance with BT sparsity\n\nmodel.encoder.transformer.layers.enable_nested_tensor = True\n\n\nprint(\"slow path:\")\nprint(\"==========\")\nwith torch.autograd.profiler.profile(use_cuda=False) as prof:\n  for i in range(ITERATIONS):\n    output = model(model_input)\nprint(prof.key_averages(group_by_stack_n=5).table(sort_by=\"self_cpu_time_total\", row_limit=5))\n\nprint(\"fast path:\")\nprint(\"==========\")\nwith torch.autograd.profiler.profile(use_cuda=False) as prof:\n  with torch.no_grad():\n    for i in range(ITERATIONS):\n      output = model(model_input)\nprint(prof.key_averages(group_by_stack_n=5).table(sort_by=\"self_cpu_time_total\", row_limit=5))\n\nslow path:\n==========\n---------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                       Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n---------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                aten::addmm        63.66%       41.845s        64.78%       42.575s      57.533ms           740  \n                   aten::mm        21.40%       14.067s        21.40%       14.067s      58.614ms           240  \n                aten::copy_         3.39%        2.228s         3.39%        2.228s     828.338us          2690  \n                  aten::bmm         2.77%        1.824s         2.77%        1.824s       7.599ms           240  \n             aten::_softmax         2.29%        1.507s         2.29%        1.507s       6.281ms           240  \n---------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \nSelf CPU time total: 65.727s\n\nfast path:\n==========\n----------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                                    Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n----------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                             aten::addmm        26.80%        8.664s        27.04%        8.743s      17.487ms           500  \n                 aten::_addmm_activation        21.29%        6.884s        22.87%        7.395s      30.814ms           240  \n                                aten::mm        15.88%        5.134s        15.88%        5.134s      21.394ms           240  \n                   aten::_masked_softmax        12.97%        4.194s        12.98%        4.195s      17.480ms           240  \n                               aten::bmm         8.82%        2.852s         8.82%        2.852s       5.941ms           480  \n----------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \nSelf CPU time total: 32.331s\n\n\n\n/usr/local/lib/python3.7/dist-packages/torch/nn/modules/transformer.py:232: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at  ../aten/src/ATen/NestedTensorImpl.cpp:50.)\n  output = torch._nested_tensor_from_mask(output, src_key_padding_mask.logical_not())\n\n\n#Measure DEVICE performance with slow and fast path, without and with sparsity\nPlease ensure that the runtime has GPUs enabled to see the performance benefits of Better Transformer fastpath execution on GPUs. You can confirm and change the Runtime type in the Google Colab menu with (Runtime &gt; Change Runtime Type)\n\nmodel.to(DEVICE)\nmodel.eval()\nmodel_input = model_input.to(DEVICE)\n\n\n\nDEVICE performance without BT sparsity\n\nmodel.encoder.transformer.layers.enable_nested_tensor=False\n\n\nprint(\"slow path:\")\nprint(\"==========\")\nwith torch.autograd.profiler.profile(use_cuda=True) as prof:\n  for i in range(ITERATIONS):\n    output = model(model_input)\nprint(prof.key_averages(group_by_stack_n=5).table(sort_by=\"self_cuda_time_total\", row_limit=5))\n\nprint(\"fast path:\")\nprint(\"==========\")\nwith torch.autograd.profiler.profile(use_cuda=True) as prof:\n  with torch.no_grad():\n    for i in range(ITERATIONS):\n      output = model(model_input)\nprint(prof.key_averages(group_by_stack_n=5).table(sort_by=\"self_cuda_time_total\", row_limit=5))\n\nslow path:\n==========\n-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                                               aten::mm         1.01%      14.178ms        44.20%     622.315ms       2.593ms     859.598ms        43.74%     859.598ms       3.582ms           240  \n                                            aten::addmm         3.29%      46.383ms         5.19%      73.045ms      98.709us     756.916ms        38.52%     761.477ms       1.029ms           740  \n                                            aten::copy_         1.26%      17.748ms         2.22%      31.256ms      18.386us      49.933ms         2.54%      49.933ms      29.372us          1700  \n                                          aten::baddbmm         0.89%      12.571ms         1.43%      20.191ms      84.129us      42.811ms         2.18%      57.249ms     238.537us           240  \n                                              aten::bmm         0.45%       6.283ms         0.54%       7.586ms      31.608us      31.677ms         1.61%      31.677ms     131.988us           240  \n-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \nSelf CPU time total: 1.408s\nSelf CUDA time total: 1.965s\n\nfast path:\n==========\n-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                                            aten::addmm         2.10%      25.898ms         4.60%      56.673ms     113.346us     434.598ms        33.15%     437.716ms     875.432us           500  \n                                aten::_addmm_activation         1.15%      14.139ms         1.89%      23.213ms      96.721us     350.782ms        26.75%     375.835ms       1.566ms           240  \n                                               aten::mm         0.70%       8.600ms         1.00%      12.305ms      51.271us     261.590ms        19.95%     261.590ms       1.090ms           240  \n                                              aten::bmm         0.82%      10.038ms         1.03%      12.653ms      26.360us      64.215ms         4.90%      64.215ms     133.781us           480  \n                                  aten::_masked_softmax         0.46%       5.717ms         0.92%      11.280ms      47.000us      29.636ms         2.26%      31.117ms     129.654us           240  \n-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \nSelf CPU time total: 1.231s\nSelf CUDA time total: 1.311s\n\n\n\n\n\nDEVICE performance performance with BT sparsity\n\nmodel.encoder.transformer.layers.enable_nested_tensor = True\n\n\nmodel.to(DEVICE)\nmodel_input = model_input.to(DEVICE)\n\nprint(\"slow path:\")\nprint(\"==========\")\nwith torch.autograd.profiler.profile(use_cuda=True) as prof:\n  for i in range(ITERATIONS):\n    output = model(model_input)\nprint(prof.key_averages(group_by_stack_n=5).table(sort_by=\"self_cuda_time_total\", row_limit=5))\n\nprint(\"fast path:\")\nprint(\"==========\")\nwith torch.autograd.profiler.profile(use_cuda=True) as prof:\n  with torch.no_grad():\n    for i in range(ITERATIONS):\n      output = model(model_input)\nprint(prof.key_averages(group_by_stack_n=5).table(sort_by=\"self_cuda_time_total\", row_limit=5))\n\nslow path:\n==========\n-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                                            aten::addmm         6.15%      48.067ms         9.02%      70.473ms      95.234us     764.116ms        55.84%     768.719ms       1.039ms           740  \n                                               aten::mm         1.40%      10.924ms         1.91%      14.890ms      62.042us     254.936ms        18.63%     254.936ms       1.062ms           240  \n                                            aten::copy_         2.24%      17.520ms         3.92%      30.668ms      18.040us      50.111ms         3.66%      50.111ms      29.477us          1700  \n                                          aten::baddbmm         1.61%      12.555ms         2.50%      19.545ms      81.438us      42.793ms         3.13%      57.277ms     238.654us           240  \n                                              aten::bmm         0.74%       5.805ms         0.89%       6.942ms      28.925us      31.709ms         2.32%      31.709ms     132.121us           240  \n-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \nSelf CPU time total: 781.406ms\nSelf CUDA time total: 1.368s\n\nfast path:\n==========\n-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                                            aten::addmm         3.54%      25.463ms         5.47%      39.314ms      78.628us     161.586ms        21.55%     164.918ms     329.836us           500  \n                                aten::_addmm_activation         2.00%      14.361ms         3.22%      23.155ms      96.479us     132.618ms        17.68%     141.226ms     588.442us           240  \n                                               aten::mm         1.23%       8.851ms         1.76%      12.657ms      52.737us      92.039ms        12.27%      92.039ms     383.496us           240  \n                                              aten::bmm         1.48%      10.635ms         1.87%      13.466ms      28.054us      64.850ms         8.65%      64.850ms     135.104us           480  \n                   aten::_transformer_encoder_layer_fwd        21.83%     156.867ms        87.09%     625.735ms       2.607ms      49.826ms         6.64%     725.680ms       3.024ms           240  \n-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \nSelf CPU time total: 718.471ms\nSelf CUDA time total: 749.945ms\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Pytorch",
      "Text",
      "System Information"
    ]
  },
  {
    "objectID": "Pytorch/Text/05_seq2seq_translation_tutorial.html",
    "href": "Pytorch/Text/05_seq2seq_translation_tutorial.html",
    "title": "NLP From Scratch: Translation with a Sequence to Sequence Network and Attention",
    "section": "",
    "text": "Photo by DATAIDEA\n# For tips on running notebooks in Google Colab, see\n# https://pytorch.org/tutorials/beginner/colab",
    "crumbs": [
      "Pytorch",
      "Text",
      "NLP From Scratch: Translation with a Sequence to Sequence Network and Attention"
    ]
  },
  {
    "objectID": "Pytorch/Text/05_seq2seq_translation_tutorial.html#preparing-training-data",
    "href": "Pytorch/Text/05_seq2seq_translation_tutorial.html#preparing-training-data",
    "title": "NLP From Scratch: Translation with a Sequence to Sequence Network and Attention",
    "section": "Preparing Training Data",
    "text": "Preparing Training Data\nTo train, for each pair we will need an input tensor (indexes of the words in the input sentence) and target tensor (indexes of the words in the target sentence). While creating these vectors we will append the EOS token to both sequences.\n\ndef indexesFromSentence(lang, sentence):\n    return [lang.word2index[word] for word in sentence.split(' ')]\n\ndef tensorFromSentence(lang, sentence):\n    indexes = indexesFromSentence(lang, sentence)\n    indexes.append(EOS_token)\n    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n\ndef tensorsFromPair(pair):\n    input_tensor = tensorFromSentence(input_lang, pair[0])\n    target_tensor = tensorFromSentence(output_lang, pair[1])\n    return (input_tensor, target_tensor)\n\ndef get_dataloader(batch_size):\n    input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n\n    n = len(pairs)\n    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n\n    for idx, (inp, tgt) in enumerate(pairs):\n        inp_ids = indexesFromSentence(input_lang, inp)\n        tgt_ids = indexesFromSentence(output_lang, tgt)\n        inp_ids.append(EOS_token)\n        tgt_ids.append(EOS_token)\n        input_ids[idx, :len(inp_ids)] = inp_ids\n        target_ids[idx, :len(tgt_ids)] = tgt_ids\n\n    train_data = TensorDataset(torch.LongTensor(input_ids).to(device),\n                               torch.LongTensor(target_ids).to(device))\n\n    train_sampler = RandomSampler(train_data)\n    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n    return input_lang, output_lang, train_dataloader",
    "crumbs": [
      "Pytorch",
      "Text",
      "NLP From Scratch: Translation with a Sequence to Sequence Network and Attention"
    ]
  },
  {
    "objectID": "Pytorch/Text/04_seq2seq_translation_tutorial.html",
    "href": "Pytorch/Text/04_seq2seq_translation_tutorial.html",
    "title": "NLP From Scratch: Translation with a Sequence to Sequence Network and Attention",
    "section": "",
    "text": "Photo by DATAIDEA\n# For tips on running notebooks in Google Colab, see\n# https://pytorch.org/tutorials/beginner/colab",
    "crumbs": [
      "Pytorch",
      "Text",
      "NLP From Scratch: Translation with a Sequence to Sequence Network and Attention"
    ]
  },
  {
    "objectID": "Pytorch/Text/04_seq2seq_translation_tutorial.html#preparing-training-data",
    "href": "Pytorch/Text/04_seq2seq_translation_tutorial.html#preparing-training-data",
    "title": "NLP From Scratch: Translation with a Sequence to Sequence Network and Attention",
    "section": "Preparing Training Data",
    "text": "Preparing Training Data\nTo train, for each pair we will need an input tensor (indexes of the words in the input sentence) and target tensor (indexes of the words in the target sentence). While creating these vectors we will append the EOS token to both sequences.\n\ndef indexesFromSentence(lang, sentence):\n    return [lang.word2index[word] for word in sentence.split(' ')]\n\ndef tensorFromSentence(lang, sentence):\n    indexes = indexesFromSentence(lang, sentence)\n    indexes.append(EOS_token)\n    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n\ndef tensorsFromPair(pair):\n    input_tensor = tensorFromSentence(input_lang, pair[0])\n    target_tensor = tensorFromSentence(output_lang, pair[1])\n    return (input_tensor, target_tensor)\n\ndef get_dataloader(batch_size):\n    input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n\n    n = len(pairs)\n    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n\n    for idx, (inp, tgt) in enumerate(pairs):\n        inp_ids = indexesFromSentence(input_lang, inp)\n        tgt_ids = indexesFromSentence(output_lang, tgt)\n        inp_ids.append(EOS_token)\n        tgt_ids.append(EOS_token)\n        input_ids[idx, :len(inp_ids)] = inp_ids\n        target_ids[idx, :len(tgt_ids)] = tgt_ids\n\n    train_data = TensorDataset(torch.LongTensor(input_ids).to(device),\n                               torch.LongTensor(target_ids).to(device))\n\n    train_sampler = RandomSampler(train_data)\n    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n    return input_lang, output_lang, train_dataloader",
    "crumbs": [
      "Pytorch",
      "Text",
      "NLP From Scratch: Translation with a Sequence to Sequence Network and Attention"
    ]
  },
  {
    "objectID": "dataidea/quickstart.html",
    "href": "dataidea/quickstart.html",
    "title": "DATAIDEA Quickstart",
    "section": "",
    "text": "Photo by DATAIDEA",
    "crumbs": [
      "dataidea",
      "DATAIDEA Quickstart"
    ]
  },
  {
    "objectID": "dataidea/quickstart.html#installing-dataidea",
    "href": "dataidea/quickstart.html#installing-dataidea",
    "title": "DATAIDEA Quickstart",
    "section": "Installing dataidea",
    "text": "Installing dataidea\n\nTo install dataidea, you must have python installed on your machine\nIt’s advised that you install it in a virtual environment\nYou can install dataidea using the command below\n\npip install dataidea\n\nLearning dataidea\nThe best way to get started with dataidea (and data analysis) is to complete the free course.\nTo see what’s possible with dataidea, take a look at the Quick Start\nRead through the Tutorials to learn how to load datasets, train your own models on your own datasets. Use the navigation to look through the dataidea documentation. Every class, function, and method is documented here.\n\n\nLoading Datasets\ndataidea has loadDataset method which allows you to quickly load inbuilt datasets useful for the course. You can import loadDataset from the datasets module and use it to load your dataset\n\nfrom dataidea.datasets import loadDataset\n\nmusic_data = loadDataset('music')\nmusic_data.head()\n\n\n\n\n\n\n\n\nday\ntemperature\nwindspead\nevent\n\n\n\n\n0\n01/01/2017\n32.0\n6.0\nRain\n\n\n1\n04/01/2017\nNaN\n9.0\nSunny\n\n\n2\n05/01/2017\n28.0\nNaN\nSnow\n\n\n3\n06/01/2017\nNaN\n7.0\nNaN\n\n\n4\n07/01/2017\n32.0\nNaN\nRain\n\n\n\n\n\n\n\n\n\nSaving and loading models\nThe dataidea package offers saveModel and loadModel which allow you save and load your models while maintaining the programming priciples that we learn through the course. You can import saveModel and loadModel from the models module.\n\n# dimporting the model\nfrom sklearn.tree import DecisionTreeClassifier\n\n# setting X and y\nX = music_data.drop('genre', axis=1)\ny = music_data.genre\n\n# initializing and fitting the model\nclassifier = DecisionTreeClassifier()\nclassifier.fit(X, y)\n\nDecisionTreeClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFittedDecisionTreeClassifier() \n\n\nNow that we have trained our model, let’s try saving and loading it, and use the loaded model for predictions\n\nfrom dataidea.models import saveModel, loadModel\n\n# saving the model\nsaveModel(model=classifier, filename='genre_classifier.di')\n\n# loading the model\nloaded_model = loadModel(filename='genre_classifier.di')\n# predicting X\nloaded_model.predict(X)\n\narray(['HipHop', 'HipHop', 'HipHop', 'Jazz', 'Jazz', 'Jazz', 'Classical',\n       'Classical', 'Classical', 'Dance', 'Dance', 'Dance', 'Acoustic',\n       'Acoustic', 'Acoustic', 'Classical', 'Classical', 'Classical',\n       'Classical', 'Classical'], dtype=object)\n\n\nmore to follow…\nA few ads maybe displayed for income as resources are now offered freely. 🤝🤝🤝",
    "crumbs": [
      "dataidea",
      "DATAIDEA Quickstart"
    ]
  },
  {
    "objectID": "Python/00_outline.html",
    "href": "Python/00_outline.html",
    "title": "DATAIDEA",
    "section": "",
    "text": "Photo by DATAIDEA",
    "crumbs": [
      "Python",
      "Python Course Outline"
    ]
  },
  {
    "objectID": "Python/00_outline.html#python-course-outline",
    "href": "Python/00_outline.html#python-course-outline",
    "title": "DATAIDEA",
    "section": "Python Course Outline",
    "text": "Python Course Outline",
    "crumbs": [
      "Python",
      "Python Course Outline"
    ]
  },
  {
    "objectID": "Python/00_outline.html#basics",
    "href": "Python/00_outline.html#basics",
    "title": "DATAIDEA",
    "section": "Basics ",
    "text": "Basics \n\nPython Overview\nIntroduction\nInstalling\nWriting Code\nDisplaying Output\nStatements\nSyntax\nComments\nExercise\n\nGet Started",
    "crumbs": [
      "Python",
      "Python Course Outline"
    ]
  },
  {
    "objectID": "Python/00_outline.html#variables",
    "href": "Python/00_outline.html#variables",
    "title": "DATAIDEA",
    "section": "Variables ",
    "text": "Variables \n\nVariables\nData Types\nNumbers\nNumber Methods\nStrings\nType Conversion\nPython Booleans\nExercise\n\nGet Started",
    "crumbs": [
      "Python",
      "Python Course Outline"
    ]
  },
  {
    "objectID": "Python/00_outline.html#operations",
    "href": "Python/00_outline.html#operations",
    "title": "DATAIDEA",
    "section": "Operations ",
    "text": "Operations \n\nOperators Intro\nArithmetics\nAssignment\nComparison\nLogical\nIdentity\nMembership\nExercise\n\nGet Started",
    "crumbs": [
      "Python",
      "Python Course Outline"
    ]
  },
  {
    "objectID": "Python/00_outline.html#collections",
    "href": "Python/00_outline.html#collections",
    "title": "DATAIDEA",
    "section": "Collections ",
    "text": "Collections \n\nContainers\nList\nTuple\nSet\nDictionary\nExercise\n\nGet Started",
    "crumbs": [
      "Python",
      "Python Course Outline"
    ]
  },
  {
    "objectID": "Python/00_outline.html#flow-control",
    "href": "Python/00_outline.html#flow-control",
    "title": "DATAIDEA",
    "section": "Flow Control ",
    "text": "Flow Control \n\nFunctions\nLambda functions\nIf else\nIf else shorthand\nFor Loop\nWhile Loop\nBreak and Continue\nPass\nExercise\n\nGet Started",
    "crumbs": [
      "Python",
      "Python Course Outline"
    ]
  },
  {
    "objectID": "Python/00_outline.html#advanced",
    "href": "Python/00_outline.html#advanced",
    "title": "DATAIDEA",
    "section": "Advanced ",
    "text": "Advanced \n\nFunctions\nClasses and Objects\nInheritance\nVariable Scope\nFormatting Strings\nTry … Except\nIterators\nUser Input\nExercise\n\nGet Started",
    "crumbs": [
      "Python",
      "Python Course Outline"
    ]
  },
  {
    "objectID": "Python/00_outline.html#modules",
    "href": "Python/00_outline.html#modules",
    "title": "DATAIDEA",
    "section": "Modules ",
    "text": "Modules \n\nIntro\nMath\nRandom\nDate and Time\nJSON\nRegular Expressions\nExercise\n\nGet Started",
    "crumbs": [
      "Python",
      "Python Course Outline"
    ]
  },
  {
    "objectID": "Python/00_outline.html#working-with-files",
    "href": "Python/00_outline.html#working-with-files",
    "title": "DATAIDEA",
    "section": "Working With Files ",
    "text": "Working With Files \n\nIntro\nFile Handling\nFile Reading\nFile Writing/Creating/Appending\nFile Deleting\nExercise\n\nGet Started \n\n\nDon’t miss out on any updates and developments! Subscribe to the DATAIDEA Newsletter it’s easy and safe.",
    "crumbs": [
      "Python",
      "Python Course Outline"
    ]
  },
  {
    "objectID": "Python/09_file_handling.html",
    "href": "Python/09_file_handling.html",
    "title": "DATAIDEA",
    "section": "",
    "text": "Photo by DATAIDEA",
    "crumbs": [
      "Python",
      "Python File Handling"
    ]
  },
  {
    "objectID": "Python/09_file_handling.html#python-file-handling",
    "href": "Python/09_file_handling.html#python-file-handling",
    "title": "DATAIDEA",
    "section": "Python File Handling",
    "text": "Python File Handling\nPython allows us to read, write, create and delete files. This process is called file handling.\n\n \n\n\nThe open() function\nThe open() function allows us to read, create and update files\nIt takes 2 parameters:\n\nfile - the file or file path to be opened\nmode - the mode in which a file is opened for\n\nThe mode is a string that can either be any of the following:\n\n\n\nMode\nMeaning\n\n\n\n\n'r'\nOpen a file for reading\n\n\n'w'\nOpen a file for writing, creates the file if it does not exist\n\n\n'a'\nOpen a file for writing, appends to the end of the file\n\n\n'x'\nOpen a file for creating, fails if file already exists",
    "crumbs": [
      "Python",
      "Python File Handling"
    ]
  },
  {
    "objectID": "Python/09_file_handling.html#python-file-reading",
    "href": "Python/09_file_handling.html#python-file-reading",
    "title": "DATAIDEA",
    "section": "Python File reading",
    "text": "Python File reading\nTo better explain this, let us say we have a folder named my_folder.\nInside my_folder we have the following files:\n\ndemo.txt\nmain_code.py\n\nThe content of the demo.txt file is the following\nHello World!\nI love Python\nNow our goal is to read the content of the demo.txt file and then print it using the main_code.py file\nTo achieve this, we will use the open() function with 'r' mode.\n\n# this is main code\n\nfile = open(\n    file='demo.txt',\n    mode='r'\n)\ncontent = file.read()\nprint(content)\n\nHello World!\nI love Python\n\n\n\n\n\n\n\n\nReading Lines\nWe can also read each line using the readline() method.\n\n# this is main_code.py\n\nfile = open(\n    file='demo.txt',\n    mode='r'\n)\n\nfirst_line = file.readline()\nsecond_line = file.readline()\n\nprint('First line:', first_line)\nprint('Second line:', second_line)\n\nFirst line: Hello World!\n\nSecond line: I love Python",
    "crumbs": [
      "Python",
      "Python File Handling"
    ]
  },
  {
    "objectID": "Python/09_file_handling.html#writing-a-file",
    "href": "Python/09_file_handling.html#writing-a-file",
    "title": "DATAIDEA",
    "section": "Writing a File",
    "text": "Writing a File\nIn simplest terms, writing a file means modifying the content of a file or creating it if it doesnot exist yet.\nIn Python, there are 2 modes to write to file.\n\n'w' - overwrites content of a file, creates file if it does not exist\n'a' - appends content to the end of a file, creates the file if it does not exist\n\nExample To better explain this, lets say we have a folder named my_folder. Inside my_folder we have the following files\n\ndemo.txt\nmain_code.py\n\nThe content of the demo.txt file is the following\nI love Python\nIn this example, we will use the 'w' mode which will overwrite(replace) the content of the file\n\n# this is main_code.py\n\nfile = open(\n    file='demo.txt',\n    mode='w'\n)\nfile.write('I love JavaScript')\nfile.close()\n\nWhen the above code is run, the content of the file demo.txt will be this:\nI love JavaScript\nAnother example, this time we will use the a mode which will append or add content to the end of the file\n\n# this is main_code.py\n\nfile = open(\n    file='demo.txt',\n    mode='a'\n)\nfile.write(' and JavaScript')\nfile.close()\n\nWhen the above script is run, the content of the demo.txt file will be this:\nI love Python and JavaScript",
    "crumbs": [
      "Python",
      "Python File Handling"
    ]
  },
  {
    "objectID": "Python/09_file_handling.html#deleting-a-file",
    "href": "Python/09_file_handling.html#deleting-a-file",
    "title": "DATAIDEA",
    "section": "Deleting a file",
    "text": "Deleting a file\nTo delete a file, use the os module. The os modules contains the remove() method which we can use to delete files.\n\n# this is main_code.py\n\n# import os\n\n# os.remove('demo.txt')\n\n\n\nDon’t miss out on any updates and developments! Subscribe to the DATAIDEA Newsletter it’s easy and safe.",
    "crumbs": [
      "Python",
      "Python File Handling"
    ]
  },
  {
    "objectID": "Python/10_exercise.html",
    "href": "Python/10_exercise.html",
    "title": "Exercise",
    "section": "",
    "text": "Photo by DATAIDEA",
    "crumbs": [
      "Python",
      "Exercise"
    ]
  },
  {
    "objectID": "Python/10_exercise.html#homework",
    "href": "Python/10_exercise.html#homework",
    "title": "Exercise",
    "section": "Homework",
    "text": "Homework\n\nTheory\nPython Overview:\nWhat are the fundamental components of a Python program, and how does Python execute code?\nyour solution\nPython Variables:\nExplain the differences between mutable and immutable data types in Python and provide examples of each.\nyour solution\nPython Operations:\nDiscuss the importance of operator precedence in Python and how it impacts the evaluation of expressions.\nyour solution\nPython Collections:\nCompare and contrast the usage of lists, tuples, sets, and dictionaries in Python, highlighting their key characteristics and when to use each.\nyour solution\n\n\n\n\n\n\n\nPractical\nProblem: Pizza Party\nYou’re organizing a pizza party and want to ensure there’s enough pizza for everyone attending. Each pizza has 8 slices. Write a Python script that calculates the number of pizzas needed based on the number of guests and slices per person.\nConsider the following inputs:\n\nNumber of guests attending the party.\nSlices each guest should have (assume each guest will have the same number of slices).\n\nYour script should:\n\nPrompt the user for the number of guests attending.\nPrompt for the number of slices each guest should have.\nCalculate the total number of slices needed.\nCalculate the total number of pizzas required (round up to the nearest whole pizza).\nFor instance, if 10 guests are attending and each should have 3 slices, your program should output the number of pizzas needed to fulfill the requirement.\n\n\n# your solution\n\n\n\nDon’t miss out on any updates and developments! Subscribe to the DATAIDEA Newsletter it’s easy and safe.",
    "crumbs": [
      "Python",
      "Exercise"
    ]
  },
  {
    "objectID": "Python/08_modules.html",
    "href": "Python/08_modules.html",
    "title": "DATAIDEA",
    "section": "",
    "text": "Photo by DATAIDEA",
    "crumbs": [
      "Python",
      "Modules"
    ]
  },
  {
    "objectID": "Python/08_modules.html#modules",
    "href": "Python/08_modules.html#modules",
    "title": "DATAIDEA",
    "section": "Modules",
    "text": "Modules\nA module in Python is a python file that can contain variables, functions and classes\n\nWhy use Modules?\nModules allow us to split our code into multiple files\nInstead of writing all our codes inside a sigle Python file, we can use modules\n\n\nNote!\n\n\nThat way, our code will be easier to read, understand and maintain\n\n\n\n\nCreating a Module\nThere is nothing so special with creating a module, simply write you Python code and save it with the .py extension.\nIn this example, we have a module saved as my_module.py and it contains the following code\n\n# this is my_module.py\n\nfirst_name = 'Viola'\nlast_name = 'Akullu'\n\ndef add(number1, number2):\n    return number1 + number2\n\ndef multiply(number1, number2):\n    return number1 * number2\n\nAfter that, to use my_module.py, we need to import it.\nTo import, use the import statement and the module name.\nThen we can use the variables and functions in the module.\nIn this example, the code below is saved as main_code.py and it imports the module.py.\n\n# this is main_code.py\n\nimport my_module\n\nfull_name = my_module.first_name + my_module.last_name\nprint('Full name:', full_name)\n\nsummation = my_module.add(3, 7)\nprint('Summation:', summation)\n\nFull name: ViolaAkullu\nSummation: 10\n\n\n\n\n\n\n\n\n\nUsing Aliases\nWe can use an alias to refer to the module\nTo use an alias, use the as keyword\n\n# this is main_code.py\n\nimport my_module as mm\n\nfull_name = mm.first_name + mm.last_name\nprint('Full name:', full_name)\n\nsummation = mm.add(3, 7)\nprint('Summation:', summation)\n\nFull name: ViolaAkullu\nSummation: 10\n\n\n\n\nImporting Parts of a Module\nWe can choose to import only some specific parts of a module\n\nNote! When we import a part of a module, we will be able to use its variables and functions directly\n\nUse the from keyword to import a part of a module.\nIn this example, we will import the first_name variable and access it directly\n\nfrom my_module import first_name\n\n# now we can use it directly as \nprint(first_name)\n\nViola\n\n\n\n\n\n\n\n\n\nThe dir() Function\nThe dir() function returns a list of all the variables, functions and classes available in a module\n\nimport my_module\n\ndir_ = dir(my_module)\n\nprint(dir_)\n\n['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'add', 'first_name', 'last_name', 'multiply']\n\n\n\n\nBuilt in Modules\nPython has many useful built-in modules that we can use to make coding easier.\nBuilt-in modules can be imported without having to create them\nIn this example, we will import the sysconfig module and use its get_python_version() to return the Python version we’re using\n\nimport sysconfig\n\npython_version = sysconfig.get_python_version()\nprint(python_version)\n\n3.10",
    "crumbs": [
      "Python",
      "Modules"
    ]
  },
  {
    "objectID": "Python/08_modules.html#math-module",
    "href": "Python/08_modules.html#math-module",
    "title": "DATAIDEA",
    "section": "Math Module",
    "text": "Math Module\nThe math module gives us access to mathematical functions\nTo use the math module, import it first, then we can start using it.\nWe can use the math module to find the square root of a number using the math.sqrt() method\n\nimport math\n\nnumber = 16\nnumber_sqrt = math.sqrt(number)\n\nprint('Number:', number)\nprint('Square root of number:', number_sqrt)\n\nNumber: 16\nSquare root of number: 4.0\n\n\nWe can use the math module to get the factorial of a number by using the math.factorial() method\n\nimport math\n\nnumber = 5\nnumber_factorial = math.factorial(number)\n\nprint('Number:', number)\nprint('Factorial:', number_factorial)\n\nNumber: 5\nFactorial: 120\n\n\nThe math module also contains some constants like pi and e\n\nimport math\n\nprint('e:', math.e)\nprint('pi:', math.pi)\n\ne: 2.718281828459045\npi: 3.141592653589793\n\n\nThe math module can do those and so much more",
    "crumbs": [
      "Python",
      "Modules"
    ]
  },
  {
    "objectID": "Python/08_modules.html#random-module",
    "href": "Python/08_modules.html#random-module",
    "title": "DATAIDEA",
    "section": "Random Module",
    "text": "Random Module\nThe random module lets us generate a random number\nAs usual, to use the random module, import it first.\nWe can generate a random number that falls within a specified range by using the random.randint() method\n\nimport random\n\nrandom_integer = random.randint(1,100)\nprint('Random Integer:', random_integer)\n\nRandom Integer: 4\n\n\nWe can generate numbers from a gaussian distribution with mean (mu) as 0 and standard deviation (sigma) as 1\n\nnumbers = []\n\ncounter = 0\nwhile counter &lt; 100:\n    numbers.append(random.gauss(mu=0, sigma=1))\n    counter += 1\n    \nprint(numbers)\n\n[0.8310128735410047, 2.402375340413018, -1.2769617295659348, 0.7569506717477539, 1.6026026122392498, 1.4142936594217554, -0.3169917649104485, -0.07305941097531603, -0.7885301448554015, -0.0674611332298377, 0.28288857512573684, 0.08844216926370602, -1.249987094506388, 0.870793290313952, -0.6607737394803138, 0.3780605189691181, 0.20288623881856632, 0.8439702923769746, 1.6500270929422152, -0.5579247768953991, -0.3076290349937902, 0.8927675985413197, -2.3716599434459114, 0.23253728473684382, 0.01698634011714592, -1.506684284668113, -1.516156046117149, -0.7549199652372819, 0.4855840249497611, -1.9426218553454226, -0.5672748318805165, 1.7849639815888045, -0.4223703532919884, -1.4182523392919628, 0.3817982448773813, -1.2151583559744263, 0.21736913499460964, 0.0743448686041854, -0.6217874541247053, -0.05369712902089164, 0.06560332100098984, 0.5791279113149166, 1.5329264216964942, -1.5523813284095307, 0.256018716284597, 1.498941708596562, 0.6484203278916434, 0.956658998431066, -0.7469607705965761, 0.9093585267915438, -0.3301676177291813, -2.1020486475752564, -0.6324768823835674, -0.2621489739923403, 0.36805271395009337, -0.1987104858441708, -0.20226660046300027, -1.0227302328088852, 0.9440428943259802, 1.3499647213634605, 0.28655811659281705, -0.48212404896946465, 1.5732404576352244, 1.7024230857294205, -0.32802550098029193, 2.0808443667109597, 2.2783854541239874, -0.265626754707208, -0.04641950638081212, 0.7941371582079103, -0.36860553191079254, -0.9098450679735101, 1.234946260813307, -2.835066105841072, 1.3883254119625694, 1.2853299658795028, 1.178005875662903, 0.3186472037221876, -1.0006920744966419, -2.3745959188263885, 1.8440465299894964, -0.35610549619690796, 0.5857012223823791, 0.7400382246661824, 0.07225122970263118, -0.5508995490344698, -0.038356750477046286, -0.040997463659922434, 0.6802546773316889, -1.3861271290488735, 0.7275261286416534, 0.3729374034245036, -0.013616473457934613, -0.7620103036607296, 0.15556952852877587, -1.7898533901375224, -1.137248630020012, -1.71518120153122, -0.5817297506694047, -0.4035542913039588]",
    "crumbs": [
      "Python",
      "Modules"
    ]
  },
  {
    "objectID": "Python/08_modules.html#date-and-time",
    "href": "Python/08_modules.html#date-and-time",
    "title": "DATAIDEA",
    "section": "Date and Time",
    "text": "Date and Time\nThe datetime module allows us to work with dates\nAs usual we have to import the datetime module to be able to use it.\n\nCurrent Date and Time\nThe datetime.datetime.now() method returns the current date and time\n\nimport datetime\n\ntime_now = datetime.datetime.now()\nprint(time_now)\n\n2024-05-01 08:18:09.070054\n\n\n\n\nThe date Object\nThe date object represents a date (year, month and day)\nTo create a date object, import it from the datetime module first.\n\nfrom datetime import date\n\ntoday = date.today()\nprint('Current date:', today)\n\nCurrent date: 2024-05-01",
    "crumbs": [
      "Python",
      "Modules"
    ]
  },
  {
    "objectID": "Python/08_modules.html#json",
    "href": "Python/08_modules.html#json",
    "title": "DATAIDEA",
    "section": "JSON",
    "text": "JSON\nJSON stands for JavaScript Object Notation.\nJSON contains data that are sent or received to and from a server\nJSON is simply a string, if follows a format similar to a Python dictionary\nExample:\n\ndata = \"{'first_name': 'Juma','last_name': 'Shafara', 'age': 39}\"\n\nprint(data)\n\n{'first_name': 'Juma','last_name': 'Shafara', 'age': 39}\n\n\n\nJSON to Dictionary\nBefore we can individually access the data of a JSON, we need to convert it to a Python dictionary first.\nTo do that, we need to import the json module\n\nimport json \n\ndata = '{\"first_name\": \"Juma\",\"last_name\": \"Shafara\", \"age\": 39}'\n\n# convert to dictionary\ndata_dict = json.loads(data)\n\nprint('Fist name:',data_dict['first_name'])\nprint('Last name:', data_dict['last_name'])\nprint('Age:', data_dict['age'])\n\nFist name: Juma\nLast name: Shafara\nAge: 39\n\n\n\n\nDictionary to JSON\nTo convert a dictionay to JSON, use the json.dumps() method.\n\nimport json\n\ndata_dict = {\n    \"first_name\": \"Juma\",\n    \"last_name\": \"Shafara\", \n    \"age\": 39\n    }\n\ndata_json = json.dumps(data_dict)\n\n\n\nDon’t miss out on any updates and developments! Subscribe to the DATAIDEA Newsletter it’s easy and safe.",
    "crumbs": [
      "Python",
      "Modules"
    ]
  },
  {
    "objectID": "Python/05_containers.html",
    "href": "Python/05_containers.html",
    "title": "DATAIDEA",
    "section": "",
    "text": "Photo by DATAIDEA",
    "crumbs": [
      "Python",
      "Containers"
    ]
  },
  {
    "objectID": "Python/05_containers.html#containers",
    "href": "Python/05_containers.html#containers",
    "title": "DATAIDEA",
    "section": "Containers",
    "text": "Containers\nContainers are objects that contain other objects\n\nWhat is an object?\nIn python, everything is an object. Even the simplest strings and numbers are considered as objects",
    "crumbs": [
      "Python",
      "Containers"
    ]
  },
  {
    "objectID": "Python/05_containers.html#lists",
    "href": "Python/05_containers.html#lists",
    "title": "DATAIDEA",
    "section": "Lists",
    "text": "Lists\n\n \n\n\nA python list is an ordered container\nA list is created by using square brackets ([])\nObjects are poaced inside those brackets and are separated by commas (,)\n\n\npets = ['dog', 'cat', 'rabbit', 'monkey']\nprint(pets)\nprint(type(pets))\n\n['dog', 'cat', 'rabbit', 'monkey']\n&lt;class 'list'&gt;\n\n\n\nIndexing\n\nIndexing is used to access items of a list\nIndexing uses square brackets and numbers to access individual items of a list\nWhere 0 refers to the first item, 1 refers to the second item, and so on\n\n\n# indexing\nprint(pets[2])\n\nrabbit\n\n\n\n#range of indexes\nprint(pets[1:3])\n\n['cat', 'rabbit']\n\n\n\n\nAdding items to a list\n\npets = ['dog', 'cat', 'rabbit', 'monkey']\npets.append('hamster')\nprint(pets)\n\n['dog', 'cat', 'rabbit', 'monkey', 'hamster']\n\n\n\npets = ['dog', 'cat', 'rabbit', 'monkey']\npets.insert(1, 'hamster')\nprint(pets)\n\n['dog', 'hamster', 'cat', 'rabbit', 'monkey']\n\n\n\n\nDeleting Items from a list\n\npets = ['dog', 'cat', 'rabbit', 'monkey']\npets.pop()\nprint(pets)\n\n['dog', 'cat', 'rabbit']\n\n\n\npets = ['dog', 'cat', 'rabbit', 'monkey']\npets.remove('rabbit')\nprint(pets)\n\n['dog', 'cat', 'monkey']\n\n\n\npets = ['dog', 'cat', 'rabbit', 'monkey']\ndel pets [2]\nprint(pets)\n\n['dog', 'cat', 'monkey']\n\n\n\n\nGetting the length of a list\nThe length of a list refers to the number of items in a list, use the len() method\n\n\nHomework\n\nCheck if an item exist\n\n\n\nExtending a list\nThe extend() methods adds all items from one list to another\n\npets = ['dog', 'cat']\nother_pets = ['rabbit', 'monkey']\npets.extend(other_pets)\nprint(pets)\n\n['dog', 'cat', 'rabbit', 'monkey']",
    "crumbs": [
      "Python",
      "Containers"
    ]
  },
  {
    "objectID": "Python/05_containers.html#tuple",
    "href": "Python/05_containers.html#tuple",
    "title": "DATAIDEA",
    "section": "Tuple",
    "text": "Tuple\n\nPython tuple is an ordered container\nIts the same as a list but the items of tuples cannot be changed\nWe create a tuple using round brackets ()\n\n\npets = ('dog', 'cat', 'rabbit')\nprint(pets)\nprint(type(pets))\n\n('dog', 'cat', 'rabbit')\n&lt;class 'tuple'&gt;",
    "crumbs": [
      "Python",
      "Containers"
    ]
  },
  {
    "objectID": "Python/05_containers.html#sets",
    "href": "Python/05_containers.html#sets",
    "title": "DATAIDEA",
    "section": "Sets",
    "text": "Sets\n\n \n\n\nA set is a container/collection that is unordered and immutable\nWe create a set using {}\n\n\npets = {'dog', 'cat', 'rabbit'}\nprint(pets)\n\n{'rabbit', 'dog', 'cat'}\n\n\n\n# A set can contain objects of different data types\nmixed = {'dog', 21, True}\nprint(mixed)\nprint(type(mixed))\n\n{True, 'dog', 21}\n&lt;class 'set'&gt;\n\n\n\nAccessing set elements\n\nUnlike lists and tuples, you cannot access the items in a set using indexes\nThis is because a set is unordered and not indexed\nHowever, we can use a for loop to access all its items one-by-one\n\nNote: We’ll discuss a for loop in the next chapter\n\n# Accessing\npets = {'dog', 'cat', 'rabbit'}\nfor pet in pets:\n    print(pet)\n\nrabbit\ndog\ncat\n\n\n\n\nAdding elements to a set\n\n# Adding items to a set\npets = {'dog', 'cat', 'rabbit'}\npets.add('fish')\nprint(pets)\n\n{'rabbit', 'dog', 'cat', 'fish'}\n\n\n\n\nRemoving set elements\n\n# Removing items from a set\npets = {'dog', 'cat', 'rabbit'}\npets.remove('cat') # remove\nprint(pets)\n\n{'rabbit', 'dog'}\n\n\n\npets = {'dog', 'cat', 'rabbit'}\npets.discard('rabbit') #discard\nprint(pets)\n\n{'dog', 'cat'}\n\n\n\npets = {'dog', 'cat', 'rabbit'}\npets.pop() # pop removes the last item from the set\nprint(pets)\n\n{'dog', 'cat'}\n\n\n\n\nHomework\n\nFind the length of a set\nCheck if an element exists\nCombine sets\n\n\n\nGetting the difference between sets\n\n# Getting the difference\nfirst_numbers = {1, 2, 3, 4}\nsecond_numbers = {3, 4, 5, 6}\n\ndifference = first_numbers - second_numbers\n# another way\ndifference2 = first_numbers.difference(second_numbers)\nprint(difference)\n\n{1, 2}",
    "crumbs": [
      "Python",
      "Containers"
    ]
  },
  {
    "objectID": "Python/05_containers.html#dictionaries",
    "href": "Python/05_containers.html#dictionaries",
    "title": "DATAIDEA",
    "section": "Dictionaries",
    "text": "Dictionaries\nA dictionary is an unordered and mutable colletion of items\n\n \n\n\n# Creating \nperson = {\n    'first_name': 'Voila', \n    'last_name': 'Akullu',\n    'age': 16\n    }\nprint(person)\n\n{'first_name': 'Voila', 'last_name': 'Akullu', 'age': 16}\n\n\n\n# Accessing items\nprint(person['last_name'])\n\nAkullu\n\n\n\n# Adding items \nperson['middle_name'] = 'Vee'\nprint(person)\n\n{'first_name': 'Voila', 'last_name': 'Akullu', 'age': 16, 'middle_name': 'Vee'}\n\n\n\n# Remove items\nperson.pop('age')\nprint(person)\n\n{'first_name': 'Voila', 'last_name': 'Akullu', 'middle_name': 'Vee'}\n\n\n\nHomework\n\nCheck if an element exists\nFind the lenght of a dictionary\n\n\n# Nesting dictionaries\nemployees = {\n    'manager': {\n        'name': 'Akullu Viola',\n        'age': 29\n    },\n    'programmer': {\n        'name': 'Juma Shafara',\n        'age': 30\n    }\n}\n\nprint(employees)\n\n{'manager': {'name': 'Akullu Viola', 'age': 29}, 'programmer': {'name': 'Juma Shafara', 'age': 30}}\n\n\n\n# Accessing nested dictionary\nprogrammer = employees['programmer']\nprint(programmer['name'])\n\nJuma Shafara\n\n\n\n# Using a dictionary constructer\nnames = ('a1', 'b2', 'c3')\ndictionary = dict(names)\nprint(dictionary)\n\n{'a': '1', 'b': '2', 'c': '3'}\n\n\n\n\nDon’t miss out on any updates and developments! Subscribe to the DATAIDEA Newsletter it’s easy and safe.",
    "crumbs": [
      "Python",
      "Containers"
    ]
  },
  {
    "objectID": "Python/03_numbers.html",
    "href": "Python/03_numbers.html",
    "title": "DATAIDEA",
    "section": "",
    "text": "Photo by DATAIDEA",
    "crumbs": [
      "Python",
      "Numbers"
    ]
  },
  {
    "objectID": "Python/03_numbers.html#numbers",
    "href": "Python/03_numbers.html#numbers",
    "title": "DATAIDEA",
    "section": "Numbers",
    "text": "Numbers\n\n \n\nIn python, there are three types of numbers\n\nInteger - int\nFloating Point - float\nComplex - complex",
    "crumbs": [
      "Python",
      "Numbers"
    ]
  },
  {
    "objectID": "Python/03_numbers.html#types",
    "href": "Python/03_numbers.html#types",
    "title": "DATAIDEA",
    "section": "Types",
    "text": "Types\n\nInteger\nAn integer is a number without decimals\n\n# Python Numbers: intgers\n\na = 3\nb = 4\nnumber = 5\n\nprint('a:', a)\nprint('b:', b)\nprint('number:', number)\n\na: 3\nb: 4\nnumber: 5\n\n\n\n\nFloating Point\nA floating point number of just a float is a number with decimals\n\n# Python Numbers: floating point\na = 3.0\nb = 4.21\nnumber = 5.33\n\nprint('a:', a)\nprint('b:', b)\nprint('number:', number)\n\na: 3.0\nb: 4.21\nnumber: 5.33\n\n\n\n\nComplex\nA comple number is an imaginary number. To yield a complex number, append a j o J to a numeric value\n\n# Python Numbers: complex\n\na = 3j\nb = 5.21j\nnumber = 4 + 5.33j\n\nprint('a:', a)\nprint('b:', b)\nprint('number:', number)\n\na: 3j\nb: 5.21j\nnumber: (4+5.33j)",
    "crumbs": [
      "Python",
      "Numbers"
    ]
  },
  {
    "objectID": "Python/03_numbers.html#number-arthmetics",
    "href": "Python/03_numbers.html#number-arthmetics",
    "title": "DATAIDEA",
    "section": "Number Arthmetics",
    "text": "Number Arthmetics\n\n# Python Numbers: arthmetics\n\nsummation = 4 + 2\nprint('sum:', summation)\n\ndifference = 4 - 2\nprint('difference:', difference)\n\nproduct = 4 * 2\nprint('product:', product)\n\nquotient = 4 / 2\nprint('quotient:', quotient)\n\nsum: 6\ndifference: 2\nproduct: 8\nquotient: 2.0",
    "crumbs": [
      "Python",
      "Numbers"
    ]
  },
  {
    "objectID": "Python/03_numbers.html#number-methods",
    "href": "Python/03_numbers.html#number-methods",
    "title": "DATAIDEA",
    "section": "Number Methods",
    "text": "Number Methods\nNumber methods are special functions used to work with numbers\n\n# sum() can add many numbers at once\nsummation = sum([1,2,3,4,5,6,7,8,9,10])\nprint(summation) # 55\n\n55\n\n\n\n# round() rounds a number to a \n# specified number of decimal places\npi = 3.14159265358979\nrounded_pi = round(pi, 3)\n\nprint(pi) # 3.14159265358979\nprint(rounded_pi) # 3.142\n\npi: 3.14159265358979\nrounded_pi: 3.142\n\n\n\n# abs() returns the absolute value of a number\nnumber = -5\nabsolute_value = abs(number)\nprint(absolute_value) # 5\n\nabsolute value of -5 is 5\n\n\n\n# pow() returns the value of \n# x to the power of y\nfour_power_two = pow(4, 2)\nprint(four_power_two) # 16\n\n16\n\n\n\n# divmod() returns the quotient and \n# remainder of a division\nquotient, remainder = divmod(10, 3)\nprint(quotient) # 3\nprint(remainder) # 1\n\nQuotient: 3\nRemainder: 1\n\n\n\n\nDon’t miss out on any updates and developments! Subscribe to the DATAIDEA Newsletter it’s easy and safe.",
    "crumbs": [
      "Python",
      "Numbers"
    ]
  },
  {
    "objectID": "Python/04_operators.html",
    "href": "Python/04_operators.html",
    "title": "DATAIDEA",
    "section": "",
    "text": "Photo by DATAIDEA",
    "crumbs": [
      "Python",
      "Operators"
    ]
  },
  {
    "objectID": "Python/04_operators.html#operators",
    "href": "Python/04_operators.html#operators",
    "title": "DATAIDEA",
    "section": "Operators",
    "text": "Operators\nOperators are symbols that perform operations on operands. Operands can be variables, strings, numbers, booleans etc",
    "crumbs": [
      "Python",
      "Operators"
    ]
  },
  {
    "objectID": "Python/04_operators.html#arithmetic",
    "href": "Python/04_operators.html#arithmetic",
    "title": "DATAIDEA",
    "section": "Arithmetic",
    "text": "Arithmetic\nArithemators are symbols that perform mathematical operations on operands\n\n\n\nArithmetic Operator\nDescription\n\n\n\n\n+\nAddition\n\n\n-\nSubraction\n\n\n/\nDivision\n\n\n*\nMultiplication\n\n\n**\nExponentiates\n\n\n%\nRemainder\n\n\n\n\n# Let \nx = 10 \ny = 5\n\n\n# Addition\nsummation = x + y\nprint(summation)\n\n15\n\n\n\n# Subraction\ndifference = x - y\nprint(difference)\n\n5\n\n\n\n# Division\nquotient = x / y\nprint(quotient)\n\n2.0\n\n\n\n# Multiplication\nproduct = x * y\nprint(product)\n\n50\n\n\n\n# Exponentiation \nexponent = x ** y\nprint(exponent)\n\n100000\n\n\n\n# Remainder\nremainder = x % y\nprint(remainder)\n\n0\n\n\n\n# Floor Division\nfloor = 10 / 4\nprint(floor)\n\n2.5\n\n\n\n# Perform in a sequence\nans = 10 * 3 / 2 + 1\nprint(ans)\n\n16.0",
    "crumbs": [
      "Python",
      "Operators"
    ]
  },
  {
    "objectID": "Python/04_operators.html#assignment",
    "href": "Python/04_operators.html#assignment",
    "title": "DATAIDEA",
    "section": "Assignment",
    "text": "Assignment\nAssignment operators are used to assign values to variables.\n\n\n\nName\nOperation\nSame As\n\n\n\n\nAssignment\nx = y\nx = y\n\n\nAddition Ass\nx += y\nx = x + y\n\n\nSubtraction Ass\nx -= y\nx = x - y\n\n\nMult Ass\nx *= y\nx = x * y\n\n\nDivision Ass\nx /= y\nx = x / y\n\n\nExpo Ass\nx **= y\nx = x ** y\n\n\nRemainder Ass\nx %= y\nx = x % y\n\n\nFloor Div Ass\nx //= y\nx = x // y\n\n\n\n\n# Examples\n# Assignment\nx = 10\n# Addition Ass\nx += 5 # x = x + 5 =&gt; x = 10 + 5 =&gt; x = 15\nprint(x)\n\n15\n\n\n\n# Subraction Ass\nx = 10\nx -= 5 # x = x - 5 =&gt; x = 10 - 5 =&gt; x = 5\nprint(x)\n\n5",
    "crumbs": [
      "Python",
      "Operators"
    ]
  },
  {
    "objectID": "Python/04_operators.html#comparison",
    "href": "Python/04_operators.html#comparison",
    "title": "DATAIDEA",
    "section": "Comparison",
    "text": "Comparison\nA comparison operator compares its operands and returns a Boolean value based on whether the comparison is True of False\n\n\n\nName\nOperation\n\n\n\n\nEquality\n==\n\n\nInequality\n!=\n\n\nGreater than\n&gt;\n\n\nLess than\n&lt;\n\n\nGreater or equal\n&gt;=\n\n\nLess or equal\n&lt;=\n\n\n\n\n# Examples\n# Equality \n'Voila' == 'Viola'\n\nFalse\n\n\n\n# Inequality \n'Voila' != 'Viola'\n\nTrue\n\n\n\n# Greater or Equal\n34 &gt;= 43\n\nFalse\n\n\n\n# Tip\nprint('Voila' == 'Viola' == 'Voila')\n#       False == True =&gt; False\n\nFalse\n\n\n\n# weight = int(input(\"Enter your weight: \"))\n# height = int(input('Enter your height: '))\n\nweight = 56\nheight = 1.5\n\nbmi = weight/(height**2)\n\nif bmi &gt; 28:\n    print('You are over weight')\nelif bmi &gt; 18:\n    print('You are normal weight')\nelse:\n    print('You are under weight')\n\nYou are normal weight",
    "crumbs": [
      "Python",
      "Operators"
    ]
  },
  {
    "objectID": "Python/04_operators.html#identity",
    "href": "Python/04_operators.html#identity",
    "title": "DATAIDEA",
    "section": "Identity",
    "text": "Identity\nIdentity operators are used to compare two values to determine if they point to the same object\n\n\n\nOperator\nName\n\n\n\n\nis\nThe is operator\n\n\nis not\nThe is not operator\n\n\n\n\n# Example\n# is\nx = 5\ny = 4\nz = x # x = 5 =&gt; z = 5\n\nprint(x is not z)\n\nFalse",
    "crumbs": [
      "Python",
      "Operators"
    ]
  },
  {
    "objectID": "Python/04_operators.html#logical",
    "href": "Python/04_operators.html#logical",
    "title": "DATAIDEA",
    "section": "Logical",
    "text": "Logical\nLogical operators are commonly used with Booleans. In Python, there are 3 logical operators\n\n\n\nOperator\nDescription\n\n\n\n\nand\nLogical and operator\n\n\nor\nLogical or\n\n\nnot\nLogical not\n\n\n\n\nLogical and\nThe logical and operator returns True if both operands are True\n\n# Example\n# Logical and\nx = 4\nprint(x &gt; 3 and 8 &lt; x)\n#               True and False =&gt; False\n\nFalse\n\n\n\n\nLogical or\nThe logical or operator returns True if one of the operands is True\n\n# Logical or\ny = 7\nexpression_2 = 10 &gt; y or 4 &gt; y\n#                   True or False =&gt; True\nprint(expression_2)\n\nTrue\n\n\n\n\nLogical not\nThe logical not operator returns True if the operand is False, otherwise returns False if the operand is True\n\n# Logical not\nz = 8\nexpression_3 = not(10 == z)\n#               not False =&gt; True\nprint(expression_3)\n\nTrue",
    "crumbs": [
      "Python",
      "Operators"
    ]
  },
  {
    "objectID": "Python/04_operators.html#membership",
    "href": "Python/04_operators.html#membership",
    "title": "DATAIDEA",
    "section": "Membership",
    "text": "Membership\nMembership operators are used to check if a sequence is present in an object like a string, list etc\n\n\n\nOperator\nName\n\n\n\n\nin\nThe in operator\n\n\nnot in\nThe not in operator\n\n\n\n\n# Example\nname = 'Tinye Robert'\nprint('Robert' not in name)\n\nFalse\n\n\n\n\nDon’t miss out on any updates and developments! Subscribe to the DATAIDEA Newsletter it’s easy and safe.",
    "crumbs": [
      "Python",
      "Operators"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week2-Cleaning-Intro/21_weather_data.html",
    "href": "Python-Data-Analysis/Week2-Cleaning-Intro/21_weather_data.html",
    "title": "DATAIDEA",
    "section": "",
    "text": "Photo by DATAIDEA",
    "crumbs": [
      "Python-Data-Analysis",
      "Week2-Cleaning-Intro",
      "Cleaning the weather dataset"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week2-Cleaning-Intro/21_weather_data.html#cleaning-the-weather-dataset",
    "href": "Python-Data-Analysis/Week2-Cleaning-Intro/21_weather_data.html#cleaning-the-weather-dataset",
    "title": "DATAIDEA",
    "section": "Cleaning the weather dataset",
    "text": "Cleaning the weather dataset\nIn this notebook, we’ll be using numpy and pandas, to explore some techniques we can use to manipulate and clean a dataset. We’ll be using the weather dataset which I developed specifically for the purpose of this lesson.\nPandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool whereas Numpy is the fundamental package for scientific computing with Python.\nTo continue with this notebook, you must have python, pandas and numpy installed.\n\n## Uncomment and run this cell to install pandas and numpy\n#!pip install pandas numpy\n\n\n# import the libraries\nimport pandas as pd\nimport numpy as np\nfrom dataidea.datasets import loadDataset\n\nLet’s check the versions of python, numpy and pandas we’ll be using for this notebook\n\n# checking python version\nprint('Python Version: ',)\n!python --version\n\nPython Version: \nPython 3.10.12\n\n\n\n# Checking numpy and pandas versions\nprint('Pandas Version: ', pd.__version__)\nprint('Numpy Version: ', np.__version__)\n\nPandas Version:  2.2.2\nNumpy Version:  1.26.4\n\n\n\n\n\n\nLet’s load the dataset. We’ll be using a weather dataset that imagined for learning purposes.\n\n# load the dataset\nweather_data = loadDataset('weather')\n\nWe can sample out random rows from the dataset using the sample() method, we can use the n parameter to specify the number of rows to sample\n\n# sample out random values from the dataset\nweather_data.sample(n=5)\n\n\n\n\n\n\n\n\nday\ntemperature\nwindspead\nevent\n\n\n\n\n4\n07/01/2017\n32.0\nNaN\nRain\n\n\n1\n04/01/2017\nNaN\n9.0\nSunny\n\n\n7\n10/01/2017\n34.0\n8.0\nCloudy\n\n\n6\n09/01/2017\nNaN\nNaN\nNaN\n\n\n3\n06/01/2017\nNaN\n7.0\nNaN\n\n\n\n\n\n\n\nFrom our quick our sample, we can already observe some probles with the data that will need fixing\nDisplay some info about the dataset eg number of entries, count of non-null values and variable datatypes using the info() method\n\n# get quick dataframe info\nweather_data.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 9 entries, 0 to 8\nData columns (total 4 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   day          9 non-null      object \n 1   temperature  5 non-null      float64\n 2   windspead    5 non-null      float64\n 3   event        7 non-null      object \ndtypes: float64(2), object(2)\nmemory usage: 416.0+ bytes\n\n\n\n\n\n\nWe can count all missing values in each column in our dataframe by using dataframe.isna().sum(), eg\n\n# count missing values in each column\nweather_data.isna().sum()\n\nday            0\ntemperature    4\nwindspead      4\nevent          2\ndtype: int64\n\n\nWe can use a boolean-indexing like technique to find all rows in a dataset with missing values in a specific column.\n\n# get rows with missing data in temperature\nweather_data[weather_data.temperature.isna()]\n\n\n\n\n\n\n\n\nday\ntemperature\nwindspead\nevent\n\n\n\n\n1\n04/01/2017\nNaN\n9.0\nSunny\n\n\n3\n06/01/2017\nNaN\n7.0\nNaN\n\n\n5\n08/01/2017\nNaN\nNaN\nSunny\n\n\n6\n09/01/2017\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\n# get rows with missing data in event column\nweather_data[weather_data.event.isna()]\n\n\n\n\n\n\n\n\nday\ntemperature\nwindspead\nevent\n\n\n\n\n3\n06/01/2017\nNaN\n7.0\nNaN\n\n\n6\n09/01/2017\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\n\n\n\nFor the next part, we would like to demonstrate forward fill (ffill()) and backward fill (bfill), we first create two copies of the dataframe to avoid modifying our original copy in memory. - ffill() fills the missing values with the previous valid value in the column - bfill() fills the missing values with the next valid value in the column\nLet’s create 2 copies of our dataframe and test out each of these concepts on either of the copies\nFor the first copy, let’s fill NaN values in the event column with ffill()\n\n# fill with the previous valid value\nweather_data['event'] = weather_data.event.ffill()\nweather_data\n\n\n\n\n\n\n\n\nday\ntemperature\nwindspead\nevent\n\n\n\n\n0\n01/01/2017\n32.0\n6.0\nRain\n\n\n1\n04/01/2017\nNaN\n9.0\nSunny\n\n\n2\n05/01/2017\n28.0\nNaN\nSnow\n\n\n3\n06/01/2017\nNaN\n7.0\nSnow\n\n\n4\n07/01/2017\n32.0\nNaN\nRain\n\n\n5\n08/01/2017\nNaN\nNaN\nSunny\n\n\n6\n09/01/2017\nNaN\nNaN\nSunny\n\n\n7\n10/01/2017\n34.0\n8.0\nCloudy\n\n\n8\n11/01/2017\n40.0\n12.0\nSunny\n\n\n\n\n\n\n\nFrom the returned dataframe we can observe that the NaN values in event have been replaced with their corresponding non-null values orccurring earlier than them ie Snow at row 3 and Sunny at row 6\nExercise: Demonstrate how to replace missing values with the bfill() method\n\nChoosing Between ffill and bfill\n\nContext: Choose based on the context and the logical assumption that fits the nature of your data. If the past influences the present, use ffill. If the future influences the present, use bfill.\nData Patterns: Consider the patterns in your data and what makes sense for your specific analysis or model. Ensure that the method you choose maintains the integrity and meaning of your data.\n\n\n\n\n\n\n\nFill with a specific value\nWe can modify (or fill) a specific value in the dataframe by using the loc[] method. This picks the value by its row (index) and column names. Assigning it a new value modifies it in the dataframe as illustrated below\n\n# modify a specific value in the dataframe\nweather_data.loc[1, 'temperature'] = 29\nweather_data\n\n\n\n\n\n\n\n\nday\ntemperature\nwindspead\nevent\n\n\n\n\n0\n01/01/2017\n32.0\n6.0\nRain\n\n\n1\n04/01/2017\n29.0\n9.0\nSunny\n\n\n2\n05/01/2017\n28.0\nNaN\nSnow\n\n\n3\n06/01/2017\nNaN\n7.0\nSnow\n\n\n4\n07/01/2017\n32.0\nNaN\nRain\n\n\n5\n08/01/2017\nNaN\nNaN\nSunny\n\n\n6\n09/01/2017\nNaN\nNaN\nSunny\n\n\n7\n10/01/2017\n34.0\n8.0\nCloudy\n\n\n8\n11/01/2017\n40.0\n12.0\nSunny\n\n\n\n\n\n\n\nObserve that the missing value in row 1 and temperature has been replaced with 29\nWe can use the fillna() method to replace all missing values in a column with a specific value as demostrated value\n\n# replace missing values in temperature column with mean\nweather_data['temperature'] = weather_data.temperature.fillna(\n    value=weather_data.temperature.mean()\n)\nweather_data\n\n\n\n\n\n\n\n\nday\ntemperature\nwindspead\nevent\n\n\n\n\n0\n01/01/2017\n32.0\n6.0\nRain\n\n\n1\n04/01/2017\n29.0\n9.0\nSunny\n\n\n2\n05/01/2017\n28.0\nNaN\nSnow\n\n\n3\n06/01/2017\n32.5\n7.0\nSnow\n\n\n4\n07/01/2017\n32.0\nNaN\nRain\n\n\n5\n08/01/2017\n32.5\nNaN\nSunny\n\n\n6\n09/01/2017\n32.5\nNaN\nSunny\n\n\n7\n10/01/2017\n34.0\n8.0\nCloudy\n\n\n8\n11/01/2017\n40.0\n12.0\nSunny\n\n\n\n\n\n\n\nExercise: Demonstrate some technniques to replace missing data for numeric, and categorical data using the fillna()\n\n\n\n\nWe can also use the fillna() method to fill missing values in multiple columns by passing in the dictionary of key/value pairs of column-name and value to replace. To demonstrate this, let’s first reload a fresh dataframe with missing data\n\nweather_data = loadDataset('weather')\nweather_data\n\n\n\n\n\n\n\n\nday\ntemperature\nwindspead\nevent\n\n\n\n\n0\n01/01/2017\n32.0\n6.0\nRain\n\n\n1\n04/01/2017\nNaN\n9.0\nSunny\n\n\n2\n05/01/2017\n28.0\nNaN\nSnow\n\n\n3\n06/01/2017\nNaN\n7.0\nNaN\n\n\n4\n07/01/2017\n32.0\nNaN\nRain\n\n\n5\n08/01/2017\nNaN\nNaN\nSunny\n\n\n6\n09/01/2017\nNaN\nNaN\nNaN\n\n\n7\n10/01/2017\n34.0\n8.0\nCloudy\n\n\n8\n11/01/2017\n40.0\n12.0\nSunny\n\n\n\n\n\n\n\n\n# Replace missing values in temperature, column and event\nweather_data.fillna(value={\n    'temperature': weather_data.temperature.mean(), \n    'windspead': weather_data.windspead.max(), \n    'event': weather_data.event.bfill()\n    }, inplace=True)\n\n# Now let's look at our data\nweather_data\n\n\n\n\n\n\n\n\nday\ntemperature\nwindspead\nevent\n\n\n\n\n0\n01/01/2017\n32.0\n6.0\nRain\n\n\n1\n04/01/2017\n29.0\n9.0\nSunny\n\n\n2\n05/01/2017\n28.0\n12.0\nSnow\n\n\n3\n06/01/2017\n32.5\n7.0\nSnow\n\n\n4\n07/01/2017\n32.0\n12.0\nRain\n\n\n5\n08/01/2017\n32.5\n12.0\nSunny\n\n\n6\n09/01/2017\n32.5\n12.0\nSunny\n\n\n7\n10/01/2017\n34.0\n8.0\nCloudy\n\n\n8\n11/01/2017\n40.0\n12.0\nSunny\n\n\n\n\n\n\n\nWe can optionally drop all rows with missing values using the dropna() method. To demonstrate this, let’s first reload a fresh dataframe with missing data\n\nweather_data = loadDataset('weather')\nweather_data\n\n\n\n\n\n\n\n\nday\ntemperature\nwindspead\nevent\n\n\n\n\n0\n01/01/2017\n32.0\n6.0\nRain\n\n\n1\n04/01/2017\nNaN\n9.0\nSunny\n\n\n2\n05/01/2017\n28.0\nNaN\nSnow\n\n\n3\n06/01/2017\nNaN\n7.0\nNaN\n\n\n4\n07/01/2017\n32.0\nNaN\nRain\n\n\n5\n08/01/2017\nNaN\nNaN\nSunny\n\n\n6\n09/01/2017\nNaN\nNaN\nNaN\n\n\n7\n10/01/2017\n34.0\n8.0\nCloudy\n\n\n8\n11/01/2017\n40.0\n12.0\nSunny\n\n\n\n\n\n\n\n\n# Drop all rows with missing values\nweather_data.dropna()\n\n\n\n\n\n\n\n\nday\ntemperature\nwindspead\nevent\n\n\n\n\n0\n01/01/2017\n32.0\n6.0\nRain\n\n\n7\n10/01/2017\n34.0\n8.0\nCloudy\n\n\n8\n11/01/2017\n40.0\n12.0\nSunny\n\n\n\n\n\n\n\nIn the next chapter we’ll look at some data visualization tools in Python",
    "crumbs": [
      "Python-Data-Analysis",
      "Week2-Cleaning-Intro",
      "Cleaning the weather dataset"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week8-More-ML/82_sklearn_pipeline.html",
    "href": "Python-Data-Analysis/Week8-More-ML/82_sklearn_pipeline.html",
    "title": "DATAIDEA",
    "section": "",
    "text": "Photo by DATAIDEA",
    "crumbs": [
      "Python-Data-Analysis",
      "Week8-More-ML",
      "Pipeline"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week8-More-ML/82_sklearn_pipeline.html#pipeline",
    "href": "Python-Data-Analysis/Week8-More-ML/82_sklearn_pipeline.html#pipeline",
    "title": "DATAIDEA",
    "section": "Pipeline",
    "text": "Pipeline\nA pipeline is a series of data processing steps that are chained together sequentially. Each step in the pipeline typically performs some transformation on the data, such as preprocessing, feature extraction, feature selection, or model fitting.\n\nLet’s redefine a model\nIn week 4, we introduced ourselves to Machine Learning Concepts, in week 5 we learned some statistical tests and we applied them in week 7 to find the best feature and transform them to efficient forms. In this section, we will build on top of those concepts to redefine what a Machine Learning model is and hence come up with a more efficient way of developing good Machine Learning models\nFirst, let’s install the dataidea package, which will help us with loading packages and datasets with much more ease\n\n## install the version of dataidea used for this notebook\n\n!pip install --upgrade dataidea\n\nRequirement already satisfied: dataidea in /home/jumashafara/work/dataidea/dataidea_websites/dataidea (0.1.2)\n\n\n\n# Let's import some packages\n\nfrom dataidea.packages import * # imports np, pd, plt, etc\nfrom dataidea.datasets import loadDataset\nfrom sklearn.neighbors import KNeighborsRegressor\n\n\n# loading the data set\n\ndata = loadDataset('boston')\n\nThe Boston Housing Dataset\nThe Boston Housing Dataset is a derived from information collected by the U.S. Census Service concerning housing in the area of Boston MA. The following describes the dataset columns:\n\nCRIM - per capita crime rate by town\nZN - proportion of residential land zoned for lots over 25,000 sq.ft.\nINDUS - proportion of non-retail business acres per town.\nCHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)\nNOX - nitric oxides concentration (parts per 10 million)\nRM - average number of rooms per dwelling\nAGE - proportion of owner-occupied units built prior to 1940\nDIS - weighted distances to five Boston employment centres\nRAD - index of accessibility to radial highways\nTAX - full-value property-tax rate per $10,000\nPTRATIO - pupil-teacher ratio by town\nB - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\nLSTAT - % lower status of the population\nMEDV - Median value of owner-occupied homes in $1000’s\n\n\n# looking at the top part\n\ndata.head()\n\n\n\n\n\n\n\n\nCRIM\nZN\nINDUS\nCHAS\nNOX\nRM\nAGE\nDIS\nRAD\nTAX\nPTRATIO\nB\nLSTAT\nMEDV\n\n\n\n\n0\n0.00632\n18.0\n2.31\n0\n0.538\n6.575\n65.2\n4.0900\n1\n296.0\n15.3\n396.90\n4.98\n24.0\n\n\n1\n0.02731\n0.0\n7.07\n0\n0.469\n6.421\n78.9\n4.9671\n2\n242.0\n17.8\n396.90\n9.14\n21.6\n\n\n2\n0.02729\n0.0\n7.07\n0\n0.469\n7.185\n61.1\n4.9671\n2\n242.0\n17.8\n392.83\n4.03\n34.7\n\n\n3\n0.03237\n0.0\n2.18\n0\n0.458\n6.998\n45.8\n6.0622\n3\n222.0\n18.7\n394.63\n2.94\n33.4\n\n\n4\n0.06905\n0.0\n2.18\n0\n0.458\n7.147\n54.2\n6.0622\n3\n222.0\n18.7\n396.90\n5.33\n36.2\n\n\n\n\n\n\n\n\n\nTraining our first model\nIn week 4, we learned that to train a model (for supervised machine learning), we needed to have a set of X variables (also called independent, predictor etc), and then, we needed a y variable (also called dependent, outcome, predicted etc).\n\n# Selecting our X set and y\n\nX = data.drop('MEDV', axis=1)\ny = data.MEDV\n\nNow we can train the KNeighborsRegressor model, this model naturally makes predictions by averaging the values of the 5 neighbors to the point that you want to predict\n\n# lets traing the KNeighborsRegressor\n\nknn_model = KNeighborsRegressor() # instanciate the model class\nknn_model.fit(X, y) # train the model on X, y\nscore = knn_model.score(X, y) # obtain the model score on X, y\npredicted_y = knn_model.predict(X) # make predictions on X\n \nprint('score:', score)\n\nscore: 0.716098217736928\n\n\nNow lets go ahead and try to visualize the performance of the model. The scatter plot is of true labels against predicted labels. Do you think the model is doing well?\n\n# looking at the performance \n\nplt.scatter(y, predicted_y)\nplt.title('Model Performance')\nplt.xlabel('Predicted y')\nplt.ylabel('True y')\nplt.show()",
    "crumbs": [
      "Python-Data-Analysis",
      "Week8-More-ML",
      "Pipeline"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week8-More-ML/82_sklearn_pipeline.html#some-feature-selection.",
    "href": "Python-Data-Analysis/Week8-More-ML/82_sklearn_pipeline.html#some-feature-selection.",
    "title": "DATAIDEA",
    "section": "Some feature selection.",
    "text": "Some feature selection.\nFeature selection is a process where you automatically select those features in your data that contribute most to the prediction variable or output in which you are interested.\nIn week 7 we learned that having irrelevant features in your data can decrease the accuracy of many models. In the code below, we try to find out the best features that best contribute to the outcome variable\n\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_regression # score function for ANOVA with continuous outcome\n\n\n# lets do some feature selection using ANOVA\n\ndata_num = data.drop(['CHAS','RAD'], axis=1) # dropping categorical\nX = data_num.drop(\"MEDV\", axis=1) \ny = data_num.MEDV\n\n# using SelectKBest\ntest_reg = SelectKBest(score_func=f_regression, k=6) \nfit_boston = test_reg.fit(X, y)\nindexes = fit_boston.get_support(indices=True)\n\nprint(fit_boston.scores_)\nprint(indexes)\n\n[ 89.48611476  75.2576423  153.95488314 112.59148028 471.84673988\n  83.47745922  33.57957033 141.76135658 175.10554288  63.05422911\n 601.61787111]\n[ 2  3  4  7  8 10]\n\n\nFrom above, we can see from above that the best features for now are those in indexes [ 2  3  4  7  8 10] in the num_data dataset. Lets find them in the data and add on our categorical ones to set up our new X set\n\n# redifining the X set \n\nnew_X = data[['INDUS', 'NOX', 'RM', 'TAX', 'PTRATIO', 'LSTAT', 'CHAS','RAD']]\n\n\nTraining our second model\nNow that we have selected out the features, X that we thing best contribute to the outcome, let’s retrain our machine learning model and see if we are gonna get better results\n\nknn_model = KNeighborsRegressor()\nknn_model.fit(new_X, y)\nnew_score = knn_model.score(new_X, y)\nnew_predicted_y = knn_model.predict(new_X)\n\nprint('Feature selected score:', new_score)\n\nFeature selected score: 0.8324963639640872\n\n\nThe model seems to score better with a significant increment in accuracy from 0.71 to 0.83. As like last time, let us try to visualize the difference in performance\n\nplt.scatter(y, new_predicted_y)\nplt.title('Model Performance')\nplt.xlabel('New Predicted y')\nplt.ylabel('True y')\nplt.show()\n\n\n\n\n\n\n\n\nI do not know about you, but as for me, I notice a meaningful improvement in the predictions made from the model considering this scatter plot",
    "crumbs": [
      "Python-Data-Analysis",
      "Week8-More-ML",
      "Pipeline"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week8-More-ML/82_sklearn_pipeline.html#transforming-the-data",
    "href": "Python-Data-Analysis/Week8-More-ML/82_sklearn_pipeline.html#transforming-the-data",
    "title": "DATAIDEA",
    "section": "Transforming the data",
    "text": "Transforming the data\nIn week 7, we learned some advantages of scaling our data like:\n\npreventing dominance by features with larger scales\nfaster convergence in optimization algorithms\nreduce the impact of outliers\n\nIn the next section, we will use the sklearn StandardScaler to rescale our numeric data and the OneHotEncoder to encode the categorical data. Read more about these transformers in the sklearn documentation\n\n# importing the StandardScaler\n\nfrom sklearn.preprocessing import StandardScaler\n\n\nscaler = StandardScaler() # instanciating the StandardScaler\nstandardized_data_num = scaler.fit_transform(\n    data[['INDUS', 'NOX', 'RM', 'TAX', 'PTRATIO', 'LSTAT']]\n    ) # rescaline numeric features\nstandardized_data_num_df = pd.DataFrame(\n    standardized_data_num, \n    columns=['INDUS', 'NOX', 'RM', 'TAX', 'PTRATIO', 'LSTAT'] \n    ) # converting the standardized to dataframe\n\n\nstandardized_data_num_df.head()\n\n\n\n\n\n\n\n\nINDUS\nNOX\nRM\nTAX\nPTRATIO\nLSTAT\n\n\n\n\n0\n-1.287909\n-0.144217\n0.413672\n-0.666608\n-1.459000\n-1.075562\n\n\n1\n-0.593381\n-0.740262\n0.194274\n-0.987329\n-0.303094\n-0.492439\n\n\n2\n-0.593381\n-0.740262\n1.282714\n-0.987329\n-0.303094\n-1.208727\n\n\n3\n-1.306878\n-0.835284\n1.016303\n-1.106115\n0.113032\n-1.361517\n\n\n4\n-1.306878\n-0.835284\n1.228577\n-1.106115\n0.113032\n-1.026501\n\n\n\n\n\n\n\n\nfrom sklearn.preprocessing import OneHotEncoder\n\n\none_hot_encoder = OneHotEncoder()\nencoded_data_cat = one_hot_encoder.fit_transform(data[['CHAS', 'RAD']])\nencoded_data_cat_array = encoded_data_cat.toarray()\n# Get feature names\nfeature_names = one_hot_encoder.get_feature_names_out(['CHAS', 'RAD'])\n\nencoded_data_cat_df = pd.DataFrame(\n    data=encoded_data_cat_array,\n    columns=feature_names\n)\n\n\nencoded_data_cat_df.head()\n\n\n\n\n\n\n\n\nCHAS_0\nCHAS_1\nRAD_1\nRAD_2\nRAD_3\nRAD_4\nRAD_5\nRAD_6\nRAD_7\nRAD_8\nRAD_24\n\n\n\n\n0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n3\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n4\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n\n\n\nLet us add that to the new X and form a standardized new X set\n\ntransformed_new_X = pd.concat(\n    [standardized_data_num_df, encoded_data_cat_df], \n    axis=1\n    )\n\n\ntransformed_new_X.head()\n\n\n\n\n\n\n\n\nINDUS\nNOX\nRM\nTAX\nPTRATIO\nLSTAT\nCHAS_0\nCHAS_1\nRAD_1\nRAD_2\nRAD_3\nRAD_4\nRAD_5\nRAD_6\nRAD_7\nRAD_8\nRAD_24\n\n\n\n\n0\n-1.287909\n-0.144217\n0.413672\n-0.666608\n-1.459000\n-1.075562\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\n-0.593381\n-0.740262\n0.194274\n-0.987329\n-0.303094\n-0.492439\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n-0.593381\n-0.740262\n1.282714\n-0.987329\n-0.303094\n-1.208727\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n3\n-1.306878\n-0.835284\n1.016303\n-1.106115\n0.113032\n-1.361517\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n4\n-1.306878\n-0.835284\n1.228577\n-1.106115\n0.113032\n-1.026501\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n\n\n\n\nTraining our third model\nNow that we have the right features selected and standardized, let us train a new model and see if it is gonna beat the first models\n\nknn_model = KNeighborsRegressor()\nknn_model.fit(transformed_new_X, y)\nnew_transformed_score = knn_model.score(transformed_new_X, y)\nnew_predicted_y = knn_model.predict(transformed_new_X)\n\nprint('Transformed score:', new_transformed_score)\n\nTransformed score: 0.8734524530397529\n\n\nThis new models appears to do better than the earlier ones with an improvement in score from 0.83 to 0.87. Do you think this is now a good model?",
    "crumbs": [
      "Python-Data-Analysis",
      "Week8-More-ML",
      "Pipeline"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week8-More-ML/82_sklearn_pipeline.html#the-pipeline",
    "href": "Python-Data-Analysis/Week8-More-ML/82_sklearn_pipeline.html#the-pipeline",
    "title": "DATAIDEA",
    "section": "The Pipeline",
    "text": "The Pipeline\nIt turns out the above efforts to improve the performance of the model add extra steps to pass before you can have a good model. But what about if we can put together the transformers into on object we do most of that stuff.\nThe sklearn Pipeline allows you to sequentially apply a list of transformers to preprocess the data and, if desired, conclude the sequence with a final predictor for predictive modeling.\nIntermediate steps of the pipeline must be ‘transforms’, that is, they must implement fit and transform methods. The final estimator only needs to implement fit.\nLet us build a model that puts together transformation and modelling steps into one pipeline object\n\n# lets import the Pipeline from sklearn\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n\n\nnumeric_cols = ['INDUS', 'NOX', 'RM', 'TAX', 'PTRATIO', 'LSTAT']\ncategorical_cols = ['CHAS', 'RAD']\n\n\n# Preprocessing steps\nnumeric_transformer = StandardScaler()\ncategorical_transformer = OneHotEncoder()\n\n# Combine preprocessing steps\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])\n\n# Pipeline\npipe = Pipeline([\n    ('preprocessor', preprocessor),\n    ('model', KNeighborsRegressor())\n])\n\nprint(pipe)\n\nPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num', StandardScaler(),\n                                                  ['INDUS', 'NOX', 'RM', 'TAX',\n                                                   'PTRATIO', 'LSTAT']),\n                                                 ('cat', OneHotEncoder(),\n                                                  ['CHAS', 'RAD'])])),\n                ('model', KNeighborsRegressor())])\n\n\n\n# Fit the pipeline\npipe.fit(new_X, y)\n\n# Score the pipeline\npipe_score = pipe.score(new_X, y)\n\n# Predict using the pipeline\npipe_predicted_y = pipe.predict(new_X)\n\nprint('Pipe Score:', pipe_score)\n\nPipe Score: 0.8734524530397529\n\n\n\nplt.scatter(y, pipe_predicted_y)\nplt.title('Pipe Performance')\nplt.xlabel('Pipe Predicted y')\nplt.ylabel('True y')\nplt.show()\n\n\n\n\n\n\n\n\nWe can observe that the model still gets the same good score, but now all the transformation steps, both on numeric and categorical variables are in a single pipeline object together with the model.\nA few ads maybe displayed for income as resources are now offered freely. 🤝🤝🤝",
    "crumbs": [
      "Python-Data-Analysis",
      "Week8-More-ML",
      "Pipeline"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week8-More-ML/81_regression_metrics.html",
    "href": "Python-Data-Analysis/Week8-More-ML/81_regression_metrics.html",
    "title": "DATAIDEA",
    "section": "",
    "text": "Photo by DATAIDEA",
    "crumbs": [
      "Python-Data-Analysis",
      "Week8-More-ML",
      "Regression Metrics"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week8-More-ML/81_regression_metrics.html#regression-metrics",
    "href": "Python-Data-Analysis/Week8-More-ML/81_regression_metrics.html#regression-metrics",
    "title": "DATAIDEA",
    "section": "Regression Metrics",
    "text": "Regression Metrics\nIn regression tasks, the goal is to predict continuous numerical values. Scikit-learn provides several metrics to evaluate the performance of regression models.\n\n# True labels\ny_true = [2.5, 3.7, 5.1, 4.2, 6.8]\n# Predicted labels\ny_pred = [2.3, 3.5, 4.9, 4.0, 6.5]\n\n\nMean Absolute Error (MAE):\n\nMAE measures the average absolute errors between predicted values and actual values.\nImagine you’re trying to hit a target with darts. The MAE is like calculating the average distance between where your darts hit and the bullseye. You just sum up how far each dart landed from the center (without caring if it was too short or too far) and then find the average. The smaller the MAE, the closer your predictions are to the actual values.\nFormula: \\[\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_{\\text{true}} - y_{\\text{pred}}|\\]\n\n\nfrom sklearn.metrics import mean_absolute_error\n\n# Calculate Mean Absolute Error (MAE)\nmae = mean_absolute_error(y_true, y_pred)\nprint(\"Mean Absolute Error (MAE):\", mae)\n\nMean Absolute Error (MAE): 0.21999999999999992\n\n\n\n\nMean Squared Error (MSE):\n\nMSE measures the average of the squares of the errors between predicted values and actual values.\nThis is similar to MAE, but instead of just adding up the distances, you square them before averaging. Squaring makes bigger differences more noticeable (by making them even bigger), so MSE penalizes larger errors more than smaller ones.\nFormula: \\[\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_{\\text{true}} - y_{\\text{pred}})^2\\]\n\n\nfrom sklearn.metrics import mean_squared_error\n\n# Calculate Mean Squared Error (MSE)\nmse = mean_squared_error(y_true, y_pred)\nprint(\"Mean Squared Error (MSE):\", mse)\n\nMean Squared Error (MSE): 0.04999999999999997\n\n\n\n\nRoot Mean Squared Error (RMSE):\n\nRMSE is the square root of the MSE, providing a more interpretable scale since it’s in the same units as the target variable.\nIt’s just like MSE, but we take the square root of the result. This brings the error back to the same scale as the original target variable, which makes it easier to interpret. RMSE gives you an idea of how spread out your errors are in the same units as your data.\nFormula: \\[\\text{RMSE} = \\sqrt{\\text{MSE}}\\]\n\n\nfrom sklearn.metrics import root_mean_squared_error\n\n# Calculate Root Mean Squared Error (RMSE)\nrmse = root_mean_squared_error(y_true, y_pred,)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\n\nRoot Mean Squared Error (RMSE): 0.2236067977499789\n\n\n\nR-squared (Coefficient of Determination):\n\nR-squared measures the proportion of the variance in the dependent variable that is predictable from the independent variables.\nThis tells you how well your model’s predictions match the actual data compared to a simple average. If R-squared is 1, it means your model perfectly predicts the target variable. If it’s 0, it means your model is no better than just predicting the mean of the target variable. So, the closer R-squared is to 1, the better your model fits the data.\nFormula: \\[R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_{\\text{true}} - y_{\\text{pred}})^2}{\\sum_{i=1}^{n} (y_{\\text{true}} - \\bar{y}_{\\text{true}})^2}\\]\nwhere \\[ \\bar{y}_{\\text{true}}\\] is the mean of the observed data.\n\n\n\nfrom sklearn.metrics import r2_score\n\n# Calculate R-squared (Coefficient of Determination)\nr2 = r2_score(y_true, y_pred)\nprint(\"R-squared (R2 Score):\", r2)\n\nR-squared (R2 Score): 0.975896644812958\n\n\nUnderstanding these metrics can help you assess the performance of your regression model and make necessary adjustments to improve its accuracy.\nA few ads maybe displayed for income as resources are now offered freely. 🤝🤝🤝",
    "crumbs": [
      "Python-Data-Analysis",
      "Week8-More-ML",
      "Regression Metrics"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week5-Statistics/51_descriptive_statistics.html",
    "href": "Python-Data-Analysis/Week5-Statistics/51_descriptive_statistics.html",
    "title": "Descriptive Statistics",
    "section": "",
    "text": "Photo by DATAIDEA\n## Uncomment and run this cell to install the packages\n# !pip install --upgrade dataidea\nimport pandas as pd\nimport numpy as np\nimport scipy as sp\nimport matplotlib.pyplot as plt\nThis notebook has been modified to use the Nobel Price Laureates Dataset which you can download from opendatasoft",
    "crumbs": [
      "Python-Data-Analysis",
      "Week5-Statistics",
      "Descriptive Statistics"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week5-Statistics/51_descriptive_statistics.html#descriptive-statistics-and-summary-metrics",
    "href": "Python-Data-Analysis/Week5-Statistics/51_descriptive_statistics.html#descriptive-statistics-and-summary-metrics",
    "title": "Descriptive Statistics",
    "section": "Descriptive Statistics and Summary Metrics",
    "text": "Descriptive Statistics and Summary Metrics\nDescriptive statistics is a branch of statistics that deals with the presentation and summary of data in a meaningful and informative way. Its primary goal is to describe and summarize the main features of a dataset.\nCommonly used measures in descriptive statistics include:\n\nMeasures of central tendency: These describe the center or average of a dataset and include metrics like mean, median, and mode.\nMeasures of variability: These indicate the spread or dispersion of the data and include metrics like range, variance, and standard deviation.\nMeasures of distribution shape: These describe the distribution of data points and include metrics like skewness and kurtosis.\nMeasures of association: These quantify the relationship between variables and include correlation coefficients.\n\nDescriptive statistics provide simple summaries about the sample and the observations that have been made.\n\n1. Measures of central tendency ie Mean, Median, Mode:\nThe Center of the Data:\nThe center of the data is where most of the values are concentrated.\n\nMean: It is the average value of a dataset calculated by summing all values(numerical) and dividing by the total count.\nMedian: It is the middle value of a dataset when arranged in ascending order. If there is an even number of observations, the median is the average of the two middle values.\nMode: It is the value that appears most frequently in a dataset.\n\n\n# load the dataset (modify the path to point to your copy of the dataset)\ndata = pd.read_csv('../assets/nobel_prize_year.csv')\ndata.sample(n=5)\n\n\n\n\n\n\n\n\nYear\nGender\nCategory\nbirth_year\nage\n\n\n\n\n912\n1949\nmale\nPhysics\n1907\n42\n\n\n576\n1945\nmale\nPhysics\n1900\n45\n\n\n205\n1913\nmale\nMedicine\n1850\n63\n\n\n466\n2020\nmale\nPhysics\n1952\n68\n\n\n602\n1982\nmale\nMedicine\n1916\n66\n\n\n\n\n\n\n\n\n# removing organizations\ndata = data[data.Gender != 'org']\n\n\nages = data.age\n\n# Let's get the values that \n# describe the center of the ages data \nmean_value = np.mean(ages)\nmedian_value = np.median(ages)\nmode_value = sp.stats.mode(ages)[0]\n\n# Let's print the values\nprint(\"Mean:\", mean_value)\nprint(\"Median:\", median_value)\nprint(\"Mode:\", mode_value)\n\nMean: 60.21383647798742\nMedian: 60.0\nMode: 56\n\n\nHomework: - Other ways to find mode (ie using pandas and numpy)\n\n\n2. Measures of variability\nThe Variation of the Data:\nThe variation of the data is how spread out the data are around the center.\na) Variance and Standard Deviation: - Variance: It measures the spread of the data points around the mean. - Standard Deviation: It is the square root of the variance, providing a measure of the average distance between each data point and the mean.\n\nIn summary, variance provides a measure of dispersion in squared units, while standard deviation provides a measure of dispersion in the original units of the data\n\n\n# how to implement the variance and standard deviation using numpy\nvariance_value = np.var(ages)\nstd_deviation_value = np.std(ages)\n\nprint(\"Variance:\", variance_value)\nprint(\"Standard Deviation:\", std_deviation_value)\n\nVariance: 159.28551085795658\nStandard Deviation: 12.620836377116873\n\n\n\nSmaller variances and standard deviation values mean that the data has values similar to each other and closer to the mean and the vice versa is true\n\n\n# Multiply by 2 for the second standard deviation\nstd_second = 2 * std_deviation_value  \n\n# Multiply by 3 for the third standard deviation\nstd_third = 3 * std_deviation_value   \n\nprint(\"First Standard Deviation:\", std_deviation_value)\nprint(\"Second Standard Deviation:\", std_second)\nprint(\"Third Standard Deviation:\", std_third)\n\n# empirical rule, also known as the 68-95-99.7 rule,\n\nFirst Standard Deviation: 12.620836377116873\nSecond Standard Deviation: 25.241672754233747\nThird Standard Deviation: 37.86250913135062\n\n\n\nplt.hist(x=ages, bins=20, edgecolor='black')\n# add standard deviation lines\nplt.axvline(mean_value, color='red', linestyle='--', label='Mean')\nplt.axvline(mean_value+std_deviation_value, color='orange', linestyle='--', label='1st std Dev')\nplt.axvline(mean_value-std_deviation_value, color='orange', linestyle='--')\nplt.axvline(mean_value+std_second, color='black', linestyle='--', label='2nd Std Dev')\nplt.axvline(mean_value-std_second, color='black', linestyle='--')\nplt.axvline(mean_value+std_third, color='green', linestyle='--', label='3rd Std Dev')\nplt.axvline(mean_value-std_third, color='green', linestyle='--')\nplt.title('Age of Nobel Prize Winners')\nplt.ylabel('Frequency')\nplt.xlabel('Age')\n# Adjust the position of the legend\nplt.legend(loc='upper left')\n\nplt.show()\n\n\n\n\n\n\n\n\nThe rule to consider:\nThe empirical rule, also known as the 68-95-99.7 rule, describes the distribution of data in a normal distribution. According to this rule:\n\nApproximately 68% of the data falls within one standard deviation of the mean.\nApproximately 95% of the data falls within two standard deviations of the mean.\nApproximately 99.7% of the data falls within three standard deviations of the mean.\n\n\nRange and Interquartile Range (IQR):\n\n\nRange: It is the difference between the maximum and minimum values in a dataset. It is simplest measure of variation\nInterquartile Range (IQR): It is the range between the first quartile (25th percentile) and the third quartile (75th percentile) of the dataset.\n\n\nIn summary, while the range gives an overview of the entire spread of the data from lowest to highest, the interquartile range focuses s`pecifically on the spread of the middle portion of the data, making it more robust against outliers.\n\n\n# One way to obtain range\nmin_age = min(ages)\nmax_age = max(ages)\nage_range = max_age - min_age\nprint('Range:', age_range)\n\nRange: 80\n\n\n\n# Calculating the range using numpy\nrange_value = np.ptp(ages)\n\nprint(\"Range:\", range_value)\n\nRange: 80\n\n\n\nplt.hist(x=ages, bins=20, edgecolor='black')\n# add standard deviation lines\nplt.axvline(min_age, color='green', linestyle='--', label=f'Min({min_age})')\nplt.axvline(max_age, color='red', linestyle='--', label=f'Max({max_age})')\nplt.plot([min_age, max_age], [170, 170], label=f'Range({age_range})')\n# labels\nplt.title('Age of Nobel Prize Winners')\nplt.ylabel('Frequency')\nplt.xlabel('Age')\n# Adjust the position of the legend\nplt.legend(loc='upper left')\nplt.show()\n\n\n\n\n\n\n\n\nQuartiles:\nCalculating Quartiles\nThe quartiles (Q0,Q1,Q2,Q3,Q4) are the values that separate each quarter.\nBetween Q0 and Q1 are the 25% lowest values in the data. Between Q1 and Q2 are the next 25%. And so on.\n\nQ0 is the smallest value in the data.\nQ1 is the value separating the first quarter from the second quarter of the data.\nQ2 is the middle value (median), separating the bottom from the top half.\nQ3 is the value separating the third quarter from the fourth quarter\nQ4 is the largest value in the data.\n\n\n# Calculate the quartile\nquartiles = np.quantile(a=ages, q=[0, 0.25, 0.5, 0.75, 1])\n\nprint(quartiles)\n\n[17. 51. 60. 69. 97.]\n\n\n\nplt.hist(x=ages, bins=20, edgecolor='black')\n# add standard deviation lines\nplt.axvline(quartiles[0], color='orange', linestyle='--', label=f'Q0({quartiles[0]})')\nplt.axvline(quartiles[1], color='red', linestyle='--', label=f'Q1({quartiles[1]})')\nplt.axvline(quartiles[2], color='green', linestyle='--', label=f'Q2({quartiles[2]})')\nplt.axvline(quartiles[3], color='blue', linestyle='--', label=f'Q3({quartiles[3]})')\nplt.axvline(quartiles[4], color='black', linestyle='--', label=f'Q4({quartiles[4]})')\nplt.plot([quartiles[0], quartiles[1]], [180, 180], color='red', label=f'25%')\nplt.plot([quartiles[0], quartiles[2]], [175, 175], color='green', label=f'50%')\nplt.plot([quartiles[0], quartiles[3]], [170, 170], color='blue', label=f'75%')\nplt.plot([quartiles[0], quartiles[4]], [165, 165], color='black', label=f'100%')\n# labels\nplt.title('Age of Nobel Prize Winners')\nplt.ylabel('Frequency')\nplt.xlabel('Age')\n# Adjust the position of the legend\nplt.legend(loc='center left')\nplt.show()\n\n\n\n\n\n\n\n\nPercentiles:\nPercentiles are values that separate the data into 100 equal parts.\nFor example, The 95th percentile separates the lowest 95% of the values from the top 5%\n\nThe 25th percentile (P25%) is the same as the first quartile (Q1).\nThe 50th percentile (P50%) is the same as the second quartile (Q2) and the median.\nThe 75th percentile (P75%) is the same as the third quartile (Q3)\n\nCalculating Percentiles with Python\nTo get all the percentile values, we can use np.percentile() method and pass in the data, and the list of the percentiles as showed below.\n\n# Getting many percentiles\nnp.percentile(ages, [25, 50, 75])\n\narray([51., 60., 69.])\n\n\nTo get a single percentile value, we can again use the np.percentile() method and pass in the data, and a the specicific percentile you’re interested in eg:\n\n# Getting one percentile at a time\nfirst_quartile = np.percentile(a=ages, q=25) # 25th percentile\nmiddle_percentile = np.percentile(ages, 50)\nthird_quartile = np.percentile(ages, 75) # 75th percentile\n\nprint('first_quartile: ', first_quartile)\nprint('middle_percentile: ', middle_percentile)  \nprint('third_quartile', third_quartile)\n\nfirst_quartile:  51.0\nmiddle_percentile:  60.0\nthird_quartile 69.0\n\n\n\n\nNote!\n\n\nNote also that we can be able to use the np.quantile() method to calculate the percentiles which makes logical sense as all the values mark a fraction(percentage) of the data\n\n\n\npercentiles = np.quantile(a=ages, q=[0.25, 0.50, 0.75])\nprint('Percentiles:', percentiles)\n\nPercentiles: [51. 60. 69.]\n\n\nNow we can be able to obtain the interquartile range as the difference between the third and first quartiles as predefined.\n\n# obtain the interquartile\niqr_value = third_quartile - first_quartile\n\nprint('Interquartile range: ', iqr_value)\n\nInterquartile range:  18.0\n\n\nNote: Quartiles and percentiles are both types of quantiles\n\n\n3. Measures of distribution shape ie Skewness and Kurtosis:\nThe shape of the Data:\nThe shape of the data refers to how the data are bounded on either side of the center. - Skewness: It measures the asymmetry of the distribution. - Kurtosis: It measures the peakedness or flatness of the distribution.\n\n\nNote!\n\n\nIn simple terms, skewness tells you if your data is leaning more to one side or the other, while kurtosis tells you if your data has heavy or light tails and how sharply it peaks.\n\n\n\n# Plot the histogram\n# Set density=True for normalized histogram\nplt.hist(x=ages, bins=20, density=True, edgecolor='black')  \n\n# Create a normal distribution curve\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\np = sp.stats.norm.pdf(x, mean_value, std_deviation_value)\nplt.plot(x, p, 'k', linewidth=2)  \n# 'k' indicates black color, you can change it to any color\n\n# Labels and legend\nplt.xlabel('Age')\nplt.ylabel('Probability Density')\nplt.title('Histogram with Normal Distribution Curve')\nplt.legend(['Normal Distribution', 'Histogram'])\n\nplt.show()\n\n\n\n\n\n\n\n\n\n# let's get skew from scipy\nskewness_value = sp.stats.skew(ages)\n\nprint(\"Skewness:\", skewness_value)\n\nSkewness: -0.028324578326524283\n\n\nHow to interpret Skewness:\n\nPositive skewness (&gt; 0) indicates that the tail on the right side of the distribution is longer than the left side (right skewed).\nNegative skewness (&lt; 0) indicates that the tail on the left side of the distribution is longer than the right side (left skewed).\n\n\n# let's get kurtosis from scipy\nkurtosis_value = sp.stats.kurtosis(ages)\n\nprint(\"Kurtosis:\", kurtosis_value)\n\nKurtosis: -0.3811155702676823\n\n\nHow to interpret Kurtosis:\n\nA kurtosis of 3 indicates the normal distribution (mesokurtic), also known as Gaussian distribution.\nPositive kurtosis (&gt; 3) indicates a distribution with heavier tails and a sharper peak than the normal distribution. This is called leptokurtic.\nNegative kurtosis (&lt; 3) indicates a distribution with lighter tails and a flatter peak than the normal distribution. This is called platykurtic.\n\n\n\n4.Measures of association\n\nCorrelation\n\n\nCorrelation measures the relationship between two numerical variables.\n\nCorrelation Matrix\n\nA correlation matrix is simply a table showing the correlation coefficients between variables\n\nCorrelation Matrix in Python\nWe can use the corrcoef() function in Python to create a correlation matrix.\n\n# Generate example data\nx = np.array([1, 1, 3, 5, 15])\ny = np.array([2, 4, 6, 8, 10])\n\ncorrelation_matrix = np.corrcoef(x, y)\n\ncorrelation_matrix_df = pd.DataFrame(\n    correlation_matrix, \n    columns=['x', 'y'], \n    index=['x', 'y']\n    )\ncorrelation_matrix_df\n\n\n\n\n\n\n\n\nx\ny\n\n\n\n\nx\n1.000000\n0.867722\n\n\ny\n0.867722\n1.000000\n\n\n\n\n\n\n\nCorrelation Coefficient: - The correlation coefficient measures the strength and direction of the linear relationship between two continuous variables. - It ranges from -1 to 1, where: - 1 indicates a perfect positive linear relationship, eg complementary good bread and blueband, battery and torch, fuel and car - -1 indicates a perfect negative linear relationship, eg substitute goods like tea and coffee - 0 indicates no linear relationship, eg phones and socks, house and mouse\n\n# Calculate correlation coefficient\ncorrelation = np.corrcoef(x, y)[0, 1]\nprint(\"Correlation Coefficient:\", correlation)\n\nCorrelation Coefficient: 0.8677218312746245\n\n\nCorrelation vs Causality:\nCorrelation measures the numerical relationship between two varaibles\nA high correlation coefficient (close to 1), does not mean that we can for sure conclude an actual relationship between two variables.\nA classic example:\n\nDuring the summer, the sale of ice cream at a beach increases\nSimultaneously, drowning accidents also increase as well\n\nDoes this mean that increase of ice cream sale is a direct cause of increased drowning accidents?\nMeasures of Association for Categorical Variables\n\nContingency Tables and Chi-square Test for Independence:\n\nContingency tables are used to summarize the relationship between two categorical variables by counting the frequency of observations for each combination of categories.\nChi-square test for independence determines whether there is a statistically significant association between the two categorical variables.\n\n\n\ndemo_data = data[['Gender', 'Category']]\n\n# We drop all the missing values just for demonstration purposes\ndemo_data = demo_data.dropna()\n\n\n# Obtain the cross tabulation of Gender and Category\n# The cross tabulation is also known as the contingency table\ngender_category_tab = pd.crosstab(\n    demo_data.Gender, \n    demo_data.Category\n    )\n\n# Let's have a look at the outcome\ngender_category_tab\n\n\n\n\n\n\n\nCategory\nChemistry\nEconomics\nLiterature\nMedicine\nPeace\nPhysics\n\n\nGender\n\n\n\n\n\n\n\n\n\n\nfemale\n8\n2\n17\n13\n18\n5\n\n\nmale\n181\n87\n102\n212\n90\n219\n\n\n\n\n\n\n\n\n\nTest of Independence:\nThis test is used to determine whether there is a significant association between two categorical variables.\nFormula: \\[χ² = \\sum \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\\] where: - \\(O_{ij}\\) = Observed frequency for each cell in the contingency table - \\(E_{ij}\\) = Expected frequency for each cell under the assumption of independence\n\nchi2_stat, p_value, dof, expected = sp.stats.chi2_contingency(gender_category_tab)\n\nprint('Chi-square Statistic:', chi2_stat)\nprint('p-value:', p_value)\nprint('Degrees of freedom (dof):', dof)\n# print('Expected:', expected)\n\nChi-square Statistic: 40.7686907732235\np-value: 1.044840181761602e-07\nDegrees of freedom (dof): 5\n\n\nInterpretation of Chi2 Test Results:\n\nThe Chi-square statistic measures the difference between the observed frequencies in the contingency table and the frequencies that would be expected if the variables were independent.\nThe p-value is the probability of obtaining a Chi-square statistic as extreme as, or more extreme than, the one observed in the sample, assuming that the null hypothesis is true (i.e., assuming that there is no association between the variables).\nA low p-value indicates strong evidence against the null hypothesis, suggesting that there is a significant association between the variables.\nA high p-value indicates weak evidence against the null hypothesis, suggesting that there is no significant association between the variables.\n\n\nMeasures of Association for Categorical Variables:\n\nMeasures like Cramer’s V or phi coefficient quantify the strength of association between two categorical variables.\nThese measures are based on chi-square statistics and the dimensions of the contingency table.\n\n\n\n\nThe formula for Cramer’s V is:\n\\[V = \\sqrt{\\frac{χ²}{n(k - 1)}}\\]\nWhere: - \\(χ²\\) is the chi-square statistic from the chi-square test of independence. - \\(n\\) is the total number of observations in the contingency table. - \\(k\\) is the minimum of the number of rows and the number of columns in the contingency table.\nCramer’s V is a normalized measure of association, making it easier to interpret compared to the raw chi-square statistic. A larger value of Cramer’s V indicates a stronger association between the variables.\n\nfrom dataidea.statistics import cramersV\n\n\ncramersV(contingency_table=gender_category_tab)\n\nNameError: name 'sp' is not defined\n\n\n\nfrom dataidea.statistics import cramersVCorrected\n\n\ncramersVCorrected(gender_category_tab)\n\n0.19371955249110775\n\n\nCramer’s V is measure of association between two categorical variables. It ranges from 0 to 1 where:\n\n0 indicates no association between the variables\n1 indicates a perfect association between the variables\n\nHere’s an interpretation of the Cramer’s V:\n\nSmall effect: Around 0.1\nMedium effect: Around 0.3\nLarge effect: Around 0.5 or greater\n\n\nFrequency Tables\nFrequency means the number of times a value appears in the data. A table can quickly show us how many times each value appears. If the data has many different values, it is easier to use intervals of values to present them in a table.\nHere’s the age of the 934 Nobel Prize winners up until the year 2020. IN the table, each row is an age interval of 10 years\n\n\n\nAge Interval\nFrequency\n\n\n\n\n10-19\n1\n\n\n20-29\n2\n\n\n30-39\n48\n\n\n40-49\n158\n\n\n50-59\n236\n\n\n60-69\n262\n\n\n70-79\n174\n\n\n80-89\n50\n\n\n90-99\n3\n\n\n\nNote: The intervals for the values are also called bin\n\n\n\nFurther Reading\nChapter 3 of An Introduction to Statistical Methods and Data Analysis 7th Edition_New\n\nTo be among the first to hear about future updates of the course materials, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel.",
    "crumbs": [
      "Python-Data-Analysis",
      "Week5-Statistics",
      "Descriptive Statistics"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week5-Statistics/53_implementing_statistical_models.html",
    "href": "Python-Data-Analysis/Week5-Statistics/53_implementing_statistical_models.html",
    "title": "Statistical Models",
    "section": "",
    "text": "Photo by DATAIDEA\n## Uncomment and run this cell to install the packages\n# !pip install pandas numpy statsmodels",
    "crumbs": [
      "Python-Data-Analysis",
      "Week5-Statistics",
      "Statistical Models"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week5-Statistics/53_implementing_statistical_models.html#implementing-statistical-models-in-python",
    "href": "Python-Data-Analysis/Week5-Statistics/53_implementing_statistical_models.html#implementing-statistical-models-in-python",
    "title": "Statistical Models",
    "section": "Implementing Statistical Models in Python",
    "text": "Implementing Statistical Models in Python\n\nStatistical models\nStatistical models are mathematical representations of relationships between variables in a dataset. These models are used to make predictions, infer causal relationships, and understand patterns in data. Statistical modeling involves formulating hypotheses about the data generating process, estimating model parameters from observed data, and evaluating the fit of the model to the data.\nThe statsmodels.api library in Python provides a wide range of tools for statistical modeling and inference. It allows users to build, estimate, and analyze various statistical models using a simple and intuitive interface.\n\nimport statsmodels.api as sm\nimport numpy as np\n\n\nLinear Regression:\n\nLinear regression is used to model the relationship between one or more independent variables and a continuous dependent variable.\n\n\n\n# Generate example data\nnp.random.seed(0)\nX = np.random.rand(100, 2)  # Two independent variables\n\n\n# Dependent variable with noise\n\ny = 2 * X[:, 0] + 3 * X[:, 1] + np.random.normal(0, 1, 100)\n\n\n# Add constant term for intercept\nX = sm.add_constant(X)\n\n\n# Fit linear regression model\nmodel = sm.OLS(y, X).fit()\n\n\n# Print model summary\nprint(model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.440\nModel:                            OLS   Adj. R-squared:                  0.428\nMethod:                 Least Squares   F-statistic:                     38.06\nDate:                Wed, 17 Apr 2024   Prob (F-statistic):           6.31e-13\nTime:                        12:22:26   Log-Likelihood:                -140.25\nNo. Observations:                 100   AIC:                             286.5\nDf Residuals:                      97   BIC:                             294.3\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.2554      0.277      0.922      0.359      -0.294       0.805\nx1             1.4260      0.356      4.011      0.000       0.720       2.132\nx2             2.8054      0.351      8.004      0.000       2.110       3.501\n==============================================================================\nOmnibus:                        1.210   Durbin-Watson:                   2.349\nProb(Omnibus):                  0.546   Jarque-Bera (JB):                0.703\nSkew:                           0.122   Prob(JB):                        0.704\nKurtosis:                       3.330   Cond. No.                         5.58\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n0.2554/0.277\n\n0.9220216606498195\n\n\n\nR-squared measures how well the independant variables explain the variability of the dependant variable\nF-Statistic measures the significance of the regression model\nt-statistic for each coefficient measures the significance level of each independent variable\n\n\nLogistic Regression:\n\nLogistic regression is used when the dependent variable is binary (e.g., 0 or 1, True or False).\n\n\n\n# Generate example data for logistic regression\nnp.random.seed(0)\nX = np.random.rand(100, 2)  # Two independent variables\n# Generate binary outcome variable based on a threshold\nthreshold = 0.6\ny = (2 * X[:, 0] + 3 * X[:, 1] &gt; threshold).astype(int)\n\n# Add constant term for intercept\nX = sm.add_constant(X)\n\n# Fit logistic regression model\nlogit_model = sm.Logit(y, X).fit()\n\n# Print model summary\nprint(logit_model.summary())\n\nWarning: Maximum number of iterations has been exceeded.\n         Current function value: 0.000000\n         Iterations: 35\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:                      y   No. Observations:                  100\nModel:                          Logit   Df Residuals:                       97\nMethod:                           MLE   Df Model:                            2\nDate:                Wed, 17 Apr 2024   Pseudo R-squ.:                   1.000\nTime:                        13:05:57   Log-Likelihood:            -1.1958e-06\nconverged:                      False   LL-Null:                       -9.8039\nCovariance Type:            nonrobust   LLR p-value:                 5.524e-05\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst        -70.8342   4.95e+04     -0.001      0.999   -9.71e+04     9.7e+04\nx1           196.7613   1.92e+05      0.001      0.999   -3.76e+05    3.76e+05\nx2           488.7691   7.23e+05      0.001      0.999   -1.42e+06    1.42e+06\n==============================================================================\n\nComplete Separation: The results show that there iscomplete separation or perfect prediction.\nIn this case the Maximum Likelihood Estimator does not exist and the parameters\nare not identified.\n\n\n/home/jumashafara/venvs/dataanalysis/lib/python3.10/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n  warnings.warn(\"Maximum Likelihood optimization failed to \"\n\n\nThese examples demonstrate how to implement linear regression and logistic regression using statsmodels.api. The summary output provides detailed information about the model parameters, goodness-of-fit measures, and statistical significance of predictors. This can be useful for interpreting the results and assessing the performance of the models.\n\nTo be among the first to hear about future updates, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel.",
    "crumbs": [
      "Python-Data-Analysis",
      "Week5-Statistics",
      "Statistical Models"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week1-Intro/11_python_tutorial.html",
    "href": "Python-Data-Analysis/Week1-Intro/11_python_tutorial.html",
    "title": "DATAIDEA",
    "section": "",
    "text": "Photo by DATAIDEA",
    "crumbs": [
      "Python-Data-Analysis",
      "Week1-Intro",
      "Python Quick Review"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week1-Intro/11_python_tutorial.html#python-quick-review",
    "href": "Python-Data-Analysis/Week1-Intro/11_python_tutorial.html#python-quick-review",
    "title": "DATAIDEA",
    "section": "Python Quick Review",
    "text": "Python Quick Review",
    "crumbs": [
      "Python-Data-Analysis",
      "Week1-Intro",
      "Python Quick Review"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week1-Intro/11_python_tutorial.html#introduction",
    "href": "Python-Data-Analysis/Week1-Intro/11_python_tutorial.html#introduction",
    "title": "DATAIDEA",
    "section": "Introduction",
    "text": "Introduction\nPython is a great general-purpose programming language on its own, but with the help of a few popular libraries (numpy, scipy, matplotlib) it becomes a powerful environment for scientific computing.\nI expect that many of you will have some experience with Python; for the rest of you, this section will serve as a quick crash course both on the Python programming language and on the use of Python for scientific computing.\nIn this tutorial, we will cover:\n\nBasic Python: Basic data types (Containers, Lists, Dictionaries, Sets, Tuples), Functions, Classes",
    "crumbs": [
      "Python-Data-Analysis",
      "Week1-Intro",
      "Python Quick Review"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week1-Intro/11_python_tutorial.html#a-brief-note-on-python-versions",
    "href": "Python-Data-Analysis/Week1-Intro/11_python_tutorial.html#a-brief-note-on-python-versions",
    "title": "DATAIDEA",
    "section": "A Brief Note on Python Versions",
    "text": "A Brief Note on Python Versions\nAs of Janurary 1, 2024, Python has officially dropped support for python2. We’ll be using Python 3.10 for this iteration of the course. You can check your Python version at the command line by running python --version.\n\n# checking python version\n!python --version\n\nPython 3.10.12",
    "crumbs": [
      "Python-Data-Analysis",
      "Week1-Intro",
      "Python Quick Review"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week1-Intro/11_python_tutorial.html#basics-of-python",
    "href": "Python-Data-Analysis/Week1-Intro/11_python_tutorial.html#basics-of-python",
    "title": "DATAIDEA",
    "section": "Basics of Python",
    "text": "Basics of Python\nPython is a high-level, dynamically typed multiparadigm programming language. Python code is often said to be almost like pseudocode, since it allows you to express very powerful ideas in very few lines of code while being very readable. As an example, here is an implementation of the classic quicksort algorithm in Python:\n\ndef quicksort(array):\n    if len(array) &lt;= 1:\n        return array\n    pivot = array[len(array) // 2]\n    left = [number for number in array if number &lt; pivot]\n    middle = [number for number in array if number == pivot]\n    right = [number for number in array if number &gt; pivot]\n    return quicksort(left) + middle + quicksort(right)\n\nquicksort([3,6,8,10,1,2,1])\n\n[1, 1, 2, 3, 6, 8, 10]\n\n\n\nsorted('listen', reverse=True)\n\n['t', 's', 'n', 'l', 'i', 'e']\n\n\n\nVariables\nVariables are stores of value\n\nname = 'Juma'\nage = 19\nid_number = 190045\n\n\nRules to consider\n\nVariable names should be meaningful eg number instead of x\nVariable names should contain only alpha-numberic characters, and maybe under_scores\nVariable names can only start with letters or an underscore\nVariable name cannot contain special characters\nVariables names are case sensitive\n\n\nname = 'Eva'\nlist_of_names = ['Eva', 'Shafara', 'Bob'] # snake case\n\n\ndef calculateBMI(weight_kg, height_m): # camel case\n    bmi = weight_kg / height_m ** 2\n    rounded_bmi = round(bmi, 3)\n    return rounded_bmi\n\n\ncalculateBMI(59, 1.63)\n\n22.206\n\n\n\nclass MathFunction:    # Pascal Case\n    def __init__(self, number):\n        self.number = number\n\n    def square(self):\n        return self.number ** 2\n\n    def cube(self):\n        return self.number ** 3\n\n\nmath = MathFunction(number=7)\n\nmath.square()\n\n49\n\n\n\n\n\nBasic data types\n\nNumbers\nIntegers and floats work as you would expect from other languages:\n\nnumber = 3\nprint('Number: ', number)\nprint('Type: ', type(number))\n\nNumber:  3\nType:  &lt;class 'int'&gt;\n\n\n\n# Quick number arithmetics\n\nprint(number + 1)   # Addition\nprint(number - 1)   # Subtraction\nprint(number * 2)   # Multiplication\nprint(number ** 2)  # Enumberponentiation\n\n4\n2\n6\n9\n\n\n\n# Some compound assingment operators\n\nnumber += 1 # number = number + 1\nprint(number)\nnumber *= 2\nprint(number)\nnumber /= 1 # number = number / 1\nprint(number)\nnumber -= 2\nprint(number)\n\n4\n8\n8.0\n6.0\n\n\n\nnumber = 2.5\nprint(type(number))\nprint(number, number + 1, number * 2, number ** 2)\n\n&lt;class 'float'&gt;\n2.5 3.5 5.0 6.25\n\n\n\n# complex numbers\nvector = 2 + 6j\ntype(vector)\n\ncomplex\n\n\n\n\nBooleans\nPython implements all of the usual operators for Boolean logic, but uses English words rather than symbols:\n\nt, f = True, False\ntype(t)\n\nbool\n\n\nNow we let’s look at the operations:\n\n# Logical Operators\n\nprint(t and f) # Logical AND;\nprint(t or f)  # Logical OR;\nprint(not t)   # Logical NOT;\nprint(t != f)  # Logical XOR;\n\nFalse\nTrue\nFalse\nTrue\n\n\n\n\nStrings\nA string is a sequence of characters under some quotes. Eg.\n\nhello = 'hello'   # String literals can use single quotes\nworld = \"world\"   # or double quotes; it does not matter\nprint(hello, len(hello))\n\nhello 5\n\n\n\n# We can string in python\nfull = hello + ' ' + world  # String concatenation\nprint(full)\n\nhello world\n\n\n\nhw12 = '{} {} {}'.format(hello, world, 12)  # string formatting\nprint(hw12)\n\nhello world 12\n\n\n\nstatement = 'I love to code in {}'\nmodified = statement.format('JavaScript')\nprint(modified)\n\nI love to code in JavaScript\n\n\n\n# formatting by indexing\nstatement = '{0} loves to code in {2} and {1}'\nstatement.format('Juma', 'Python', 'JavaScript')\n\n'Juma loves to code in JavaScript and Python'\n\n\n\n# formatting by name\nstatement = '{name} loves to code in {language1} and {language2}'\nstatement.format(language2='Python', name='Juma', language1='JavaScript')\n\n'Juma loves to code in JavaScript and Python'\n\n\n\n# String Literal Interpolation\nname = 'Juma'\nlanguage1 = 'JavaScript'\nlanguage2 = 'Python'\n\nstatement = f'{name} loves to code in {language1} and {language2}'\n\nprint(statement)\n\nJuma loves to code in JavaScript and Python\n\n\nString objects have a bunch of useful methods; for example:\n\nstring_ = \"hello\"\nprint(string_.capitalize())  # Capitalize a string\nprint(string_.upper())       # Convert a string to uppercase; prints \"HELLO\"\nprint(string_.rjust(7))      # Right-justify a string, padding with spaces\nprint(string_.center(7))     # Center a string, padding with spaces\nprint(string_.replace('l', '(ell)'))  # Replace all instances of one substring with another\nprint('  world '.strip())  # Strip leading and trailing whitespace\n\nHello\nHELLO\n  hello\n hello \nhe(ell)(ell)o\nworld\n\n\n\nstatement = 'i love to code in Python '\n\ncapitalized = statement.capitalize()\nupped = statement.upper()\nreplaced = statement.replace('Python', 'javascript')\nstatement.strip()\n\n'i love to code in Python'\n\n\nYou can find a list of all string methods in the documentation.\n\n\n\nContainers\n\nPython containers (collections) are objects that we use to group other objects\nPython includes several built-in container types: lists, dictionaries, sets, and tuples.\n\n\nLists\nA list is an ordered collection of python objects or elements. A list can contain objects of different data types\n\nlist_of_numbers = [3, 1, 2]   # Create a list\nprint(list_of_numbers)\nprint(list_of_numbers[2])\nprint(list_of_numbers[-1])     # Negative indices count from the end of the list; prints \"2\"\n\n[3, 1, 2]\n2\n2\n\n\n\nlist_of_numbers[2] = 'foo'    # replacing a specific value in a list\nprint(list_of_numbers)\n\n[3, 1, 'foo']\n\n\n\nlist_of_numbers.append('bar') # Add a new element to the end of the list\nprint(list_of_numbers)\n\n[3, 1, 'foo', 'bar']\n\n\n\nlast_item = list_of_numbers.pop()     # Remove and return the last element of the list\nprint(last_item)    # returns the last item \nprint(list_of_numbers) # Modifies the original list\n\nbar\n[3, 1, 'foo']\n\n\nResearch on: - del - remove()\nAs usual, you can find all the gory details about lists in the documentation.\n\n\nSlicing\nIn addition to accessing list elements one at a time, Python provides concise syntax to access a range of values in a list; this is known as slicing:\n\nlist_of_numbers = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\nprint(list_of_numbers)\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\n\nprint(list_of_numbers)         # Prints \"[0, 1, 2, 3, 4]\"\nprint(list_of_numbers[2:4])    # Get a slice from index 2 to 4 (exclusive); prints \"[2, 3]\"\nprint(list_of_numbers[2:])     # Get a slice from index 2 to the end; prints \"[2, 3, 4]\"\nprint(list_of_numbers[:2])     # Get a slice from the start to index 2 (exclusive); prints \"[0, 1]\"\nprint(list_of_numbers[:])      # Get a slice of the whole list; prints [\"0, 1, 2, 3, 4]\"\nprint(list_of_numbers[:-1])    # Slice indices can be negative; prints [\"0, 1, 2, 3]\"\nlist_of_numbers[2:4] = [8, 9] # Assign a new sublist to a slice\nprint(list_of_numbers)         # Prints \"[0, 1, 8, 9, 4]\"\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n[2, 3]\n[2, 3, 4, 5, 6, 7, 8, 9]\n[0, 1]\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n[0, 1, 2, 3, 4, 5, 6, 7, 8]\n[0, 1, 8, 9, 4, 5, 6, 7, 8, 9]\n\n\n\n\nLoops\nA for loop is used to loop through (or iterate) over a sequence of objects (iterable objects). Iterable objects in python include strings, lists, sets etc\nYou can loop over the elements of a list like this:\n\nlist_of_animals = ['cat', 'dog', 'monkey']\n\nfor animal in list_of_animals:\n    print(animal)\n\ncat\ndog\nmonkey\n\n\n\nlist_of_numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 0]\nlist_of_squared_numbers = []\n\nfor number in list_of_numbers:\n    list_of_squared_numbers.append(pow(number, 2))\n\nlist_of_squared_numbers\n\n[1, 4, 9, 16, 25, 36, 49, 64, 81, 0]\n\n\nIf you want access to the index of each element within the body of a loop, use the built-in enumerate function:\n\nanimals = ['cat', 'dog', 'monkey']\n\nfor index, animal in enumerate(animals):\n    print(f'{index}: {animal}')\n\n0: cat\n1: dog\n2: monkey\n\n\n\n\nList comprehensions:\n\nnumbers = [0, 1, 2, 3, 4]\nsquares = []\n\nfor number in numbers:\n    squares.append(pow(number, 2))\n\nprint(squares)\n\n[0, 1, 4, 9, 16]\n\n\nYou can make this code simpler using a list comprehension:\n\nlist_of_numbers = [0, 1, 2, 3, 4]\n\nsquares = [pow(number, 2) for number in list_of_numbers]\n\nprint(squares)\n\n[0, 1, 4, 9, 16]\n\n\nList comprehensions can also contain conditions:\n\nnumbers = [0, 1, 2, 3, 4]\n\neven_squares = [pow(number, 2) for number in numbers if number % 2 == 0]\n\nprint(even_squares)\n\n[0, 4, 16]\n\n\nResearch: - How to combine lists\n\n\nDictionaries\n\nA dictionary is an unordered and mutable collection of items\nA dictionary is created using curly brackets\nEach item in a dictionary contains a key/value pair\n\n\n# creating a dictionary\nperson = {\n    'first_name': 'Juma',\n    'last_name': 'Shafara',\n    'age': 51,\n    'married': True\n}\nperson\n\n{'first_name': 'Juma', 'last_name': 'Shafara', 'age': 51, 'married': True}\n\n\n\n# accessing items in a dictionary\nfirst_name = person['first_name']\nlast_name = person['last_name']\nfull_name = first_name + ' ' + last_name\n\n# display\nfull_name\n\n'Juma Shafara'\n\n\n\n# add items to a dictionary\nperson['hobby'] = 'Coding'\nperson\n\n{'first_name': 'Juma',\n 'last_name': 'Shafara',\n 'age': 51,\n 'married': True,\n 'hobby': 'Coding'}\n\n\n\nemail = person.get('email', 'email not available')\nprint(email)\n\nemail not available\n\n\n\n# modifying a value in a dictionay\nperson['married'] = False\nperson\n\n{'first_name': 'Juma',\n 'last_name': 'Shafara',\n 'age': 51,\n 'married': False,\n 'hobby': 'Coding'}\n\n\n\n# remove an item from a dictionary\nperson.pop('age')\nperson\n\n{'first_name': 'Juma',\n 'last_name': 'Shafara',\n 'married': False,\n 'hobby': 'Coding'}\n\n\nResearch: - How to remove an item using the del method - How to iterate over objects in a dictionary - Imitate list comprehension with dictionaries\nYou can find all you need to know about dictionaries in the documentation.\n\n\nSets\n\nA set is an unordered, immutable collection of distinct elements.\nA set is created using curly braces\nThe objects are placed inside the brackets and are separated by commas\nAs a simple example, consider the following:\n\n\nanimals = {'cat', 'dog'}\n\nprint('cat' in animals)   # Check if an element is in a set; prints \"True\"\nprint('fish' not in animals)  # prints \"True\"\n\nTrue\nTrue\n\n\n\nanimals.add('fish')      # Add an element to a set\n\nprint('fish' in animals) # Returns \"True\"\n\nprint(len(animals))       # Number of elements in a set;\n\nTrue\n3\n\n\n\nanimals.add('cat')       # Adding an element that is already in the set does nothing\nprint(len(animals)) \n      \nanimals.remove('cat')    # Remove an element from a set\nprint(len(animals))\n\n3\n2\n\n\nResearch: - How to remove with discard() - How to remove with pop() - How to combine sets - How to get the difference between 2 sets - What happens when we have repeated elements in a set\nLoops: Iterating over a set has the same syntax as iterating over a list; however since sets are unordered, you cannot make assumptions about the order in which you visit the elements of the set:\n\nanimals = {'cat', 'dog', 'fish'}\n\nfor index, animal in enumerate(animals):\n    print(f'{index}: {animal}')\n\n0: fish\n1: cat\n2: dog\n\n\nSet comprehensions: Like lists and dictionaries, we can easily construct sets using set comprehensions:\n\nfrom math import sqrt\n\nprint({int(sqrt(x)) for x in range(30)})\n\n{0, 1, 2, 3, 4, 5}\n\n\n\n\nTuples\n\nA tuple is an (immutable) ordered list of values.\nA tuple is in many ways similar to a list; one of the most important differences is that tuples can be used as keys in dictionaries and as elements of sets, while lists cannot. Here is a trivial example:\n\n\nd = {(x, x + 1): x for x in range(10)}  # Create a dictionary with tuple keys\nt = (5, 6)       # Create a tuple\nprint(type(t))\nprint(d[t])       \nprint(d[(1, 2)])\n\n&lt;class 'tuple'&gt;\n5\n1\n\n\n\n# t[0] = 1\n\nResearch: - Creating a tuple - Access items in a tuple - Negative indexing tuples - Using range of indexes - Getting the length of items in a tuple - Looping through a tuple - Checking if an item exists in a tuple - How to combine tuples - Prove that tuples are immutable\n\n\n\nFunctions\n\nA function is a group of statements that performs a particular task\nPython functions are defined using the def keyword. For example:\n\n\ndef overWeightOrUnderweightOrNormal(weight_kg:float, height_m:float) -&gt; str:\n    '''\n    Tells whether someone is overweight or underweight or normal\n    '''\n    height_m2 = pow(height_m, 2)\n    bmi = weight_kg / height_m2\n    rounded_bmi = round(bmi, 3)\n    if bmi &gt; 24:\n        return 'Overweight'\n    elif bmi &gt; 18:\n        return 'Normal'\n    else:\n        return 'Underweight'\n\noverWeightOrUnderweightOrNormal(67, 1.7)\n\n'Normal'\n\n\nWe will often use functions with optional keyword arguments, like this:\n\nbmi = calculateBMI(height_m=1.7, weight_kg=67)\n\nprint(bmi)\n\n23.183\n\n\n\ndef greet(name:str='You')-&gt;str:\n    \"\"\"\n    This function greets people by name\n    Example1:\n    &gt;&gt;&gt; greet(name='John Doe')\n    &gt;&gt;&gt; 'Hello John Doe'\n    Example2:\n    &gt;&gt;&gt; greet()\n    &gt;&gt;&gt; 'Hello You'\n    \"\"\"\n    return f'Hello {name}'\n\n# greet('Eva')\n?greet\n\n\nSignature: greet(name: str = 'You') -&gt; str\nDocstring:\nThis function greets people by name\nExample1:\n&gt;&gt;&gt; greet(name='John Doe')\n&gt;&gt;&gt; 'Hello John Doe'\nExample2:\n&gt;&gt;&gt; greet()\n&gt;&gt;&gt; 'Hello You'\nFile:      /tmp/ipykernel_66386/2049930273.py\nType:      function\n\n\n\n\n\nClasses\n\nIn python, everything is an object\nWe use classes to help us create new object\nThe syntax for defining classes in Python is straightforward:\n\n\nclass Person:\n    first_name = 'John'\n    last_name = 'Tong'\n    age = 20\n\n\n# Instantiating a class\nobject1 = Person()\n\nprint(object1.first_name)\nprint(object1.last_name)\nprint(object1.age)\n\nprint(f'object1 type: {type(object1)}')\n\nJohn\nTong\n20\nobject1 type: &lt;class '__main__.Person'&gt;\n\n\n\n# Instantiating a class\nobject2 = Person()\n\nprint(object2.first_name)\nprint(object2.last_name)\nprint(object2.age)\n\nJohn\nTong\n20\n\n\n\nclass Person:\n    def __init__(self, first_name, last_name, age):\n        self.first_name = first_name\n        self.last_name = last_name\n        self.age = age\n\n    def greet(self, name):\n        return f'Hello {name}'\n\n\nobject1 = Person('Juma', 'Shafara', 24)\nprint(object1.first_name)\nprint(object1.last_name)\nprint(object1.age)\nprint(type(object1))\n\nJuma\nShafara\n24\n&lt;class '__main__.Person'&gt;\n\n\n\nobject2 = Person('Eva', 'Ssozi', 24)\nprint(object2.first_name)\nprint(object2.last_name)\nprint(object2.age)\nprint(object2.greet('Shafara'))\nprint(type(object2))\n\nEva\nSsozi\n24\nHello Shafara\n&lt;class '__main__.Person'&gt;\n\n\n\nclass Student(Person):\n    def __init__(self, first_name, last_name, age, id_number, subjects=[]):\n        super().__init__(first_name, last_name, age)\n        self.id_number = id_number\n        self.subjects = subjects\n\n    def addSubject(self, subject):\n        self.subjects.append(subject)\n\n\nstudent1 = Student('Calvin', 'Masaba', 34, '200045', ['math', 'science'])\n\n\nstudent1.addSubject('english')\n\n\nstudent1.subjects\n\n['math', 'science', 'english']\n\n\nResearch: - Inheritance: This allows to create classes that inherit the attributes and methods of another class\nA few ads maybe displayed for income as resources are now offered freely. 🤝🤝🤝",
    "crumbs": [
      "Python-Data-Analysis",
      "Week1-Intro",
      "Python Quick Review"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week1-Intro/pandas.html",
    "href": "Python-Data-Analysis/Week1-Intro/pandas.html",
    "title": "Pandas",
    "section": "",
    "text": "Photo by DATAIDEA\nPandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool.\n# Uncomment and run this cell to install pandas\n# !pip install pandas\n # !pip install openpyxl\n# import pandas\nimport pandas as pd\n# to check python version\npd.__version__\n\n'2.2.2'",
    "crumbs": [
      "Python-Data-Analysis",
      "Week1-Intro",
      "Pandas"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week1-Intro/pandas.html#creating-dataframes",
    "href": "Python-Data-Analysis/Week1-Intro/pandas.html#creating-dataframes",
    "title": "Pandas",
    "section": "Creating Dataframes",
    "text": "Creating Dataframes\nThe reason why data analysts like pandas is because pandas provides them with a very powerful data structure called a dataframe. A dataframe is a 2D structure that offers us rows and columns similar to tables in excel, sql etc\n\n \n\n\nCreating dataframes from existing files\nIf you already have some data in maybe an excel, csv, or stata file, you can be able to load it into a dataframe and then perform manipulation.\n\n# loading an excel file into a dataframe\ndata = pd.read_excel(io='../assets/demo.xlsx')\n\nThe data structure that is returned by the statement is called a DataFrame\n\n# checking the datatype of the data object\ntype(data)\n\npandas.core.frame.DataFrame\n\n\n\n# randomly sample some values\ndata.sample(n=5)\n\n\n\n\n\n\n\n\nAge\nGender\nMarital Status\nAddress\nIncome\nIncome Category\nJob Category\n\n\n\n\n104\n42\nf\n1\n9\n49\n2\n2\n\n\n9\n29\nf\nno answer\n4\n19\n1\n2\n\n\n139\n38\nf\n1\n14\n48\n2\n1\n\n\n78\n51\nf\n0\n10\n75\n4\n2\n\n\n134\n24\nm\n0\n5\n22\n1\n1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreating a DataFrame from a Dictionary\nFor the previous case, we may be having some data already, but sometimes we may want to create a dataframe from scratch.We can create pandas dataframes using two major ways:\n\nUsing a dictionary\nUsing a 2D list\n\nWe’ve met dictionaries and lists in the Containers/Collections module of the Python 3 Beginner Course.\nTo begin with, we’re gonna create a dataframe from a dictionary. The way we create the dictionary is important, keys will be used as column names, and the values will be used as rows. So, you typically want your values to be lists or tuples.\nNow you might observe that the values (lists) are of equal length\n\n# create a pandas dataframe using a dictionary\ndata_dictionary = {\n    'age': [65, 51, 45, 38, 40],\n    'gender': ['m', 'm', 'm', 'f', 'm'],\n    'income': [42, 148, 147, 43, 89]\n}\n\ndataframe_from_dict = pd.DataFrame(data=data_dictionary)\n\n\n# display the dataframe\ndataframe_from_dict\n\n\n\n\n\n\n\n\nage\ngender\nincome\n\n\n\n\n0\n65\nm\n42\n\n\n1\n51\nm\n148\n\n\n2\n45\nm\n147\n\n\n3\n38\nf\n43\n\n\n4\n40\nm\n89\n\n\n\n\n\n\n\nNext up, we are gonna create a dataframe from a list. For this case, the list be of 2D shape. Again, the way we organize data in our list is important. We should organize that in a format close to rows and columns as showed below.\nIt turns out that when creating a dataframe from a list, we need to explicitly define the column names as demonstrated below\n\n# creating a dataframe from a 2D list\ndata_list = [\n    [28, 'm', 24],\n    [59, 'm', 841],\n    [54, 'm', 741],\n    [83, 'f', 34],\n    [34, 'm', 98]\n]\n# let's specify the column names\nnames = ['age', 'gender', 'income']\n\ndataframe_from_list = pd.DataFrame(data=data_list, \n                                   columns=names)\n\n\n# display the dataframe\ndataframe_from_list\n\n\n\n\n\n\n\n\nage\ngender\nincome\n\n\n\n\n0\n28\nm\n24\n\n\n1\n59\nm\n841\n\n\n2\n54\nm\n741\n\n\n3\n83\nf\n34\n\n\n4\n34\nm\n98\n\n\n\n\n\n\n\nBefore we continue, I would like to share some ways you would look for help or more information about pandas methods.\n\nOne way is by using the help() method.\nAnother is by using the query operator ?\n\n\n# Finding more information\n# help(pd.DataFrame)\n\n\n## Another way to find more information\n# ?pd.DataFrame\n\nThe latter is my favorite because it works in all situations.",
    "crumbs": [
      "Python-Data-Analysis",
      "Week1-Intro",
      "Pandas"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week1-Intro/pandas.html#concatenating-dataframes",
    "href": "Python-Data-Analysis/Week1-Intro/pandas.html#concatenating-dataframes",
    "title": "Pandas",
    "section": "Concatenating DataFrames",
    "text": "Concatenating DataFrames\n\n \n\nSometimes there’s a need to add two or more dataframes. To perform this, for the start, we can use the pd.concat(). The concat() method takes in a list of dataframes we would like to combine\nRemember we created 2 dataframes earlier, one from a dictionary and another from a list, now let’s combine them to make one dataframe\n\nconcatenated_dataframe = pd.concat(\n    objs=[dataframe_from_dict, dataframe_from_list], \n    ignore_index=True\n)\n\n\nconcatenated_dataframe\n\n\n\n\n\n\n\n\nage\ngender\nincome\n\n\n\n\n0\n65\nm\n42\n\n\n1\n51\nm\n148\n\n\n2\n45\nm\n147\n\n\n3\n38\nf\n43\n\n\n4\n40\nm\n89\n\n\n5\n28\nm\n24\n\n\n6\n59\nm\n841\n\n\n7\n54\nm\n741\n\n\n8\n83\nf\n34\n\n\n9\n34\nm\n98\n\n\n\n\n\n\n\nWe set ignore_index=True to correct the indexing so that we can have unique indexes and hence be able to able to uniquely identify rows by index\nExercise: Demonstrate how to concatenate two or more dataframes by column ie if dataframe A has columns a, b, c and dataframe B has columns x, y, z, the resulting dataframe should have columns a, b, c, x, y, z",
    "crumbs": [
      "Python-Data-Analysis",
      "Week1-Intro",
      "Pandas"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week1-Intro/pandas.html#sampling-values-in-the-dataframe",
    "href": "Python-Data-Analysis/Week1-Intro/pandas.html#sampling-values-in-the-dataframe",
    "title": "Pandas",
    "section": "Sampling values in the DataFrame",
    "text": "Sampling values in the DataFrame\nIn this section, we are gonna look at how to pick out some sections or parts of the data. We’ll look at head(), tail() and sample().\nTo demonstrate these, we’ll continue with our concatenated dataframe from the previous section\nWe can use head() to look at the top part of the data. Out of the box, it returns the top 5 rows, however modifying the value for n can help us pick a specific number of rows from the top.\n\n# We can have look at the top part \nconcatenated_dataframe.head(n=3)\n\n\n\n\n\n\n\n\nage\ngender\nincome\n\n\n\n\n0\n65\nm\n42\n\n\n1\n51\nm\n148\n\n\n2\n45\nm\n147\n\n\n\n\n\n\n\nWe can use tail() to look at the bottom part of the data. Out of the box, it returns the bottom 5 rows, however modifying the value for n can help us pick a specific number of rows from the bottom.\n\n# We can look at the bottom part\nconcatenated_dataframe.tail(n=3)\n\n\n\n\n\n\n\n\nage\ngender\nincome\n\n\n\n\n7\n54\nm\n741\n\n\n8\n83\nf\n34\n\n\n9\n34\nm\n98\n\n\n\n\n\n\n\nWe can use sample() to look random rows data rows. Out of the box, it returns only 1 row, however modifying the value for n can help us pick a specific number of rows at random.\n\n# We can also randomly sample out some values in a DataFrame\nconcatenated_dataframe.sample(n=3)\n\n\n\n\n\n\n\n\nage\ngender\nincome\n\n\n\n\n8\n83\nf\n34\n\n\n6\n59\nm\n841\n\n\n7\n54\nm\n741",
    "crumbs": [
      "Python-Data-Analysis",
      "Week1-Intro",
      "Pandas"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week1-Intro/pandas.html#selection",
    "href": "Python-Data-Analysis/Week1-Intro/pandas.html#selection",
    "title": "Pandas",
    "section": "Selection",
    "text": "Selection\nIn this section we are gonna look at some tricks and techniques we can use to pick some really specific values from the dataframe\n\nSelecting, Boolean Indexing and Setting\nTo demonstrate these, we’re creating a little countries dataframe. As you may observe we use pd.DataFrame() to create our dataframe and notice we’re passing in a dictionary for the value of data.\n\ncountry_data = pd.DataFrame(data={\n    'Country': ['Uganda', 'Kenya', 'Tanzania'],\n    'Capital': ['Kampala', 'Nairobi', 'Dodoma'],\n    'Population': [11190846, 1303171035, 207847528]\n    })\ncountry_data\n\n\n\n\n\n\n\n\nCountry\nCapital\nPopulation\n\n\n\n\n0\nUganda\nKampala\n11190846\n\n\n1\nKenya\nNairobi\n1303171035\n\n\n2\nTanzania\nDodoma\n207847528\n\n\n\n\n\n\n\nWe can pick a specific value (or values) from a dataframe by indexing usin the iloc and iat methods. We insert the row number and the column number of an item that we want to pick from the dataframe in the square brackets.\nWe can also use these techniques to replace values in a dataframe.\n\n# position 1\nprint(country_data.iloc[0, 0])\nprint(country_data.iloc[2, 1])\n\nUganda\nDodoma\n\n\n\n# position 2\nprint(country_data.iat[0, 0])\nprint(country_data.iat[2, 1])\n\nUganda\nDodoma\n\n\nPonder:\n\nHow can you use the pd.DataFrame.iat method to replace (or modify) a specific value in a dataframe\n\nWe can access any value(s) by their row index and column name with the help of the loc[] and at[] methods.\nAs you may observe the difference now is that we are using row index and column name instead of row index and column index for iloc[] and iat[]\n\n# using label\nprint(country_data.loc[0, 'Capital'])\nprint(country_data.loc[1, 'Population'])\n\nKampala\n1303171035\n\n\n\n# using label\nprint(country_data.at[2, 'Population'])\nprint(country_data.at[1, 'Capital'])\n\n207847528\nNairobi\n\n\nWe can be able to pick out an entire column by either using the . operator or [].\n\nWe use the . operator when a column name is one single word\nWe can use the [] when the column name is containing more that one word\nWe can also use the [] when creating or assigning values to columns in a dataframe\n\n\n# picking out data from a specific column\ncountry_data.Country\n\n0      Uganda\n1       Kenya\n2    Tanzania\nName: Country, dtype: object\n\n\n\n# another way to pick data from a specific column\ncountry_data['Country']\n\n0      Uganda\n1       Kenya\n2    Tanzania\nName: Country, dtype: object\n\n\nThe data structure that is returned by the statement is called a Series\n\nlakes = ['Albert', 'Turkana', 'Tanganyika']\ncountry_data['Lake'] = lakes\n\n# lets display the updated data\ncountry_data\n\n\n\n\n\n\n\n\nCountry\nCapital\nPopulation\nLake\n\n\n\n\n0\nUganda\nKampala\n11190846\nAlbert\n\n\n1\nKenya\nNairobi\n1303171035\nTurkana\n\n\n2\nTanzania\nDodoma\n207847528\nTanganyika\n\n\n\n\n\n\n\n\n# lets check it\ntype(country_data['Capital'])\n\npandas.core.series.Series\n\n\n\n# Get specific row data (using indexing)\ncountry_data.iloc[0]\n\nCountry         Uganda\nCapital        Kampala\nPopulation    11190846\nLake            Albert\nName: 0, dtype: object\n\n\nWe can get a range of rows by passing into iloc[] a range of indexes. The demonstration below returns rows of indexes 0 until but not including 2 ie 0 for Uganda and 1 for Kenya\n\n# Get specific rows (using subsetting)\ncountry_data.iloc[0:2]\n\n\n\n\n\n\n\n\nCountry\nCapital\nPopulation\nLake\n\n\n\n\n0\nUganda\nKampala\n11190846\nAlbert\n\n\n1\nKenya\nNairobi\n1303171035\nTurkana\n\n\n\n\n\n\n\nWe can be able to pickout only rows whose values satisfy a specific condition, this trick is called boolean indexing. In the example below, we find all rows whose contry is Uganda\n\n# get all rows that have a column-value matching a specific value\n# eg where country is Belgium\ncountry_data[country_data['Country'] == 'Uganda']\n\n\n\n\n\n\n\n\nCountry\nCapital\nPopulation\nLake\n\n\n\n\n0\nUganda\nKampala\n11190846\nAlbert\n\n\n\n\n\n\n\n\n# Think about this\ncountry_data['Country'] == 'Tanzania'\n\n0    False\n1    False\n2     True\nName: Country, dtype: bool\n\n\nYou donot have to submit that",
    "crumbs": [
      "Python-Data-Analysis",
      "Week1-Intro",
      "Pandas"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week1-Intro/pandas.html#dropping",
    "href": "Python-Data-Analysis/Week1-Intro/pandas.html#dropping",
    "title": "Pandas",
    "section": "Dropping",
    "text": "Dropping\nIn this part we will learn some tricks and techniques to drop or remove some data from a dataframe.\n\ncountry_data\n\n\n\n\n\n\n\n\nCountry\nCapital\nPopulation\nLake\n\n\n\n\n0\nUganda\nKampala\n11190846\nAlbert\n\n\n1\nKenya\nNairobi\n1303171035\nTurkana\n\n\n2\nTanzania\nDodoma\n207847528\nTanganyika\n\n\n\n\n\n\n\nWe may realize that all our population values are terribly wrong and may choose the entire column. we can do this by using the drop() method. We specify the column name and axis=1 to drop a column.\n\n# drop a column from a dataframe\ncountry_data.drop(\n    labels='Population', \n    axis=1\n)\n\n\n\n\n\n\n\n\nCountry\nCapital\nLake\n\n\n\n\n0\nUganda\nKampala\nAlbert\n\n\n1\nKenya\nNairobi\nTurkana\n\n\n2\nTanzania\nDodoma\nTanganyika\n\n\n\n\n\n\n\nTo drop many columns, we can pass all the columns to the drop() method as a list or tuple, and again specify the axis=1.\n\ncountry_data.drop(\n    labels=['Lake', 'Population'], \n    axis=1\n)\n\n\n\n\n\n\n\n\nCountry\nCapital\n\n\n\n\n0\nUganda\nKampala\n\n\n1\nKenya\nNairobi\n\n\n2\nTanzania\nDodoma\n\n\n\n\n\n\n\nAnother way to drop many columns is by passing them to the drop() method as a list value to the columns parameter. In this case we don’t need to specify the axis.\n\n# You can drop many columns by passing in a columns list\ncountry_data.drop(columns=['Country', 'Population'])\n\n\n\n\n\n\n\n\nCapital\nLake\n\n\n\n\n0\nKampala\nAlbert\n\n\n1\nNairobi\nTurkana\n\n\n2\nDodoma\nTanganyika\n\n\n\n\n\n\n\nTo drop a row or many rows, we shall pass the index(es) as labels to the drop method method and optionally set axis=0. It turns out that the default value for axis is actually 0. Below, we have some two examples.\n\n# how to drop 1 row\ncountry_data.drop(labels=0) # drops out Uganda\n\n\n\n\n\n\n\n\nCountry\nCapital\nPopulation\nLake\n\n\n\n\n1\nKenya\nNairobi\n1303171035\nTurkana\n\n\n2\nTanzania\nDodoma\n207847528\nTanganyika\n\n\n\n\n\n\n\n\n# how to drop row data\ncountry_data.drop(labels=[0, 2], axis=0)\n\n\n\n\n\n\n\n\nCountry\nCapital\nPopulation\nLake\n\n\n\n\n1\nKenya\nNairobi\n1303171035\nTurkana\n\n\n\n\n\n\n\nThis drops rows in indexes 0 and 2, ie Uganda and Tanzania\n\n\n\n\n\n\nResearch on:\n\nsort and rank data",
    "crumbs": [
      "Python-Data-Analysis",
      "Week1-Intro",
      "Pandas"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week1-Intro/pandas.html#retrieving-information-about-dataframe",
    "href": "Python-Data-Analysis/Week1-Intro/pandas.html#retrieving-information-about-dataframe",
    "title": "Pandas",
    "section": "Retrieving information about DataFrame",
    "text": "Retrieving information about DataFrame\nPandas offers us some quick way with which we can find some quick information about our dataset (or dataframe)\n\nBasic Information\n\ncountry_data = pd.DataFrame({\n    'Country': ['Uganda', 'Kenya', 'Tanzania'],\n    'Capital': ['Kampala', None, None],\n    'Population': [11190846, 1303171035, 207847528]\n    })\n\ncountry_data\n\n\n\n\n\n\n\n\nCountry\nCapital\nPopulation\n\n\n\n\n0\nUganda\nKampala\n11190846\n\n\n1\nKenya\nNone\n1303171035\n\n\n2\nTanzania\nNone\n207847528\n\n\n\n\n\n\n\nWe can use the shape attribute to obtain the number of rows and columns available in our dataset as illustrated.\n\n# shape of a dataframe (ie rows, columns)\ncountry_data.shape\n\n(3, 3)\n\n\nIf you’re only interested in the number of rows you can use the len() method to find that eg\n\n# len of a dataframe (ie no of rows)\nlen(country_data)\n\n3\n\n\nIf you are interested in looking at the columns (column names), you can use the columns attribute to obtain the Index object of column names\n\n# Get all columns in a dataframe\ncountry_data.columns\n\nIndex(['Country', 'Capital', 'Population'], dtype='object')\n\n\nWe can also use the len() method on this Index object to obtain the number of columns\n\nlen(country_data.columns)\n\n3\n\n\nWe can use the info() method to find some information on our dataframe ie:\n\nColumns (All columns in the dataframe)\nNon-Null Count (Number of non null values per column)\nDtype (Data type each column)\nTotal number of entries (rows)\n\n\n# get some basic info about the dataframe\ncountry_data.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3 entries, 0 to 2\nData columns (total 3 columns):\n #   Column      Non-Null Count  Dtype \n---  ------      --------------  ----- \n 0   Country     3 non-null      object\n 1   Capital     1 non-null      object\n 2   Population  3 non-null      int64 \ndtypes: int64(1), object(2)\nmemory usage: 200.0+ bytes\n\n\nBy using the count() method on the dataframe, we can obtain the number of non-null values per a column\n\n# Count non-null values in each column\ncountry_data.count()\n\nCountry       3\nCapital       1\nPopulation    3\ndtype: int64\n\n\n\n\nSummary\nFinally, we can use the describe() to obtain some quick summary (descriptive) statistics about our data eg count, mean, standard deviation, minimum and maximum values, percentile\n\n# summary statistics\ncountry_data.describe()\n\n\n\n\n\n\n\n\nPopulation\n\n\n\n\ncount\n3.000000e+00\n\n\nmean\n5.074031e+08\n\n\nstd\n6.961346e+08\n\n\nmin\n1.119085e+07\n\n\n25%\n1.095192e+08\n\n\n50%\n2.078475e+08\n\n\n75%\n7.555093e+08\n\n\nmax\n1.303171e+09\n\n\n\n\n\n\n\n\n\n\n\n\n\nResearch\nFind out how to get for specific columns: - mean - median - cummulative sum - min - max",
    "crumbs": [
      "Python-Data-Analysis",
      "Week1-Intro",
      "Pandas"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week7-Preprocessing/73_normalization_and_standardization.html",
    "href": "Python-Data-Analysis/Week7-Preprocessing/73_normalization_and_standardization.html",
    "title": "Normalization vs Standardization",
    "section": "",
    "text": "Photo by DATAIDEA",
    "crumbs": [
      "Python-Data-Analysis",
      "Week7-Preprocessing",
      "Normalization vs Standardization"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week7-Preprocessing/73_normalization_and_standardization.html#normalization",
    "href": "Python-Data-Analysis/Week7-Preprocessing/73_normalization_and_standardization.html#normalization",
    "title": "Normalization vs Standardization",
    "section": "Normalization",
    "text": "Normalization\n\nNormalization: Normalization typically refers to scaling numerical features to a common scale, often between 0 and 1. This is usually done by subtracting the minimum value and then dividing by the range (maximum - minimum). Normalization is useful when the distribution of the data does not follow a Gaussian distribution (Normal Distribution).\n\n\n# Data Normalization without libraries:\ndef minMaxScaling(data):\n    min_val = min(data)\n    max_val = max(data)\n    \n    scaled_data = []\n    for value in data:\n        scaled = (value - min_val) / (max_val - min_val)\n        scaled_data.append(scaled)\n    return scaled_data\n\n\n# Example data\ndata = np.array([10, 20, 30, 40, 50])\nnormalized_data = minMaxScaling(data)\nprint(\"Normalized data (Min-Max Scaling):\", normalized_data)\n\nNormalized data (Min-Max Scaling): [0.0, 0.25, 0.5, 0.75, 1.0]\n\n\n\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Sample data\ndata = np.array([[1, 2], [3, 4], [5, 6]])\n\n# Create the scaler\nscaler = MinMaxScaler()\n\n# Fit the scaler to the data and transform the data\nnormalized_data = scaler.fit_transform(data)\n\nprint(\"Original data:\")\nprint(data)\nprint(\"\\nNormalized data:\")\nprint(normalized_data)\n\nOriginal data:\n[[1 2]\n [3 4]\n [5 6]]\n\nNormalized data:\n[[0.  0. ]\n [0.5 0.5]\n [1.  1. ]]\n\n\nLet’s now try on a real world dataset!\n\nboston_data = di.loadDataset('boston')\nboston_data.head()\n\n\n\n\n\n\n\n\nCRIM\nZN\nINDUS\nCHAS\nNOX\nRM\nAGE\nDIS\nRAD\nTAX\nPTRATIO\nB\nLSTAT\nMEDV\n\n\n\n\n0\n0.00632\n18.0\n2.31\n0\n0.538\n6.575\n65.2\n4.0900\n1\n296.0\n15.3\n396.90\n4.98\n24.0\n\n\n1\n0.02731\n0.0\n7.07\n0\n0.469\n6.421\n78.9\n4.9671\n2\n242.0\n17.8\n396.90\n9.14\n21.6\n\n\n2\n0.02729\n0.0\n7.07\n0\n0.469\n7.185\n61.1\n4.9671\n2\n242.0\n17.8\n392.83\n4.03\n34.7\n\n\n3\n0.03237\n0.0\n2.18\n0\n0.458\n6.998\n45.8\n6.0622\n3\n222.0\n18.7\n394.63\n2.94\n33.4\n\n\n4\n0.06905\n0.0\n2.18\n0\n0.458\n7.147\n54.2\n6.0622\n3\n222.0\n18.7\n396.90\n5.33\n36.2\n\n\n\n\n\n\n\n\nboston_scaler = MinMaxScaler()\nnormalized_data = boston_scaler.fit_transform(boston_data[['CRIM', 'AGE', 'TAX']])\nnp.set_printoptions(suppress=True)\nnormalized_data\n\narray([[0.        , 0.64160659, 0.20801527],\n       [0.00023592, 0.78269825, 0.10496183],\n       [0.0002357 , 0.59938208, 0.10496183],\n       ...,\n       [0.00061189, 0.90731205, 0.16412214],\n       [0.00116073, 0.88980433, 0.16412214],\n       [0.00046184, 0.80226571, 0.16412214]])",
    "crumbs": [
      "Python-Data-Analysis",
      "Week7-Preprocessing",
      "Normalization vs Standardization"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week7-Preprocessing/73_normalization_and_standardization.html#standardization",
    "href": "Python-Data-Analysis/Week7-Preprocessing/73_normalization_and_standardization.html#standardization",
    "title": "Normalization vs Standardization",
    "section": "Standardization",
    "text": "Standardization\n\nStandardization: Standardization, often implemented with a method like z-score standardization, transforms the data to have a mean of 0 and a standard deviation of 1. This means that the data will have a Gaussian distribution (if the original data had a Gaussian distribution).\n\n\ndef zScoreNormalization(data):\n    mean = sum(data) / len(data)\n    variance = sum((x - mean) ** 2 for x in data) / len(data)\n    std_dev = variance ** 0.5\n    standardized_data = [(x - mean) / std_dev for x in data]\n    return standardized_data\n\n\n# Example data\ndata = [10, 20, 30, 40, 50]\nstandardized_data = zScoreNormalization(data)\nprint(\"Standardized data (Z-Score Normalization):\", standardized_data)\n\nStandardized data (Z-Score Normalization): [-1.414213562373095, -0.7071067811865475, 0.0, 0.7071067811865475, 1.414213562373095]\n\n\nIn Python, we can also typically use the StandardScaler from the sklearn.preprocessing module to standardize data.\n\nfrom sklearn.preprocessing import StandardScaler\n\n# Sample data\ndata = np.array([[1, 2, 3], [3, 4, 5], [5, 6, 7]])\n\n# Create the scaler\nscaler = StandardScaler()\n\n# Fit the scaler to the data and transform the data\nstandardized_data = scaler.fit_transform(data)\n\nprint(\"Original data:\")\nprint(data)\nprint(\"\\nStandardized data:\")\nprint(standardized_data)\n\nOriginal data:\n[[1 2 3]\n [3 4 5]\n [5 6 7]]\n\nStandardized data:\n[[-1.22474487 -1.22474487 -1.22474487]\n [ 0.          0.          0.        ]\n [ 1.22474487  1.22474487  1.22474487]]\n\n\n\nboston_scaler = StandardScaler()\nstandardized_data = boston_scaler.fit_transform(boston_data[['CRIM', 'AGE', 'TAX']])\nnp.set_printoptions(suppress=True)\nstandardized_data\n\narray([[-0.41978194, -0.12001342, -0.66660821],\n       [-0.41733926,  0.36716642, -0.98732948],\n       [-0.41734159, -0.26581176, -0.98732948],\n       ...,\n       [-0.41344658,  0.79744934, -0.80321172],\n       [-0.40776407,  0.73699637, -0.80321172],\n       [-0.41500016,  0.43473151, -0.80321172]])\n\n\n\nImportance:\n\nData Normalization:\n\nUniform Scaling: Ensures all features are scaled to a similar range, preventing dominance by features with larger scales.\nImproved Convergence: Facilitates faster convergence in optimization algorithms by making the loss surface more symmetric.\nInterpretability: Easier interpretation as values are on a consistent scale, aiding in comparison and understanding of feature importance.\n\nData Standardization:\n\nMean Centering: Transforms data to have a mean of 0 and a standard deviation of 1, simplifying interpretation of coefficients in linear models.\nHandling Different Scales: Useful when features have different scales or units, making them directly comparable.\nReducing Sensitivity to Outliers: Less affected by outliers compared to normalization, leading to more robust models.\nMaintaining Information: Preserves relative relationships between data points without altering the distribution shape.",
    "crumbs": [
      "Python-Data-Analysis",
      "Week7-Preprocessing",
      "Normalization vs Standardization"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week7-Preprocessing/73_normalization_and_standardization.html#which-one",
    "href": "Python-Data-Analysis/Week7-Preprocessing/73_normalization_and_standardization.html#which-one",
    "title": "Normalization vs Standardization",
    "section": "Which one?",
    "text": "Which one?\nThe choice between normalization and standardization depends on your data and the requirements of your analysis. Here are some guidelines to help you decide:\n\nNormalization:\n\nUse normalization when the scale of features is meaningful and should be preserved.\nNormalize data when you’re working with algorithms that require input features to be on a similar scale, such as algorithms using distance metrics like k-nearest neighbors or clustering algorithms like K-means.\nIf the distribution of your data is not Gaussian and you want to scale the features to a fixed range, normalization might be a better choice.\n\nStandardization:\n\nUse standardization when the distribution of your data is Gaussian or when you’re unsure about the distribution.\nStandardization is less affected by outliers compared to normalization, making it more suitable when your data contains outliers.\nIf you’re working with algorithms that assume your data is normally distributed, such as linear regression, logistic regression, standardization is typically preferred.\n\n\nIn some cases, you might experiment with both approaches and see which one yields better results for your specific dataset and analysis. Additionally, it’s always a good practice to understand your data and the underlying assumptions of the algorithms you’re using to make informed decisions about data preprocessing techniques.\n\n\n\n\n\n\nTo be among the first to hear about future updates, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel.",
    "crumbs": [
      "Python-Data-Analysis",
      "Week7-Preprocessing",
      "Normalization vs Standardization"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week7-Preprocessing/76_handling_missing_data.html",
    "href": "Python-Data-Analysis/Week7-Preprocessing/76_handling_missing_data.html",
    "title": "DATAIDEA",
    "section": "",
    "text": "Photo by DATAIDEA",
    "crumbs": [
      "Python-Data-Analysis",
      "Week7-Preprocessing",
      "Handling Missing Data"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week7-Preprocessing/76_handling_missing_data.html#handling-missing-data",
    "href": "Python-Data-Analysis/Week7-Preprocessing/76_handling_missing_data.html#handling-missing-data",
    "title": "DATAIDEA",
    "section": "Handling Missing Data",
    "text": "Handling Missing Data",
    "crumbs": [
      "Python-Data-Analysis",
      "Week7-Preprocessing",
      "Handling Missing Data"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week7-Preprocessing/76_handling_missing_data.html#introduction",
    "href": "Python-Data-Analysis/Week7-Preprocessing/76_handling_missing_data.html#introduction",
    "title": "DATAIDEA",
    "section": "Introduction:",
    "text": "Introduction:\nMissing data is a common hurdle in data analysis, impacting the reliability of insights drawn from datasets. Python offers a range of solutions to address this issue, some of which we discussed in the earlier weeks. In this notebook, we look into the top four missing data imputation methods:\n\nSimpleImputer\nKNNImputer\nIterativeImputer\nDatawig\n\nWe’ll explore these essential techniques, using sklearn and the weather dataset.\n\n\nDon’t miss out on any updates and developments! Subscribe to the DATAIDEA Newsletter it’s easy and safe.\n\n\n\n\n\n# install the libraries for this demonstration\n# ! pip install -U dataidea\n\n\nimport pandas as pd\nimport dataidea as di\n\nfrom dataidea.packages import * imports for us np, pd, plt, etc. loadDataset allows us to load datasets inbuilt in the dataidea library\n\nweather = di.loadDataset('weather') \nweather\n\n\n\n\n\n\n\n\nday\ntemperature\nwindspead\nevent\n\n\n\n\n0\n01/01/2017\n32.0\n6.0\nRain\n\n\n1\n04/01/2017\nNaN\n9.0\nSunny\n\n\n2\n05/01/2017\n28.0\nNaN\nSnow\n\n\n3\n06/01/2017\nNaN\n7.0\nNaN\n\n\n4\n07/01/2017\n32.0\nNaN\nRain\n\n\n5\n08/01/2017\nNaN\nNaN\nSunny\n\n\n6\n09/01/2017\nNaN\nNaN\nNaN\n\n\n7\n10/01/2017\n34.0\n8.0\nCloudy\n\n\n8\n11/01/2017\n40.0\n12.0\nSunny\n\n\n\n\n\n\n\n\nweather.isna().sum()\n\nday            0\ntemperature    4\nwindspead      4\nevent          2\ndtype: int64\n\n\nLet’s demonstrate how to use the top three missing data imputation methods—SimpleImputer, KNNImputer, and IterativeImputer—using the simple weather dataset.\n\n# select age from the data\ntemp_wind = weather[['temperature', 'windspead']].copy()\n\n\ntemp_wind_imputed = temp_wind.copy()",
    "crumbs": [
      "Python-Data-Analysis",
      "Week7-Preprocessing",
      "Handling Missing Data"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week7-Preprocessing/76_handling_missing_data.html#simpleimputer-from-scikit-learn",
    "href": "Python-Data-Analysis/Week7-Preprocessing/76_handling_missing_data.html#simpleimputer-from-scikit-learn",
    "title": "DATAIDEA",
    "section": "SimpleImputer from scikit-learn:",
    "text": "SimpleImputer from scikit-learn:\n\nUsage: SimpleImputer is a straightforward method for imputing missing values by replacing them with a constant, mean, median, or most frequent value along each column.\nPros:\n\nEasy to use and understand.\nCan handle both numerical and categorical data.\nOffers flexibility with different imputation strategies.\n\nCons:\n\nIt doesn’t consider relationships between features.\nMay not be the best choice for datasets with complex patterns of missingness.\n\nExample:\n\n\nfrom sklearn.impute import SimpleImputer\n\nsimple_imputer = SimpleImputer(strategy='mean')\ntemp_wind_simple_imputed = simple_imputer.fit_transform(temp_wind)\n\ntemp_wind_simple_imputed_df = pd.DataFrame(temp_wind_simple_imputed, columns=temp_wind.columns)\n\nLet’s have a look at the outcome\n\ntemp_wind_simple_imputed_df\n\n\n\n\n\n\n\n\ntemperature\nwindspead\n\n\n\n\n0\n32.0\n6.0\n\n\n1\n33.2\n9.0\n\n\n2\n28.0\n8.4\n\n\n3\n33.2\n7.0\n\n\n4\n32.0\n8.4\n\n\n5\n33.2\n8.4\n\n\n6\n33.2\n8.4\n\n\n7\n34.0\n8.0\n\n\n8\n40.0\n12.0\n\n\n\n\n\n\n\n\nExercise:\n\nTry out the SimpleImputer with different imputation strategies like mode, constant\nChoose and try some imputation techniques on categorical data",
    "crumbs": [
      "Python-Data-Analysis",
      "Week7-Preprocessing",
      "Handling Missing Data"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week7-Preprocessing/76_handling_missing_data.html#knnimputer-from-scikit-learn",
    "href": "Python-Data-Analysis/Week7-Preprocessing/76_handling_missing_data.html#knnimputer-from-scikit-learn",
    "title": "DATAIDEA",
    "section": "KNNImputer from scikit-learn:",
    "text": "KNNImputer from scikit-learn:\n\nUsage:\n\nKNNImputer imputes missing values using k-nearest neighbors, replacing them with the mean value of the nearest neighbors.\nYou can read more about the KNNImputer from the sklearn official docs site\n\nPros:\n\nConsiders relationships between features, making it suitable for datasets with complex patterns of missingness.\nCan handle both numerical and categorical data.\n\nCons:\n\nComputationally expensive for large datasets.\nRequires careful selection of the number of neighbors (k).\n\n\n\n\nNote!\n\n\nBy default, the KNNImputer uses ‘nan’ values as missing data and the ‘nan_euclidean’ metric to calculate the distances between values.\n\n\n\nExample:\n\n\nfrom sklearn.impute import KNNImputer\n\nknn_imputer = KNNImputer(n_neighbors=2)\ntemp_wind_knn_imputed = knn_imputer.fit_transform(temp_wind)\n\ntemp_wind_knn_imputed_df = pd.DataFrame(temp_wind_knn_imputed, columns=temp_wind.columns)\n\nIf we take a look at the outcome\n\ntemp_wind_knn_imputed_df\n\n\n\n\n\n\n\n\ntemperature\nwindspead\n\n\n\n\n0\n32.0\n6.0\n\n\n1\n33.0\n9.0\n\n\n2\n28.0\n7.0\n\n\n3\n33.0\n7.0\n\n\n4\n32.0\n7.0\n\n\n5\n33.2\n8.4\n\n\n6\n33.2\n8.4\n\n\n7\n34.0\n8.0\n\n\n8\n40.0\n12.0\n\n\n\n\n\n\n\n\n\n\n\n\n\nFilling a single column independently using the KNNImputer\nTo use the KNNImputer for a single independ column, you can use the index as the other column instead, this will result into equal euclidean distances resulting into the use of the physical neighbors in the data table.\n\nfrom sklearn.impute import KNNImputer\n\nknn_imputer = KNNImputer(n_neighbors=2)\nwindspead_imputed = knn_imputer.fit_transform(weather[['windspead']].reset_index())\n\nwindspead_imputed\n\narray([[ 0. ,  6. ],\n       [ 1. ,  9. ],\n       [ 2. ,  8. ],\n       [ 3. ,  7. ],\n       [ 4. ,  8. ],\n       [ 5. ,  7.5],\n       [ 6. , 10. ],\n       [ 7. ,  8. ],\n       [ 8. , 12. ]])\n\n\n\n# we can fill it back in the weather data\nweather['windspead'] = windspead_imputed[:, 1]\n\n# now looking at the data\nweather\n\n\n\n\n\n\n\n\nday\ntemperature\nwindspead\nevent\n\n\n\n\n0\n01/01/2017\n32.0\n6.0\nRain\n\n\n1\n04/01/2017\nNaN\n9.0\nSunny\n\n\n2\n05/01/2017\n28.0\n8.0\nSnow\n\n\n3\n06/01/2017\nNaN\n7.0\nNaN\n\n\n4\n07/01/2017\n32.0\n8.0\nRain\n\n\n5\n08/01/2017\nNaN\n7.5\nSunny\n\n\n6\n09/01/2017\nNaN\n10.0\nNaN\n\n\n7\n10/01/2017\n34.0\n8.0\nCloudy\n\n\n8\n11/01/2017\n40.0\n12.0\nSunny\n\n\n\n\n\n\n\n\n\nExercise\n\nTry out the KNNImputer with different numbers of neighbors and compare the results\nFindo out how to use KNNImputer to fill categorical data",
    "crumbs": [
      "Python-Data-Analysis",
      "Week7-Preprocessing",
      "Handling Missing Data"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week7-Preprocessing/76_handling_missing_data.html#iterativeimputer-from-scikit-learn",
    "href": "Python-Data-Analysis/Week7-Preprocessing/76_handling_missing_data.html#iterativeimputer-from-scikit-learn",
    "title": "DATAIDEA",
    "section": "IterativeImputer from scikit-learn:",
    "text": "IterativeImputer from scikit-learn:\n\nUsage: IterativeImputer models each feature with missing values as a function of other features and uses that estimate for imputation. It iteratively estimates the missing values.\nPros:\n\nTakes into account relationships between features, making it suitable for datasets with complex missing patterns.\nMore robust than SimpleImputer for handling missing data.\n\nCons:\n\nCan be computationally intensive and slower than SimpleImputer.\nRequires careful tuning of model parameters.\n\nExample:\n\n\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\niterative_imputer = IterativeImputer()\ntemp_wind_iterative_imputed = iterative_imputer.fit_transform(temp_wind)\n\ntemp_wind_iterative_imputed_df = pd.DataFrame(temp_wind_iterative_imputed, columns=temp_wind.columns)\n\ntemp_wind_iterative_imputed_df\n\n\n\n\n\n\n\n\ntemperature\nwindspead\n\n\n\n\n0\n32.000000\n6.0\n\n\n1\n33.967053\n9.0\n\n\n2\n28.000000\n8.0\n\n\n3\n31.410210\n7.0\n\n\n4\n32.000000\n8.0\n\n\n5\n32.049421\n7.5\n\n\n6\n35.245474\n10.0\n\n\n7\n34.000000\n8.0\n\n\n8\n40.000000\n12.0\n\n\n\n\n\n\n\nYou can also choose an estimator of your choice, let’s try a Linear Regression model\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\n# set estimator to an instance of a model\niterative_imputer = IterativeImputer(estimator=LinearRegression())\ntemp_wind_iterative_imputed = iterative_imputer.fit_transform(temp_wind)\n\ntemp_wind_iterative_imputed_df = pd.DataFrame(temp_wind_iterative_imputed, columns=temp_wind.columns)\n\ntemp_wind_iterative_imputed_df\n\n\n\n\n\n\n\n\ntemperature\nwindspead\n\n\n\n\n0\n32.000000\n6.0\n\n\n1\n34.125000\n9.0\n\n\n2\n28.000000\n8.0\n\n\n3\n31.041667\n7.0\n\n\n4\n32.000000\n8.0\n\n\n5\n31.812500\n7.5\n\n\n6\n35.666667\n10.0\n\n\n7\n34.000000\n8.0\n\n\n8\n40.000000\n12.0",
    "crumbs": [
      "Python-Data-Analysis",
      "Week7-Preprocessing",
      "Handling Missing Data"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week7-Preprocessing/76_handling_missing_data.html#datawig",
    "href": "Python-Data-Analysis/Week7-Preprocessing/76_handling_missing_data.html#datawig",
    "title": "DATAIDEA",
    "section": "Datawig:",
    "text": "Datawig:\nDatawig is a library specifically designed for imputing missing values in tabular data using deep learning models.\n\n# import datawig\n\n# # Impute missing values\n# df_imputed = datawig.SimpleImputer.complete(weather)\n\nThese top imputation methods offer different trade-offs in terms of computational complexity, handling of missing data patterns, and ease of use. The choice between them depends on the specific characteristics of the dataset and the requirements of the analysis.",
    "crumbs": [
      "Python-Data-Analysis",
      "Week7-Preprocessing",
      "Handling Missing Data"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week7-Preprocessing/76_handling_missing_data.html#homework",
    "href": "Python-Data-Analysis/Week7-Preprocessing/76_handling_missing_data.html#homework",
    "title": "DATAIDEA",
    "section": "Homework",
    "text": "Homework\n\nTry out these techniques for categorical data\n\n\n\nDon’t miss out on any updates and developments! Subscribe to the DATAIDEA Newsletter it’s easy and safe.\n\n\n\n\n\n\n\n\n\n\nTo be among the first to hear about future updates, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel.",
    "crumbs": [
      "Python-Data-Analysis",
      "Week7-Preprocessing",
      "Handling Missing Data"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week7-Preprocessing/72_why_scaling_fpl.html",
    "href": "Python-Data-Analysis/Week7-Preprocessing/72_why_scaling_fpl.html",
    "title": "DATAIDEA",
    "section": "",
    "text": "Photo by DATAIDEA",
    "crumbs": [
      "Python-Data-Analysis",
      "Week7-Preprocessing",
      "Why re-scale data (fpl)?"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week7-Preprocessing/72_why_scaling_fpl.html#why-re-scale-data-fpl",
    "href": "Python-Data-Analysis/Week7-Preprocessing/72_why_scaling_fpl.html#why-re-scale-data-fpl",
    "title": "DATAIDEA",
    "section": "Why re-scale data (fpl)?",
    "text": "Why re-scale data (fpl)?\nIn this notebook, we’ll use Kmeans clustering to demonstrate the importance of scaling data\nK-means clustering is an unsupervised machine learning algorithm used for partitioning a dataset into K distinct, non-overlapping clusters. The goal of K-means is to minimize the sum of squared distances between data points and their respective cluster centroids.\nHere’s how the K-means algorithm works:\n\nPick K random objects as the initial cluster centers.\nClassify each object into the cluster whose center is closest to the point.\nFor each cluster of classified objects, compute the centroid (mean).\nNow reclassify each object using the centroids as cluster centers.\nCalculate the total variance of the clusters (this is the measure of goodness).\nRepeat steps 1 to 6 a few more times and pick the cluster centers with the lowest total variance.\n\nHere’s a video showing the above steps: https://www.youtube.com/watch?v=4b5d3muPQmA\nFirst, let’s import the necessary libraries and load the Iris dataset:\n\nfrom dataidea.packages import pd, plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom dataidea.datasets import loadDataset\n\n\nfpl_data = loadDataset('fpl')\n\nWe can be able to load it like this because it inbuilt in the dataidea package\nNow let’s pick out a few numeric columns that we might consider moving forward\n\nfpl_sample = fpl_data[['Goals_Scored', 'Assists','Total_Points', \n                       'Minutes', 'Saves', 'Goals_Conceded', \n                       'Creativity', 'Influence'\n                      ]]\n\n\nfpl_sample.head()\n\n\n\n\n\n\n\n\nGoals_Scored\nAssists\nTotal_Points\nMinutes\nSaves\nGoals_Conceded\nCreativity\nInfluence\n\n\n\n\n0\n18\n14\n244\n3101\n0\n36\n1414.9\n1292.6\n\n\n1\n23\n14\n242\n3083\n0\n39\n659.1\n1318.2\n\n\n2\n22\n6\n231\n3077\n0\n41\n825.7\n1056.0\n\n\n3\n17\n11\n228\n3119\n0\n36\n1049.9\n1052.2\n\n\n4\n17\n11\n194\n3052\n0\n50\n371.0\n867.2",
    "crumbs": [
      "Python-Data-Analysis",
      "Week7-Preprocessing",
      "Why re-scale data (fpl)?"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week7-Preprocessing/72_why_scaling_fpl.html#clustering-without-scaling",
    "href": "Python-Data-Analysis/Week7-Preprocessing/72_why_scaling_fpl.html#clustering-without-scaling",
    "title": "DATAIDEA",
    "section": "Clustering without Scaling",
    "text": "Clustering without Scaling\nNext, let’s perform K-means clustering on the original dataset without scaling the features:\n\n# Apply K-means clustering without scaling\nkmeans_unscaled = KMeans(n_clusters=4, random_state=42)\nkmeans_unscaled.fit(fpl_sample)\n\n# Get the cluster centers and labels\ncentroids_unscaled = kmeans_unscaled.cluster_centers_\nlabels_unscaled = kmeans_unscaled.labels_\n\nLet’s see the performance\n\n# Visualize clusters without scaling\nplt.figure(figsize=(10, 6))\n\nplt.scatter(fpl_sample.Assists, fpl_sample.Goals_Scored, c=labels_unscaled, cmap='viridis',)\n\nplt.show()\n\n\n\n\n\n\n\n\nYou’ll notice that the clusters may not seem well-separated or meaningful. This is because the features of the Iris dataset have different scales,",
    "crumbs": [
      "Python-Data-Analysis",
      "Week7-Preprocessing",
      "Why re-scale data (fpl)?"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week7-Preprocessing/72_why_scaling_fpl.html#clustering-after-scaling",
    "href": "Python-Data-Analysis/Week7-Preprocessing/72_why_scaling_fpl.html#clustering-after-scaling",
    "title": "DATAIDEA",
    "section": "Clustering after Scaling",
    "text": "Clustering after Scaling\nNow, let’s repeat the process after scaling the features using StandardScaler:\n\n# Scale the features\nscaler = StandardScaler()\nfpl_sample_scaled = scaler.fit_transform(fpl_sample)\n\n# Transform scaled features back to DataFrame\nfpl_sample_scaled_dataframe = pd.DataFrame(fpl_sample_scaled, columns=fpl_sample.columns)\n\n# Apply K-means clustering on scaled features\nkmeans_scaled = KMeans(n_clusters=4, random_state=42)\nkmeans_scaled.fit(fpl_sample_scaled)\n\n# Get the cluster centers and labels\ncentroids_scaled = kmeans_scaled.cluster_centers_\nlabels_scaled = kmeans_scaled.labels_\n\n\n# Visualize clusters without scaling\nplt.figure(figsize=(10, 6))\n\nplt.scatter(fpl_sample_scaled_dataframe.Assists, \n            fpl_sample_scaled_dataframe.Goals_Scored, \n            c=labels_scaled, cmap='viridis')\n\nplt.show()\n\n\n\n\n\n\n\n\nYou should see clearer and more meaningful clusters after scaling the features, demonstrating the importance of feature scaling for K-means clustering, especially when dealing with datasets with features of different scales.",
    "crumbs": [
      "Python-Data-Analysis",
      "Week7-Preprocessing",
      "Why re-scale data (fpl)?"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week7-Preprocessing/72_why_scaling_fpl.html#take-away",
    "href": "Python-Data-Analysis/Week7-Preprocessing/72_why_scaling_fpl.html#take-away",
    "title": "DATAIDEA",
    "section": "Take away",
    "text": "Take away\nIf the data doesn’t follow a standard scale, meaning the features have different scales or variances, it can lead to some issues when applying K-means clustering:\n\nUnequal feature influence: Features with larger scales or variances can dominate the clustering process. Since K-means relies on Euclidean distance, features with larger scales will contribute more to the distance calculation, potentially biasing the clustering results towards those features.\nIncorrect cluster shapes: K-means assumes that clusters are isotropic (spherical) and have similar variances along all dimensions. If the data has features with different scales, clusters may be stretched along certain dimensions, leading to suboptimal cluster assignments.\nConvergence speed: Features with larger scales can cause centroids to move more quickly towards areas with denser data, potentially affecting the convergence speed of the algorithm.\n\nBy scaling the data before clustering, you ensure that each feature contributes equally to the distance calculations, helping to mitigate the issues associated with different feature scales. This can lead to more accurate and reliable clustering results.\n\n\n\n\n\n\nTo be among the first to hear about future updates, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel.",
    "crumbs": [
      "Python-Data-Analysis",
      "Week7-Preprocessing",
      "Why re-scale data (fpl)?"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/outline.html",
    "href": "Python-Data-Analysis/outline.html",
    "title": "DATAIDEA",
    "section": "",
    "text": "Photo by DATAIDEA",
    "crumbs": [
      "Python-Data-Analysis",
      "Data Analysis Outline"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/outline.html#data-analysis-outline",
    "href": "Python-Data-Analysis/outline.html#data-analysis-outline",
    "title": "DATAIDEA",
    "section": "Data Analysis Outline",
    "text": "Data Analysis Outline",
    "crumbs": [
      "Python-Data-Analysis",
      "Data Analysis Outline"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/outline.html#week-1-introduction-to-data-analysis",
    "href": "Python-Data-Analysis/outline.html#week-1-introduction-to-data-analysis",
    "title": "DATAIDEA",
    "section": "Week 1: Introduction to Data Analysis ",
    "text": "Week 1: Introduction to Data Analysis \n\nUnderstanding the role of data analysis in decision-making\nIntroduction to Python for data analysis (Numpy and Pandas)\nExploring data types, data structures, and data manipulation\n\nGet Started",
    "crumbs": [
      "Python-Data-Analysis",
      "Data Analysis Outline"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/outline.html#week-2-introduction-to-data-cleaning-and-preprocessing",
    "href": "Python-Data-Analysis/outline.html#week-2-introduction-to-data-cleaning-and-preprocessing",
    "title": "DATAIDEA",
    "section": "Week 2: Introduction to Data Cleaning and Preprocessing ",
    "text": "Week 2: Introduction to Data Cleaning and Preprocessing \n\nData quality assurance\nIdentifying and handling missing data\nDealing with outliers and other data anomalies\n\nGet Started",
    "crumbs": [
      "Python-Data-Analysis",
      "Data Analysis Outline"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/outline.html#week-3-introduction-to-data-visualization",
    "href": "Python-Data-Analysis/outline.html#week-3-introduction-to-data-visualization",
    "title": "DATAIDEA",
    "section": "Week 3: Introduction to Data Visualization ",
    "text": "Week 3: Introduction to Data Visualization \n\nBasic plotting techniques using Matplotlib\nExtracting insights from data distributions and relationships\nPerforming EDA using Pandas and visualizations\n\nGet Started",
    "crumbs": [
      "Python-Data-Analysis",
      "Data Analysis Outline"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/outline.html#week-4-introduction-to-machine-learning",
    "href": "Python-Data-Analysis/outline.html#week-4-introduction-to-machine-learning",
    "title": "DATAIDEA",
    "section": "Week 4: Introduction to Machine Learning ",
    "text": "Week 4: Introduction to Machine Learning \n\nOverview of machine learning concepts\nSupervised vs. unsupervised learning\nHands-on exercises with Scikit-Learn for classification and regression\n\nGet Started",
    "crumbs": [
      "Python-Data-Analysis",
      "Data Analysis Outline"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/outline.html#week-5-statistical-analysis",
    "href": "Python-Data-Analysis/outline.html#week-5-statistical-analysis",
    "title": "DATAIDEA",
    "section": "Week 5: Statistical Analysis ",
    "text": "Week 5: Statistical Analysis \n\nOverview of machine learning concepts\nDescriptive statistics and summary metrics\nHypothesis testing and p-values\nImplementing statistical analysis in Python using SciPy\n\nGet Started",
    "crumbs": [
      "Python-Data-Analysis",
      "Data Analysis Outline"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/outline.html#week-6-more-data-visualization-and-exploratory-data-analysis-eda",
    "href": "Python-Data-Analysis/outline.html#week-6-more-data-visualization-and-exploratory-data-analysis-eda",
    "title": "DATAIDEA",
    "section": "Week 6: More Data Visualization and Exploratory Data Analysis (EDA) ",
    "text": "Week 6: More Data Visualization and Exploratory Data Analysis (EDA) \n\nOverview of machine learning concepts\nAdvanced visualization with Seaborn for statistical analysis\nCreating meaningful visualizations for data exploration\nCorrelation analysis and feature selection\n\nGet Started",
    "crumbs": [
      "Python-Data-Analysis",
      "Data Analysis Outline"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/outline.html#week-7-data-wrangling-and-feature-engineering",
    "href": "Python-Data-Analysis/outline.html#week-7-data-wrangling-and-feature-engineering",
    "title": "DATAIDEA",
    "section": "Week 7: Data Wrangling and Feature Engineering ",
    "text": "Week 7: Data Wrangling and Feature Engineering \n\nFeature scaling and engineering for model improvement\nData normalization and standardization\nHandling categorical data and encoding techniques\n\nGet Started",
    "crumbs": [
      "Python-Data-Analysis",
      "Data Analysis Outline"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/outline.html#week-8-model-evaluation-and-validation",
    "href": "Python-Data-Analysis/outline.html#week-8-model-evaluation-and-validation",
    "title": "DATAIDEA",
    "section": "Week 8: Model Evaluation and Validation ",
    "text": "Week 8: Model Evaluation and Validation \n\nEvaluating machine learning models\nCross-validation and hyperparameter tuning\nModel selection and performance metrics\n\nGet Started",
    "crumbs": [
      "Python-Data-Analysis",
      "Data Analysis Outline"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/outline.html#week-9-time-series-analysis",
    "href": "Python-Data-Analysis/outline.html#week-9-time-series-analysis",
    "title": "DATAIDEA",
    "section": "Week 9: Time Series Analysis ",
    "text": "Week 9: Time Series Analysis \n\nEvaluating machine learning models\nUnderstanding time series data\nTime series visualization and decomposition\nForecasting techniques with Python\n\nGet Started",
    "crumbs": [
      "Python-Data-Analysis",
      "Data Analysis Outline"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/outline.html#week-10-capstone-project",
    "href": "Python-Data-Analysis/outline.html#week-10-capstone-project",
    "title": "DATAIDEA",
    "section": "Week 10: Capstone Project",
    "text": "Week 10: Capstone Project\n\nApplying learned concepts to a real-world dataset\nData analysis, visualization, and modeling\nPresenting findings and insights\n\n\n\nDon’t miss out on any updates and developments! Subscribe to the DATAIDEA Newsletter it’s easy and safe.",
    "crumbs": [
      "Python-Data-Analysis",
      "Data Analysis Outline"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week3-Visualization/nobel_prize_laureates_exercise.html",
    "href": "Python-Data-Analysis/Week3-Visualization/nobel_prize_laureates_exercise.html",
    "title": "Nobel Prize Laureates Exercise:",
    "section": "",
    "text": "Here are ten questions based on the Nobel Prize laureates dataset. Each question involves a mix of data exploration, cleaning, and analysis tasks. You can use Python and libraries such as pandas, matplotlib to solve these questions.\nYou can download the Nobel Prize Laureates dataset from opendatasoft\n\nHow many Nobel Prize laureates are included in the dataset?\n\n\n# your solution here\n\n\nWhich country has the highest number of Nobel laureates?\n\n\n# your solution here\n\n\n\n\n\n\n\nWhat is the distribution of Nobel laureates across different prize categories?\n\n\n# your solution here\n\n\nHow many Nobel laureates were awarded in each decade?\n\n\n# your solution here\n\n\nAre there any missing values in the dataset? If so, in which columns?\n\n\n# your solution here\n\n\nPerform data cleaning by handling missing values appropriately. Describe your approach.\n\n\n# your solution here\n\n\n\n\n\n\n\nVisualize the distribution of Nobel laureates’ birth countries on a world map.\n\n\n# your solution here\n\n\nIs there any correlation between a laureate’s birth year and the year they were awarded the Nobel Prize? Visualize if there’s any relationship.\n\n\n# your solution here\n\n\nPerform anomaly detection on the birth years of laureates. Identify and explain any outliers.\n\n\n# your solution here\n\n\n\nDon’t miss out on any updates and developments! Subscribe to the DATAIDEA Newsletter it’s easy and safe.\n\n\n\n\n\nBased on the dataset, can you identify any interesting trends or patterns regarding Nobel laureates’ demographics or the fields in which they were awarded?\n\n\n# your solution here\n\nThese questions cover various aspects of data exploration, cleaning, visualization, and analysis using the Nobel Prize laureates dataset. You can explore and analyze the dataset to find answers to these questions and gain insights into the demographics and distribution of Nobel laureates over time and across different categories.\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Python-Data-Analysis",
      "Week3-Visualization",
      "Nobel Prize Laureates Exercise:"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week3-Visualization/visualization_quiz.html",
    "href": "Python-Data-Analysis/Week3-Visualization/visualization_quiz.html",
    "title": "Data Viz Quiz",
    "section": "",
    "text": "Photo by DATAIDEA\n\n\nQuestion 1: Which type of chart is used to show the relationship between two continuous variables?\n\nBar chart\n\nPie chart\n\nScatter plot\n\nArea chart\n\nQuestion 2: What is the primary purpose of a radar chart?\n\nComparing multiple categories across a few variables\n\nShowing hierarchical data\n\nDisplaying trends over time\n\nVisualizing correlations between two variables\n\nQuestion 3: In a stacked bar chart, each bar is divided into segments to represent:\n\nDifferent categories of a single variable\n\nTime periods\n\nTrends over time\n\nCorrelation between variables\n\nQuestion 4: When should a heat map be used?\n\nShowing proportions within a population\n\nComparing individual data points\n\nVisualizing spatial relationships and correlations\n\nDisplaying data distribution\n\nQuestion 5: Which type of chart is best suited for comparing parts of a whole?\n\nScatter plot\n\nHistogram\n\nPie chart\n\nLine chart\n\nQuestion 6: Which chart type is used to detect outliers and visualize the distribution of a dataset?\n\nBox plot\n\nArea chart\n\nBubble chart\n\nRadar chart\n\nQuestion 7: What does a bubble chart represent?\n\nRelationship between two continuous variables\n\nRelationship between two categorical variables\n\nRelationship between three variables using two continuous variables and one categorical variable\n\nRelationship between three continuous variables\n\nQuestion 8: Which chart type is suitable for showing hierarchical data with nested categories?\n\nTree map\n\nScatter plot\n\nLine chart\n\nHistogram\n\nQuestion 9: When is a bar chart more appropriate than a pie chart?\n\nComparing parts of a whole\n\nShowing trends over time\n\nDisplaying a single continuous variable\n\nVisualizing correlations\n\nQuestion 10: What is the main purpose of a histogram?\n\nShowing hierarchical data\n\nVisualizing correlations\n\nDisplaying the distribution of a single continuous variable\n\nComparing multiple categories across variables\n\n\nAnswers:\n\n\nScatter plot\n\n\nComparing multiple categories across a few variables\n\n\nDifferent categories of a single variable\n\n\nVisualizing spatial relationships and correlations\n\n\nPie chart\n\n\nBox plot\n\n\nRelationship between three variables using two continuous variables and one categorical variable\n\n\nTree map\n\n\nComparing parts of a whole\n\n\nDisplaying the distribution of a single continuous variable\n\n\n\n\n\n\n\n\nTo be among the first to hear about future updates, simply enter your email below, follow us on  (formally Twitter), or subscribe to our  YouTube channel.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Python-Data-Analysis",
      "Week3-Visualization",
      "Data Viz Quiz"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week4-ML-Intro/43_sklearn-unsupervised-learning2.html",
    "href": "Python-Data-Analysis/Week4-ML-Intro/43_sklearn-unsupervised-learning2.html",
    "title": "Unsupervised Learning using Scikit Learn Part 2",
    "section": "",
    "text": "In the previous section, we looked clustering and KMeans as model for cluster analysis. In this section, we will look at more clustering algorithms and Dimensionality reduction.\nLet’s install the required libraries.\n# # uncomment and run this cell to install the packages and libraries\n# !pip install dataidea",
    "crumbs": [
      "Python-Data-Analysis",
      "Week4-ML-Intro",
      "Unsupervised Learning using Scikit Learn Part 2"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week4-ML-Intro/43_sklearn-unsupervised-learning2.html#introduction-to-unsupervised-learning",
    "href": "Python-Data-Analysis/Week4-ML-Intro/43_sklearn-unsupervised-learning2.html#introduction-to-unsupervised-learning",
    "title": "Unsupervised Learning using Scikit Learn Part 2",
    "section": "Introduction to Unsupervised Learning",
    "text": "Introduction to Unsupervised Learning\nUnsupervised machine learning refers to the category of machine learning techniques where models are trained on a dataset without labels. Unsupervised learning is generally use to discover patterns in data and reduce high-dimensional data to fewer dimensions. Here’s how unsupervised learning fits into the landscape of machine learning algorithms(source):",
    "crumbs": [
      "Python-Data-Analysis",
      "Week4-ML-Intro",
      "Unsupervised Learning using Scikit Learn Part 2"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week4-ML-Intro/43_sklearn-unsupervised-learning2.html#clustering",
    "href": "Python-Data-Analysis/Week4-ML-Intro/43_sklearn-unsupervised-learning2.html#clustering",
    "title": "Unsupervised Learning using Scikit Learn Part 2",
    "section": "Clustering",
    "text": "Clustering\nClustering is the process of grouping objects from a dataset such that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (Wikipedia). Scikit-learn offers several clustering algorithms. You can learn more about them here: https://scikit-learn.org/stable/modules/clustering.html\nHere is a visual representation of clustering:\n\nHere are some real-world applications of clustering:\n\nCustomer segmentation\nProduct recommendation\nFeature engineering\nAnomaly/fraud detection\nTaxonomy creation\n\nWe’ll use the Iris flower dataset to study some of the clustering algorithms available in scikit-learn. It contains various measurements for 150 flowers belonging to 3 different species.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\n\nsns.set_style('darkgrid')\n\nLet’s load the popular iris and penguin datasets. These datasets are already built in seaborn\n\n# load the iris dataset\niris_df = sns.load_dataset('iris')\niris_df.head()\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n\n\n\n\n\n\n# load the penguin dataset\nsns.get_dataset_names()\nping_df = sns.load_dataset('penguins')\nping_df.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nMale\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nFemale\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nFemale\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nFemale\n\n\n\n\n\n\n\n\nsns.scatterplot(data=ping_df, x='bill_length_mm', y='bill_depth_mm', hue='species')\nplt.title('Penguin Bill Depth against Bill Length per Species')\nplt.ylabel('Bill Depth')\nplt.xlabel('Bill Length')\nplt.show()\n\n\n\n\n\n\n\n\n\nsns.scatterplot(data=iris_df, x='sepal_length', y='petal_length', hue='species')\nplt.title('Flower Petal Length against Sepal Length per Species')\nplt.ylabel('Petal Lenght')\nplt.xlabel('Sepal Length')\nplt.show()\n\n\n\n\n\n\n\n\nWe’ll attempt to cluster observations using numeric columns in the data.\n\nnumeric_cols = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\nX = iris_df[numeric_cols]\nX.head()\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n\n\n1\n4.9\n3.0\n1.4\n0.2\n\n\n2\n4.7\n3.2\n1.3\n0.2\n\n\n3\n4.6\n3.1\n1.5\n0.2\n\n\n4\n5.0\n3.6\n1.4\n0.2\n\n\n\n\n\n\n\n[ad]\n\n\n\n\n\n\nDBSCAN\nDensity-based spatial clustering of applications with noise (DBSCAN) uses the density of points in a region to form clusters. It has two main parameters: “epsilon” and “min samples” using which it classifies each point as a core point, reachable point or noise point (outlier).\n\nHere’s a video explaining how the DBSCAN algorithm works: https://www.youtube.com/watch?v=C3r7tGRe2eI\n\nfrom sklearn.cluster import DBSCAN\n\n\nmodel = DBSCAN(eps=1.1, min_samples=4)\nmodel.fit(X)\n\nDBSCAN(eps=1.1, min_samples=4)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DBSCAN?Documentation for DBSCANiFittedDBSCAN(eps=1.1, min_samples=4) \n\n\nIn DBSCAN, there’s no prediction step. It directly assigns labels to all the inputs.\n\nmodel.labels_\n\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1,\n       1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 2, 2, 1, 1, 1, 1,\n       1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 2])\n\n\n\nsns.scatterplot(data=X, x='sepal_length', y='petal_length', hue=model.labels_)\nplt.title('Flower Petal Length against Sepal Length per Species')\nplt.ylabel('Petal Lenght')\nplt.xlabel('Sepal Length')\nplt.show()\n\n\n\n\n\n\n\n\n\nEXERCISE: Try changing the values of eps and min_samples and observe how the number of clusters the classification changes.\n\nHere’s how the results of DBSCAN and K Means differ:\n\n[ad]\n\n\n\n\n\n\n\nHierarchical Clustering\nHierarchical clustering, as the name suggests, creates a hierarchy or a tree of clusters.\n\nWhile there are several approaches to hierarchical clustering, the most common approach works as follows:\n\nMark each point in the dataset as a cluster.\nPick the two closest cluster centers without a parent and combine them into a new cluster.\nThe new cluster is the parent cluster of the two clusters, and its center is the mean of all the points in the cluster.\nRepeat steps 2 and 3 till there’s just one cluster left.\n\nWatch this video for a visual explanation of hierarchical clustering: https://www.youtube.com/watch?v=7xHsRkOdVwo\n\nEXERCISE: Implement hierarchical clustering for the Iris dataset using scikit-learn.\n\nThere are several other clustering algorithms in Scikit-learn. You can learn more about them and when to use them here: https://scikit-learn.org/stable/modules/clustering.html\nLet’s save our work before continuing.\n[ad]",
    "crumbs": [
      "Python-Data-Analysis",
      "Week4-ML-Intro",
      "Unsupervised Learning using Scikit Learn Part 2"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week4-ML-Intro/43_sklearn-unsupervised-learning2.html#dimensionality-reduction-and-manifold-learning",
    "href": "Python-Data-Analysis/Week4-ML-Intro/43_sklearn-unsupervised-learning2.html#dimensionality-reduction-and-manifold-learning",
    "title": "Unsupervised Learning using Scikit Learn Part 2",
    "section": "Dimensionality Reduction and Manifold Learning",
    "text": "Dimensionality Reduction and Manifold Learning\nIn machine learning problems, we often encounter datasets with a very large number of dimensions (features or columns). Dimensionality reduction techniques are used to reduce the number of dimensions or features within the data to a manageable or convenient number.\nApplications of dimensionality reduction:\n\nReducing size of data without loss of information\nTraining machine learning models efficiently\nVisualizing high-dimensional data in 2/3 dimensions\n\n\nPrincipal Component Analysis (PCA)\nPrincipal component is a dimensionality reduction technique that uses linear projections of data to reduce their dimensions, while attempting to maximize the variance of data in the projection. Watch this video to learn how PCA works: https://www.youtube.com/watch?v=FgakZw6K1QQ\nHere’s an example of PCA to reduce 2D data to 1D:\n\nLet’s apply Principal Component Analysis to the Iris dataset.\n\niris_df = sns.load_dataset('iris')\niris_df.sample(n=5)\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n86\n6.7\n3.1\n4.7\n1.5\nversicolor\n\n\n27\n5.2\n3.5\n1.5\n0.2\nsetosa\n\n\n146\n6.3\n2.5\n5.0\n1.9\nvirginica\n\n\n85\n6.0\n3.4\n4.5\n1.6\nversicolor\n\n\n30\n4.8\n3.1\n1.6\n0.2\nsetosa\n\n\n\n\n\n\n\n\nnumeric_cols\n\n['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n\n\n\nfrom sklearn.decomposition import PCA\n\n\npca = PCA(n_components=2)\npca.fit(iris_df[numeric_cols])\n\nPCA(n_components=2)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  PCA?Documentation for PCAiFittedPCA(n_components=2) \n\n\n\ntransformed = pca.transform(iris_df[numeric_cols])\n\n\nsns.scatterplot(x=transformed[:,0], y=transformed[:,1], hue=iris_df['species'])\n\n\n\n\n\n\n\n\nAs you can see, the PCA algorithm has done a very good job of separating different species of flowers using just 2 measures.\n\nEXERCISE: Apply Principal Component Analysis to a large high-dimensional dataset and train a machine learning model using the low-dimensional results. Observe the changes in the loss and training time for different numbers of target dimensions.\n\nLearn more about Principal Component Analysis here: https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html\n[ad]\n\n\n\n\n\n\n\nt-Distributed Stochastic Neighbor Embedding (t-SNE)\nManifold learning is an approach to non-linear dimensionality reduction. Algorithms for this task are based on the idea that the dimensionality of many data sets is only artificially high. Scikit-learn provides many algorithms for manifold learning: https://scikit-learn.org/stable/modules/manifold.html . A commonly-used manifold learning technique is t-Distributed Stochastic Neighbor Embedding or t-SNE, used to visualize high dimensional data in one, two or three dimensions.\nHere’s a visual representation of t-SNE applied to visualize 2 dimensional data in 1 dimension:\n\nHere’s a video explaning how t-SNE works: https://www.youtube.com/watch?v=NEaUSP4YerM\n\nfrom sklearn.manifold import TSNE\n\n\ntsne = TSNE(n_components=2)\ntransformed = tsne.fit_transform(iris_df[numeric_cols])\n\n\nsns.scatterplot(x=transformed[:,0], y=transformed[:,1], hue=iris_df['species']);\n\n\n\n\n\n\n\n\nAs you can see, the flowers from the same species are clustered very closely together. The relative distance between the species is also conveyed by the gaps between the clusters.\n\nEXERCISE: Use t-SNE to visualize the MNIST handwritten digits dataset.\n\n[ad]",
    "crumbs": [
      "Python-Data-Analysis",
      "Week4-ML-Intro",
      "Unsupervised Learning using Scikit Learn Part 2"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week4-ML-Intro/43_sklearn-unsupervised-learning2.html#summary-and-references",
    "href": "Python-Data-Analysis/Week4-ML-Intro/43_sklearn-unsupervised-learning2.html#summary-and-references",
    "title": "Unsupervised Learning using Scikit Learn Part 2",
    "section": "Summary and References",
    "text": "Summary and References\n\nThe following topics were covered in this tutorial:\n\nOverview of unsupervised learning algorithms in Scikit-learn\nClustering algorithms: K Means, DBScan, Hierarchical clustering etc.\nDimensionality reduction (PCA) and manifold learning (t-SNE)\n\nCheck out these resources to learn more:\n\nhttps://www.coursera.org/learn/machine-learning\nhttps://dashee87.github.io/data%20science/general/Clustering-with-Scikit-with-GIFs/\nhttps://scikit-learn.org/stable/unsupervised_learning.html\nhttps://scikit-learn.org/stable/modules/clustering.html",
    "crumbs": [
      "Python-Data-Analysis",
      "Week4-ML-Intro",
      "Unsupervised Learning using Scikit Learn Part 2"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week4-ML-Intro/43_sklearn-unsupervised-learning.html",
    "href": "Python-Data-Analysis/Week4-ML-Intro/43_sklearn-unsupervised-learning.html",
    "title": "DATAIDEA",
    "section": "",
    "text": "Photo by DATAIDEA",
    "crumbs": [
      "Python-Data-Analysis",
      "Week4-ML-Intro",
      "Unsupervised Learning using Scikit Learn"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week4-ML-Intro/43_sklearn-unsupervised-learning.html#unsupervised-learning-using-scikit-learn",
    "href": "Python-Data-Analysis/Week4-ML-Intro/43_sklearn-unsupervised-learning.html#unsupervised-learning-using-scikit-learn",
    "title": "DATAIDEA",
    "section": "Unsupervised Learning using Scikit Learn",
    "text": "Unsupervised Learning using Scikit Learn\nThe following topics are covered in this tutorial:\n\nOverview of unsupervised learning algorithms in Scikit-learn\nClustering algorithms: K Means, Hierarchical clustering etc.\nDimensionality reduction (PCA) and manifold learning (t-SNE)\n\nLet’s install the required libraries.\n\n# # uncomment and run this cell to install the packages and libraries\n# !pip install dataidea",
    "crumbs": [
      "Python-Data-Analysis",
      "Week4-ML-Intro",
      "Unsupervised Learning using Scikit Learn"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week4-ML-Intro/43_sklearn-unsupervised-learning.html#introduction-to-unsupervised-learning",
    "href": "Python-Data-Analysis/Week4-ML-Intro/43_sklearn-unsupervised-learning.html#introduction-to-unsupervised-learning",
    "title": "DATAIDEA",
    "section": "Introduction to Unsupervised Learning",
    "text": "Introduction to Unsupervised Learning\nUnsupervised machine learning refers to the category of machine learning techniques where models are trained on a dataset without labels. Unsupervised learning is generally use to discover patterns in data and reduce high-dimensional data to fewer dimensions. Here’s how unsupervised learning fits into the landscape of machine learning algorithms(source):\n\nHere are the topics in machine learning that we’re studying in this course (source):\n\nScikit-learn offers the following cheatsheet to decide which model to pick for a given problem. Can you identify the unsupervised learning algorithms?\n\nHere is a full list of unsupervised learning algorithms available in Scikit-learn: https://scikit-learn.org/stable/unsupervised_learning.html",
    "crumbs": [
      "Python-Data-Analysis",
      "Week4-ML-Intro",
      "Unsupervised Learning using Scikit Learn"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week4-ML-Intro/43_sklearn-unsupervised-learning.html#clustering",
    "href": "Python-Data-Analysis/Week4-ML-Intro/43_sklearn-unsupervised-learning.html#clustering",
    "title": "DATAIDEA",
    "section": "Clustering",
    "text": "Clustering\nClustering is the process of grouping objects from a dataset such that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (Wikipedia). Scikit-learn offers several clustering algorithms. You can learn more about them here: https://scikit-learn.org/stable/modules/clustering.html\nHere is a visual representation of clustering:\n\nHere are some real-world applications of clustering:\n\nCustomer segmentation\nProduct recommendation\nFeature engineering\nAnomaly/fraud detection\nTaxonomy creation\n\nWe’ll use the Iris flower dataset to study some of the clustering algorithms available in scikit-learn. It contains various measurements for 150 flowers belonging to 3 different species.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\n\nsns.set_style('darkgrid')\n\nLet’s load the popular iris and penguin datasets. These datasets are already built in seaborn\n\n# load the iris dataset\niris_df = sns.load_dataset('iris')\niris_df.head()\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n\n\n\n\n\n\nsns.scatterplot(data=iris_df, x='sepal_length', y='petal_length', hue='species')\nplt.title('Flower Petal Length against Sepal Length per Species')\nplt.ylabel('Petal Length')\nplt.xlabel('Sepal Length')\nplt.show()\n\n\n\n\n\n\n\n\nWe’ll attempt to cluster observations using numeric columns in the data.\n\nnumeric_cols = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\nX = iris_df[numeric_cols]\nX.head()\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n\n\n1\n4.9\n3.0\n1.4\n0.2\n\n\n2\n4.7\n3.2\n1.3\n0.2\n\n\n3\n4.6\n3.1\n1.5\n0.2\n\n\n4\n5.0\n3.6\n1.4\n0.2\n\n\n\n\n\n\n\n\n\n\n\n\n\nK Means Clustering\nThe K-means algorithm attempts to classify objects into a pre-determined number of clusters by finding optimal central points (called centroids) for each cluster. Each object is classifed as belonging the cluster represented by the closest centroid.\n\nHere’s how the K-means algorithm works:\n\nPick K random objects as the initial cluster centers.\nClassify each object into the cluster whose center is closest to the point.\nFor each cluster of classified objects, compute the centroid (mean).\nNow reclassify each object using the centroids as cluster centers.\nCalculate the total variance of the clusters (this is the measure of goodness).\nRepeat steps 1 to 6 a few more times and pick the cluster centers with the lowest total variance.\n\nHere’s a video showing the above steps:\n\n \n\nLet’s apply K-means clustering to the Iris dataset.\n\nfrom sklearn.cluster import KMeans\n\n\nmodel = KMeans(n_clusters=3, random_state=42)\n# training the model\nmodel.fit(X)\n\nKMeans(n_clusters=3, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  KMeans?Documentation for KMeansiFittedKMeans(n_clusters=3, random_state=42) \n\n\nWe can check the cluster centers for each cluster.\n\nmodel.cluster_centers_\n\narray([[6.85384615, 3.07692308, 5.71538462, 2.05384615],\n       [5.006     , 3.428     , 1.462     , 0.246     ],\n       [5.88360656, 2.74098361, 4.38852459, 1.43442623]])\n\n\nWe can now classify points using the model.\n\n# making predictions on X (clustering)\npreds = model.predict(X)\n\n\n# assign each row to their cluster\nX['clusters'] = preds\n# looking at some samples\nX.tail(n=5)\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nclusters\n\n\n\n\n145\n6.7\n3.0\n5.2\n2.3\n0\n\n\n146\n6.3\n2.5\n5.0\n1.9\n2\n\n\n147\n6.5\n3.0\n5.2\n2.0\n0\n\n\n148\n6.2\n3.4\n5.4\n2.3\n0\n\n\n149\n5.9\n3.0\n5.1\n1.8\n2\n\n\n\n\n\n\n\nLet’s use seaborn and pyplot to visualize the clusters\n\nsns.scatterplot(data=X, x='sepal_length', y='petal_length', hue=preds)\ncenters_x, centers_y = model.cluster_centers_[:,0], model.cluster_centers_[:,2]\nplt.plot(centers_x, centers_y, 'xb')\nplt.title('Flower Petal Length against Sepal Length per Species')\nplt.ylabel('Petal Length')\nplt.xlabel('Sepal Length')\nplt.show()\n\n\n\n\n\n\n\n\nAs you can see, K-means algorithm was able to classify (for the most part) different specifies of flowers into separate clusters. Note that we did not provide the “species” column as an input to KMeans.\nWe can check the “goodness” of the fit by looking at model.inertia_, which contains the sum of squared distances of samples to their closest cluster center. Lower the inertia, better the fit.\n\nmodel.inertia_\n\n78.8556658259773\n\n\nLet’s try creating 6 clusters.\n\nmodel = KMeans(n_clusters=6, random_state=42)\n# fitting the model\nmodel.fit(X)\n# making predictions on X (clustering)\npreds = model.predict(X)\n# assign each row to their cluster\nX['clusters'] = preds\n# looking at some samples\nX.sample(n=5)\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nclusters\n\n\n\n\n108\n6.7\n2.5\n5.8\n1.8\n3\n\n\n89\n5.5\n2.5\n4.0\n1.3\n2\n\n\n0\n5.1\n3.5\n1.4\n0.2\n5\n\n\n109\n7.2\n3.6\n6.1\n2.5\n3\n\n\n93\n5.0\n2.3\n3.3\n1.0\n2\n\n\n\n\n\n\n\nLet’s visualize the clusters\n\nsns.scatterplot(data=X, x='sepal_length', y='petal_length', hue=preds)\ncenters_x, centers_y = model.cluster_centers_[:,0], model.cluster_centers_[:,2]\nplt.plot(centers_x, centers_y, 'xb')\nplt.title('Flower Petal Length against Sepal Length per Species')\nplt.ylabel('Petal Lenght')\nplt.xlabel('Sepal Length')\nplt.show()\n\n\n\n\n\n\n\n\n\n# Let's calculate the new model inertia\nmodel.inertia_\n\n50.560990643274856\n\n\n\n\nSo, what number of clusters is good enough?\nIn most real-world scenarios, there’s no predetermined number of clusters. In such a case, you can create a plot of “No. of clusters” vs “Inertia” to pick the right number of clusters.\n\noptions = range(2, 11)\ninertias = []\n\nfor n_clusters in options:\n    model = KMeans(n_clusters, random_state=42).fit(X)\n    inertias.append(model.inertia_)\n    \nplt.plot(options, inertias, linestyle='-', marker='o')\nplt.title(\"No. of clusters vs. Inertia\")\nplt.xlabel('No. of clusters (K)')\nplt.ylabel('Inertia')\n\nText(0, 0.5, 'Inertia')\n\n\n\n\n\n\n\n\n\nThe chart is creates an “elbow” plot, and you can pick the number of clusters beyond which the reduction in inertia decreases sharply.\nMini Batch K Means: The K-means algorithm can be quite slow for really large dataset. Mini-batch K-means is an iterative alternative to K-means that works well for large datasets. Learn more about it here: https://scikit-learn.org/stable/modules/clustering.html#mini-batch-kmeans\n\nEXERCISE: Perform clustering on the Mall customers dataset on Kaggle. Study the segments carefully and report your observations.",
    "crumbs": [
      "Python-Data-Analysis",
      "Week4-ML-Intro",
      "Unsupervised Learning using Scikit Learn"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week4-ML-Intro/43_sklearn-unsupervised-learning.html#summary-and-references",
    "href": "Python-Data-Analysis/Week4-ML-Intro/43_sklearn-unsupervised-learning.html#summary-and-references",
    "title": "DATAIDEA",
    "section": "Summary and References",
    "text": "Summary and References\n\nThe following topics were covered in this tutorial:\n\nOverview of unsupervised learning algorithms in Scikit-learn\nClustering algorithms: K Means, DBScan, Hierarchical clustering etc.\nDimensionality reduction (PCA) and manifold learning (t-SNE)\n\nCheck out these resources to learn more:\n\nhttps://blog.dataidea.org/posts/cost-function-in-machine-learning\nhttps://www.coursera.org/learn/machine-learning\nhttps://dashee87.github.io/data%20science/general/Clustering-with-Scikit-with-GIFs/\nhttps://scikit-learn.org/stable/unsupervised_learning.html\nhttps://scikit-learn.org/stable/modules/clustering.html",
    "crumbs": [
      "Python-Data-Analysis",
      "Week4-ML-Intro",
      "Unsupervised Learning using Scikit Learn"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week6-Visualization2/62_seaborn_part1.html",
    "href": "Python-Data-Analysis/Week6-Visualization2/62_seaborn_part1.html",
    "title": "DATAIDEA",
    "section": "",
    "text": "Photo by DATAIDEA",
    "crumbs": [
      "Python-Data-Analysis",
      "Week6-Visualization2",
      "Seaborn Part 1"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week6-Visualization2/62_seaborn_part1.html#seaborn-part-1",
    "href": "Python-Data-Analysis/Week6-Visualization2/62_seaborn_part1.html#seaborn-part-1",
    "title": "DATAIDEA",
    "section": "Seaborn Part 1",
    "text": "Seaborn Part 1\nSeaborn is a Python data visualization library based on matplotlib. Seaborn makes it easy to create informative and attractive statistical graphics. We’ll cover basic plotting techniques and explore some of the functionalities Seaborn offers.\n\nInstalling Seaborn\nBefore we begin, make sure you have Seaborn installed. You can install it via pip if you haven’t already:\n\n# install seaborn\n# !pip install seaborn\n\n\n### Importing Seaborn and Other Libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n\nLoading Sample Dataset\nSeaborn comes with some built-in datasets for practice. For this tutorial, we’ll use the “tips” dataset, which contains information about tips given in a restaurant.\n\ntips = sns.load_dataset(\"tips\")\n### Basic Plots\n\n\ntips.head()\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4\n\n\n\n\n\n\n\nHere are the columns typically found in the “tips” dataset:\n\ntotal_bill: Total bill amount (including tip).\ntip: Tip amount.\nsex: Gender of the person paying the bill (male or female).\nsmoker: Whether the party included smokers (yes or no).\nday: The day of the week.\ntime: Whether the meal was lunch or dinner.\nsize: The size of the dining party (number of people).\n\n\n1. Scatter Plot\n\nsns.scatterplot(x='total_bill', y='tip', data=tips)\nplt.title('Scatter Plot of Total Bill vs Tip')\nplt.show()\n\n\n\n\n\n\n\n\nWe can quickly observe that seaborn adds for us the labels for both x and y.\n\n\n2. Line Plot\n\nsns.lineplot(x='size', y='total_bill', data=tips)\nplt.title('Line Plot of Party Size vs Total Bill')\nplt.show()\n\n\n\n\n\n\n\n\nThe shaded region represents the confidence interval calculated using the standard deviation. You can modify this by setting the errorbar parameter.\n\n\n3. Histogram\n\nsns.histplot(tips['total_bill'], bins=10, kde=True)\nplt.title('Histogram of Total Bill')\nplt.show()\n\n\n\n\n\n\n\n\nWe can observe that seaborn sets for us edgecolor to black out of the box, but also we can be able to add the normal distribution function with a simple parameter modification, ie kde=True. Read more about Kenel Density Estimate (kde) from the seaborn documentation\n\n\n4. Bar Plot\n\nsns.barplot(x='size', y='total_bill', data=tips)\nplt.title('Average Total Bill by Day')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCustomizing Plots\n\n1. Changing Color Palette\n\nsns.set_palette(\"dark\")\nsns.barplot(x='day', y='total_bill', data=tips)\nplt.title('Average Total Bill by Day')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n2. Adding Labels and Legends\n\nsns.scatterplot(x='total_bill', y='tip', hue='sex', data=tips)\nplt.title('Scatter Plot of Total Bill vs Tip')\nplt.xlabel('Total Bill ($)')\nplt.ylabel('Tip ($)')\nplt.legend(title='Gender')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n3. Adding Annotations\n\nsns.scatterplot(x='total_bill', y='tip', data=tips)\nplt.title('Scatter Plot of Total Bill vs Tip')\nplt.annotate('This point seems interesting', xy=(20, 2), xytext=(25, 4),\n             arrowprops=dict(facecolor='black', shrink=0.05))\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nAdvanced Visualizations\n\n1. Pair Plot\n\nsns.pairplot(tips, hue='sex')\nplt.savefig('plot.png')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n2. Heatmap\n\n# Exclude non-numeric columns from correlation calculation\nnumeric_cols = tips.select_dtypes(include=['float64', 'int64'])\ncorrelation_matrix = numeric_cols.corr()\n\n# Plotting the heatmap\nsns.heatmap(correlation_matrix, annot=True)\nplt.title('Correlation Heatmap')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n3. Violin Plot\n\nsns.violinplot(x='day', y='total_bill', data=tips, split=True)\nplt.title('Violin Plot of Total Bill by Day and Gender')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nConclusion\nSeaborn provides a high-level interface for drawing attractive and informative statistical graphics. This tutorial covered basic plotting techniques and some advanced visualizations. Experiment with different plot types and customization options to create visualizations tailored to your specific needs.\nA few ads maybe displayed for income as resources are now offered freely. 🤝🤝🤝",
    "crumbs": [
      "Python-Data-Analysis",
      "Week6-Visualization2",
      "Seaborn Part 1"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week6-Visualization2/63.visual_eda.html",
    "href": "Python-Data-Analysis/Week6-Visualization2/63.visual_eda.html",
    "title": "DATAIDEA",
    "section": "",
    "text": "Photo by DATAIDEA",
    "crumbs": [
      "Python-Data-Analysis",
      "Week6-Visualization2",
      "Visual EDA"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week6-Visualization2/63.visual_eda.html#visual-eda",
    "href": "Python-Data-Analysis/Week6-Visualization2/63.visual_eda.html#visual-eda",
    "title": "DATAIDEA",
    "section": "Visual EDA",
    "text": "Visual EDA\n\n# Import necessary libraries\nfrom dataidea.packages import *\nfrom dataidea.datasets import loadDataset\nfrom sklearn.ensemble import RandomForestClassifier\n\n\n# Load the dataset\ndemo_df = loadDataset('../assets/demo_cleaned.csv', inbuilt=False, file_type='csv')\n\n\ndemo_df.head()\n\n\n\n\n\n\n\n\nage\ngender\nmarital_status\naddress\nincome\nincome_category\njob_category\n\n\n\n\n0\n55\nf\n1\n12\n72.0\n3.0\n3\n\n\n1\n56\nm\n0\n29\n153.0\n4.0\n3\n\n\n2\n24\nm\n1\n4\n26.0\n2.0\n1\n\n\n3\n45\nm\n0\n9\n76.0\n4.0\n2\n\n\n4\n44\nm\n1\n17\n144.0\n4.0\n3\n\n\n\n\n\n\n\n\ndemo_df2 = pd.get_dummies(demo_df, ['gender'], dtype=int, drop_first=1)\n\n\ndemo_df2.head()\n\n\n\n\n\n\n\n\nage\nmarital_status\naddress\nincome\nincome_category\njob_category\ngender_m\n\n\n\n\n0\n55\n1\n12\n72.0\n3.0\n3\n0\n\n\n1\n56\n0\n29\n153.0\n4.0\n3\n1\n\n\n2\n24\n1\n4\n26.0\n2.0\n1\n1\n\n\n3\n45\n0\n9\n76.0\n4.0\n2\n1\n\n\n4\n44\n1\n17\n144.0\n4.0\n3\n1\n\n\n\n\n\n\n\n\nData Distribution Plots:\n\nHistograms for numerical features (age and income).\nBar plots for categorical features (gender, income_category, job_category etc).\n\n\n# 1. Data Distribution Plots\nfig, axes = plt.subplots(2, 3, figsize=(18, 10))\nsns.histplot(demo_df2['age'], ax=axes[0, 0], kde=True, color='skyblue')\nsns.histplot(demo_df2['income'], ax=axes[0, 1], kde=True, color='salmon')\nsns.countplot(x='gender_m', data=demo_df2, ax=axes[0, 2])\nsns.countplot(x='income_category', data=demo_df2, ax=axes[1, 0])\nsns.countplot(x='job_category', data=demo_df2, ax=axes[1, 1])\nsns.countplot(x='marital_status', data=demo_df2, ax=axes[1, 2])\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\nPairwise Feature Scatter Plots:\n\nScatter plots of age vs. income,\n\n\n# 2. Pairwise Feature Scatter Plots\ng = sns.pairplot(demo_df2, vars=['age', 'income'], hue='marital_status', palette={0: 'blue', 1: 'orange'})",
    "crumbs": [
      "Python-Data-Analysis",
      "Week6-Visualization2",
      "Visual EDA"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week6-Visualization2/63.visual_eda.html#correlation-heatmap",
    "href": "Python-Data-Analysis/Week6-Visualization2/63.visual_eda.html#correlation-heatmap",
    "title": "DATAIDEA",
    "section": "Correlation Heatmap:",
    "text": "Correlation Heatmap:\n\nA heatmap showing the correlation between numerical features.\n\n\n# 3. Correlation Heatmap\nplt.figure(figsize=(8, 6))\nsns.heatmap(demo_df2[['age', 'income']].corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n\n\n\n\n\n\n\n\n\nMissing Values Matrix:\n\nA matrix indicating missing values in different features.\n\n\n# 4. Missing Values Matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(demo_df2.isnull(), cmap='viridis')\n\n\n\n\n\n\n\n\n\nFeature Importance Plot:\n\nAfter training a model (e.g., random forest), we can visualize feature importances to see which features contribute the most to predicting survival.\n\n\n\n# 5. Feature Importance Plot\n# Prepare data for training\nX = demo_df2.drop(['marital_status'], axis=1)\ny = demo_df2['marital_status']\n\n# Train Random Forest Classifier\nrf_classifier = RandomForestClassifier()\nrf_classifier.fit(X, y)\n\n# Plot feature importances\nplt.figure(figsize=(10, 6))\nimportances = rf_classifier.feature_importances_\nindices = np.argsort(importances)[::-1]\nsns.barplot(x=importances[indices], y=X.columns[indices], palette='viridis', hue=X.columns[indices])\n\nplt.show()\n\n\n\n\n\n\n\n\nA few ads maybe displayed for income as resources are now offered freely. 🤝🤝🤝",
    "crumbs": [
      "Python-Data-Analysis",
      "Week6-Visualization2",
      "Visual EDA"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week9-Time-Series/90_introduction.html",
    "href": "Python-Data-Analysis/Week9-Time-Series/90_introduction.html",
    "title": "DATAIDEA",
    "section": "",
    "text": "Photo by DATAIDEA",
    "crumbs": [
      "Python-Data-Analysis",
      "Week9-Time-Series",
      "What is Time Series"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week9-Time-Series/90_introduction.html#what-is-time-series",
    "href": "Python-Data-Analysis/Week9-Time-Series/90_introduction.html#what-is-time-series",
    "title": "DATAIDEA",
    "section": "What is Time Series",
    "text": "What is Time Series\nAny data recorded with some fixed interval of time is called as time series data. This fixed interval can be hourly, daily, monthly or yearly. e.g. hourly temperature reading, daily changing fuel prices, monthly electricity bill, annul company profit report etc. In time series data, time will always be independent variable and there can be one or many dependent variable.\nSales forecasting time series with shampoo sales for every month will look like this,\n\n\n\nShampoo_Sales\n\n\nIn above example since there is only one variable dependent on time so its called as univariate time series. If there are multiple dependent variables, then its called as multivariate time series.\nObjective of time series analysis is to understand how change in time affect the dependent variables and accordingly predict values for future time intervals.\n[ad]",
    "crumbs": [
      "Python-Data-Analysis",
      "Week9-Time-Series",
      "What is Time Series"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week9-Time-Series/90_introduction.html#time-series-characteristics",
    "href": "Python-Data-Analysis/Week9-Time-Series/90_introduction.html#time-series-characteristics",
    "title": "DATAIDEA",
    "section": "Time Series Characteristics",
    "text": "Time Series Characteristics\nMean, standard deviation and seasonality defines different characteristics of the time series.\n\n\n\nTime_Series_Characteristics\n\n\nImportant characteristics of the time series are as below\n\nTrend\nTrend represent the change in dependent variables with respect to time from start to end. In case of increasing trend dependent variable will increase with time and vice versa. It’s not necessary to have definite trend in time series, we can have a single time series with increasing and decreasing trend. In short trend represent the varying mean of time series data.\n\n\n\nTrend\n\n\n\n\nSeasonality\nIf observations repeats after fixed time interval then they are referred as seasonal observations. These seasonal changes in data can occur because of natural events or man-made events. For example every year warm cloths sales increases just before winter season. So seasonality represent the data variations at fixed intervals.\n\n\n\nSeasonality\n\n\n\n\nIrregularities\nThis is also called as noise. Strange dips and jump in the data are called as irregularities. These fluctuations are caused by uncontrollable events like earthquakes, wars, flood, pandemic etc. For example because of COVID-19 pandemic there is huge demand for hand sanitizers and masks.\n\n\n\nIrregularities\n\n\n\n\nCyclicity\nCyclicity occurs when observations in the series repeats in random pattern. Note that if there is any fixed pattern then it becomes seasonality, in case of cyclicity observations may repeat after a week, months or may be after a year. These kinds of patterns are much harder to predict.\n\n\n\nCyclicity\n\n\nTime series data which has above characteristics is called as ‘Non-Stationary Data’. For any analysis on time series data we must convert it to ‘Stationary Data’\nThe general guideline is to estimate the trend and seasonality in the time series, and then make the time series stationary for data modeling. In data modeling step statistical techniques are used for time series analysis and forecasting. Once we have the predictions, in the final step forecasted values converted into the original scale by applying trend and seasonality constraints back.\n[ad]",
    "crumbs": [
      "Python-Data-Analysis",
      "Week9-Time-Series",
      "What is Time Series"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week9-Time-Series/91_analysis.html",
    "href": "Python-Data-Analysis/Week9-Time-Series/91_analysis.html",
    "title": "DATAIDEA",
    "section": "",
    "text": "Time_Series_Header",
    "crumbs": [
      "Python-Data-Analysis",
      "Week9-Time-Series",
      "Time Series Analysis"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week9-Time-Series/91_analysis.html#time-series-analysis",
    "href": "Python-Data-Analysis/Week9-Time-Series/91_analysis.html#time-series-analysis",
    "title": "DATAIDEA",
    "section": "Time Series Analysis",
    "text": "Time Series Analysis\nAs name suggest its analysis of the time series data to identify the patterns in it. I will briefly explain the different techniques and test for time series data analysis.\n\nAir Passengers Data Set\nWe have a monthly time series data of the air passengers from 1 Jan 1949 to 1 Dec 1960. Each row contains the air passenger number for a month of that particular year. Objective is to build a model to forecast the air passenger traffic for future months.\n[ad]",
    "crumbs": [
      "Python-Data-Analysis",
      "Week9-Time-Series",
      "Time Series Analysis"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week9-Time-Series/91_analysis.html#decomposition-of-time-series",
    "href": "Python-Data-Analysis/Week9-Time-Series/91_analysis.html#decomposition-of-time-series",
    "title": "DATAIDEA",
    "section": "Decomposition of Time Series",
    "text": "Decomposition of Time Series\nTime series decomposition helps to deconstruct the time series into several component like trend and seasonality for better visualization of its characteristics. Using time-series decomposition makes it easier to quickly identify a changing mean or variation in the data\n\n\n\nDecomposition_of_Time_Series",
    "crumbs": [
      "Python-Data-Analysis",
      "Week9-Time-Series",
      "Time Series Analysis"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week9-Time-Series/91_analysis.html#stationary-data",
    "href": "Python-Data-Analysis/Week9-Time-Series/91_analysis.html#stationary-data",
    "title": "DATAIDEA",
    "section": "Stationary Data",
    "text": "Stationary Data\nFor accurate analysis and forecasting trend and seasonality is removed from the time series and converted it into stationary series. Time series data is said to be stationary when statistical properties like mean, standard deviation are constant and there is no seasonality. In other words statistical properties of the time series data should not be a function of time.\n\n\n\nStationarity",
    "crumbs": [
      "Python-Data-Analysis",
      "Week9-Time-Series",
      "Time Series Analysis"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week9-Time-Series/91_analysis.html#test-for-stationarity",
    "href": "Python-Data-Analysis/Week9-Time-Series/91_analysis.html#test-for-stationarity",
    "title": "DATAIDEA",
    "section": "Test for Stationarity",
    "text": "Test for Stationarity\nEasy way is to look at the plot and look for any obvious trend or seasonality. While working on real world data we can also use more sophisticated methods like rolling statistic and Augmented Dickey Fuller test to check stationarity of the data.\n\nRolling Statistics\nIn rolling statistics technique we define a size of window to calculate the mean and standard deviation throughout the series. For stationary series mean and standard deviation shouldn’t change with time.\n\n\nAugmented Dickey Fuller (ADF) Test\nI won’t go into the details of how this test works. I will concentrate more on how to interpret the result of this test to determine the stationarity of the series. ADF test will return ‘p-value’ and ‘Test Statistics’ output values. * p-value &gt; 0.05: non-stationary. * p-value &lt;= 0.05: stationary. * Test statistics: More negative this value more likely we have stationary series. Also, this value should be smaller than critical values(1%, 5%, 10%). For e.g. If test statistic is smaller than the 5% critical values, then we can say with 95% confidence that this is a stationary series\n[ad]",
    "crumbs": [
      "Python-Data-Analysis",
      "Week9-Time-Series",
      "Time Series Analysis"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week9-Time-Series/91_analysis.html#convert-non-stationary-data-to-stationary-data",
    "href": "Python-Data-Analysis/Week9-Time-Series/91_analysis.html#convert-non-stationary-data-to-stationary-data",
    "title": "DATAIDEA",
    "section": "Convert Non-Stationary Data to Stationary Data",
    "text": "Convert Non-Stationary Data to Stationary Data\nAccounting for the time series data characteristics like trend and seasonality is called as making data stationary. So by making the mean and variance of the time series constant, we will get the stationary data. Below are the few technique used for the same…\n\nDifferencing\nDifferencing technique helps to remove the trend and seasonality from time series data. Differencing is performed by subtracting the previous observation from the current observation. The differenced data will contain one less data point than original data. So differencing actually reduces the number of observations and stabilize the mean of a time series.\n\\[d = t - t0\\]\nAfter performing the differencing it’s recommended to plot the data and visualize the change. In case there is not sufficient improvement you can perform second order or even third order differencing.\n\n\nTransformation\nA simple but often effective way to stabilize the variance across time is to apply a power transformation to the time series. Log, square root, cube root are most commonly used transformation techniques. Most of the time you can pick the type of growth of the time series and accordingly choose the transformation method. For. e.g. A time series that has a quadratic growth trend can be made linear by taking the square root. In case differencing don’t work, you may first want to use one of above transformation technique to remove the variation from the series.\n\n\n\nLog_Transformation\n\n\n\n\nMoving Average\nIn moving averages technique, a new series is created by taking the averages of data points from original series. In this technique we can use two or more raw data points to calculate the average. This is also called as ‘window width (w)’. Once window width is decided, averages are calculated from start to the end for each set of w consecutive values, hence the name moving averages. It can also be used for time series forecasting.\n\n\n\nMoving_Average\n\n\n\nWeighted Moving Averages(WMA)\nWMA is a technical indicator that assigns a greater weighting to the most recent data points, and less weighting to data points in the distant past. The WMA is obtained by multiplying each number in the data set by a predetermined weight and summing up the resulting values. There can be many techniques for assigning weights. A popular one is exponentially weighted moving average where weights are assigned to all the previous values with a decay factor.\n\n\nCentered Moving Averages(CMS)\nIn a centered moving average, the value of the moving average at time t is computed by centering the window around time t and averaging across the w values within the window. For example, a center moving average with a window of 3 would be calculated as\n\\[CMA(t) = mean(t-1, t, t+1)\\]\nCMA is very useful for visualizing the time series data\n\n\nTrailing Moving Averages(TMA)\nIn trailing moving average, instead of averaging over a window that is centered around a time period of interest, it simply takes the average of the last w values. For example, a trailing moving average with a window of 3 would be calculated as:\n\\[TMA(t) = mean(t-2, t-1, t)\\]\nTMA are useful for forecasting.\n[ad]",
    "crumbs": [
      "Python-Data-Analysis",
      "Week9-Time-Series",
      "Time Series Analysis"
    ]
  },
  {
    "objectID": "Python-Data-Analysis/Week9-Time-Series/91_analysis.html#correlation",
    "href": "Python-Data-Analysis/Week9-Time-Series/91_analysis.html#correlation",
    "title": "DATAIDEA",
    "section": "Correlation",
    "text": "Correlation\n\nMost important point about values in time series is its dependence on the previous values.\nWe can calculate the correlation for time series observations with previous time steps, called as lags.\nBecause the correlation of the time series observations is calculated with values of the same series at previous times, this is called an autocorrelation or serial correlation.\nTo understand it better lets consider the example of fish prices. We will use below notation to represent the fish prices.\n\n\\(P(t)\\)= Fish price of today\n\\(P(t-1)\\) = Fish price of last month\n\\(P(t-2)\\) =Fish price of last to last month\n\nTime series of fish prices can be represented as \\(P(t-n),..... P(t-3), P(t-2),P(t-1), P(t)\\)\nSo if we have fish prices for last few months then it will be easy for us to predict the fish price for today (Here we are ignoring all other external factors that may affect the fish prices)\n\nAll the past and future data points are related in time series and ACF and PACF functions help us to determine correlation in it.\n\nAuto Correlation Function (ACF)\n\nACF tells you how correlated points are with each other, based on how many time steps they are separated by.\nNow to understand it better lets consider above example of fish prices. Let’s try to find the correlation between fish price for current month P(t) and two months ago P(t-2). Important thing to note that, fish price of two months ago can directly affect the today’s fish price or it can indirectly affect the fish price through last months price P(t-1)\nSo ACF consider the direct as well indirect effect between the points while determining the correlation\n\n\n\nPartial Auto Correlation Function (PACF)\n\nUnlike ACF, PACF only consider the direct effect between the points while determining the correlation\nIn case of above fish price example PACF will determine the correlation between fish price for current month P(t) and two months ago P(t-2) by considering only P(t) and P(t-2) and ignoring P(t-1)\n\n[ad]",
    "crumbs": [
      "Python-Data-Analysis",
      "Week9-Time-Series",
      "Time Series Analysis"
    ]
  }
]